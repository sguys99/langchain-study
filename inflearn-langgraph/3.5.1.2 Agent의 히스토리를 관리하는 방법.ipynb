{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.5.1.2 Agent의 히스토리를 관리하는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (방법2) 노드를 추가해서 지우는 방법\n",
    "- 다음 파일에...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\")\n",
    "\n",
    "small_llm = ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 덧셈 도구도 추가할 예정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"숫자 a와 b를 더합니다.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"숫자 a와 b를 곱합니다.\"\"\"\n",
    "    return a * b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "간단한 웹 검색 도구를 활용하자.  \n",
    "\n",
    "[Tavily](https://tavily.com/)는 매월 1000건의 무료 호출을 제공하지만 [DuckDuckGo](https://duckduckgo.com/)는 완전 무료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "search_tool = DuckDuckGoSearchRun()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[GmailToolkit](https://python.langchain.com/docs/integrations/tools/gmail)은 구글 메일 도구를 제공   \n",
    "먼저 `credentials` 설정 후에 사용할 수 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "매뉴얼을 보면 사용하는 방법이 두가지 이다.  \n",
    "- 방법1: 로컬에 credential.json파일을 읽는 방법\n",
    "```python\n",
    "from langchain_google_community import GmailToolkit\n",
    "\n",
    "toolkit = GmailToolkit()\n",
    "```\n",
    "\n",
    "- 방법2: 커스텀 인증 방법\n",
    "```python\n",
    "from langchain_google_community.gmail.utils import (\n",
    "    build_resource_service,\n",
    "    get_gmail_credentials,\n",
    ")\n",
    "\n",
    "# Can review scopes here https://developers.google.com/gmail/api/auth/scopes\n",
    "# For instance, readonly scope is 'https://www.googleapis.com/auth/gmail.readonly'\n",
    "credentials = get_gmail_credentials(\n",
    "    token_file=\"token.json\",\n",
    "    scopes=[\"https://mail.google.com/\"],\n",
    "    client_secrets_file=\"credentials.json\",\n",
    ")\n",
    "api_resource = build_resource_service(credentials=credentials)\n",
    "toolkit = GmailToolkit(api_resource=api_resource)\n",
    "```\n",
    "\n",
    "- 두번째 방법 추천"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "우선 다음 링크를 참고해서 credential을 생성해서 json파일 다운로드\n",
    "- https://developers.google.com/workspace/gmail/api/quickstart/python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이후 안되면 권한 승인 필요\n",
    "- https://console.cloud.google.com/auth/audience?inv=1&invt=Ab1OBg&project=my-first-project-409013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_community import GmailToolkit\n",
    "\n",
    "from langchain_google_community.gmail.utils import (\n",
    "    build_resource_service,\n",
    "    get_gmail_credentials,\n",
    ")\n",
    "\n",
    "# Can review scopes here https://developers.google.com/gmail/api/auth/scopes\n",
    "# For instance, readonly scope is 'https://www.googleapis.com/auth/gmail.readonly'\n",
    "credentials = get_gmail_credentials(\n",
    "    token_file=\"./google/gmail_token.json\",\n",
    "    scopes=[\"https://mail.google.com/\"],\n",
    "    client_secrets_file=\"./google/gmail_credentials.json\",\n",
    ")\n",
    "api_resource = build_resource_service(credentials=credentials)\n",
    "gmail_toolkit = GmailToolkit(api_resource=api_resource)\n",
    "gmail_tool_list = gmail_toolkit.get_tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[GmailCreateDraft(api_resource=<googleapiclient.discovery.Resource object at 0x1334d1cd0>),\n",
       " GmailSendMessage(api_resource=<googleapiclient.discovery.Resource object at 0x1334d1cd0>),\n",
       " GmailSearch(api_resource=<googleapiclient.discovery.Resource object at 0x1334d1cd0>),\n",
       " GmailGetMessage(api_resource=<googleapiclient.discovery.Resource object at 0x1334d1cd0>),\n",
       " GmailGetThread(api_resource=<googleapiclient.discovery.Resource object at 0x1334d1cd0>)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmail_tool_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이메일 작성, 이메일 보내기, 이메일 검색, 받기 등이 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[load_tools](https://api.python.langchain.com/en/latest/agents/langchain.agents.load_tools.load_tools.html) 함수를 통해 내장 도구를 불러올 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "\n",
    "loaded_tool_list = load_tools(\n",
    "    [\"arxiv\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "마지막으로 create_retriever_tool을 사용하면 리트리버를 쉽게 도구로 구성할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.tools.retriever import create_retriever_tool\n",
    "\n",
    "embedding_function = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-large\",\n",
    ")\n",
    "vector_store = Chroma(\n",
    "    embedding_function=embedding_function,\n",
    "    collection_name=\"real_estate_tax\",\n",
    "    persist_directory=\"./real_estate_tax_collection\",\n",
    ")\n",
    "retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever=retriever,\n",
    "    name=\"real_estate_tax_retriever\",\n",
    "    description=\"Contains information about real estate tax up to December 2024\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`ToolNode`에 도구 리스트를 전달한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import ToolNode\n",
    "\n",
    "tool_list = (\n",
    "    [add, multiply, search_tool, retriever_tool] + gmail_tool_list + loaded_tool_list\n",
    ")\n",
    "llm_with_tools = small_llm.bind_tools(tool_list)\n",
    "tool_node = ToolNode(tool_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `state`를 선언하고 agent를 생성합니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import MessagesState, StateGraph\n",
    "\n",
    "graph_builder = StateGraph(MessagesState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent(state: MessagesState) -> MessagesState:\n",
    "    \"\"\"\n",
    "    에이전트 함수는 주어진 상태에서 메시지를 가져와\n",
    "    LLM과 도구를 사용하여 응답 메시지를 생성합니다.\n",
    "\n",
    "    Args:\n",
    "        state (MessagesState): 메시지 상태를 포함하는 state.\n",
    "\n",
    "    Returns:\n",
    "        MessagesState: 응답 메시지를 포함하는 새로운 state.\n",
    "    \"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    response = llm_with_tools.invoke(messages)\n",
    "    return {\"messages\": [response]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 중요: 삭제를 위한 delete_messages노드 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import RemoveMessage\n",
    "\n",
    "\n",
    "def delete_messages(state: MessagesState):\n",
    "    messages = state[\"messages\"]\n",
    "    delete_messages = [RemoveMessage(id=message.id) for message in messages[:-3]]\n",
    "    return {\"messages\": delete_messages}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 중요: should_continue 추가하기  \n",
    "delete_messages를 추가하려면 기존 prebuilt인 tools_condition을 사용하지 못한다.   \n",
    "그래서 should_continue 노드를 만들어 줘야 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_continue(state: MessagesState):\n",
    "    messages = state[\"messages\"]\n",
    "    last_ai_message = messages[-1]\n",
    "    if last_ai_message.tool_calls:\n",
    "        return \"tools\"\n",
    "\n",
    "    return \"delete_messages\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`node`를 추가하고 `edge`로 연결합니다\n",
    "\n",
    "**delete_messages도 추가해준다.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1378b6a50>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_builder.add_node(\"agent\", agent)\n",
    "graph_builder.add_node(\"tools\", tool_node)\n",
    "graph_builder.add_node(\n",
    "    delete_messages\n",
    ")  # 중요: 노드 이름과 함수 이름이 같으면 이같이 해도 괜찮다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conditional_edge 수정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x1378b6a50>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langgraph.graph import START, END\n",
    "from langgraph.prebuilt import tools_condition\n",
    "\n",
    "\n",
    "graph_builder.add_edge(START, \"agent\")\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"agent\", should_continue, [\"tools\", \"delete_messages\"]\n",
    ")\n",
    "graph_builder.add_edge(\"tools\", \"agent\")\n",
    "graph_builder.add_edge(\"delete_messages\", END)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (중요) 이제, MemorySaver를 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "checkpointer = MemorySaver()\n",
    "\n",
    "graph = graph_builder.compile(checkpointer=checkpointer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "checkpoint가 생겼기 때문에 스레드 관리를 해줘야 한다.  \n",
    "config를 사용해서 관리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"paper_summary\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "sguys99@gmail.com으로 Attention Is All You Need 논문을 요약해서 이메일 초안을 작성해주세요. 한글로 작성하세요.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  arxiv (call_Zn3FTl55FVqnOFFTVVNwXIjz)\n",
      " Call ID: call_Zn3FTl55FVqnOFFTVVNwXIjz\n",
      "  Args:\n",
      "    query: Attention Is All You Need\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: arxiv\n",
      "\n",
      "Published: 2024-07-22\n",
      "Title: Attention Is All You Need But You Don't Need All Of It For Inference of Large Language Models\n",
      "Authors: Georgy Tyukin, Gbetondji J-S Dovonon, Jean Kaddour, Pasquale Minervini\n",
      "Summary: The inference demand for LLMs has skyrocketed in recent months, and serving\n",
      "models with low latencies remains challenging due to the quadratic input length\n",
      "complexity of the attention layers. In this work, we investigate the effect of\n",
      "dropping MLP and attention layers at inference time on the performance of\n",
      "Llama-v2 models. We find that dropping dreeper attention layers only marginally\n",
      "decreases performance but leads to the best speedups alongside dropping entire\n",
      "layers. For example, removing 33\\% of attention layers in a 13B Llama2 model\n",
      "results in a 1.8\\% drop in average performance over the OpenLLM benchmark. We\n",
      "also observe that skipping layers except the latter layers reduces performances\n",
      "for more layers skipped, except for skipping the attention layers.\n",
      "\n",
      "Published: 2021-07-16\n",
      "Title: All the attention you need: Global-local, spatial-channel attention for image retrieval\n",
      "Authors: Chull Hwan Song, Hye Joo Han, Yannis Avrithis\n",
      "Summary: We address representation learning for large-scale instance-level image\n",
      "retrieval. Apart from backbone, training pipelines and loss functions, popular\n",
      "approaches have focused on different spatial pooling and attention mechanisms,\n",
      "which are at the core of learning a powerful global image representation. There\n",
      "are different forms of attention according to the interaction of elements of\n",
      "the feature tensor (local and global) and the dimensions where it is applied\n",
      "(spatial and channel). Unfortunately, each study addresses only one or two\n",
      "forms of attention and applies it to different problems like classification,\n",
      "detection or retrieval.\n",
      "  We present global-local attention module (GLAM), which is attached at the end\n",
      "of a backbone network and incorporates all four forms of attention: local and\n",
      "global, spatial and channel. We obtain a new feature tensor and, by spatial\n",
      "pooling, we learn a powerful embedding for image retrieval. Focusing on global\n",
      "descriptors, we provide empirical evidence of the interaction of all forms of\n",
      "attention and improve the state of the art on standard benchmarks.\n",
      "\n",
      "Published: 2023-06-02\n",
      "Title: RITA: Group Attention is All You Need for Timeseries Analytics\n",
      "Authors: Jiaming Liang, Lei Cao, Samuel Madden, Zachary Ives, Guoliang Li\n",
      "Summary: Timeseries analytics is of great importance in many real-world applications.\n",
      "Recently, the Transformer model, popular in natural language processing, has\n",
      "been leveraged to learn high quality feature embeddings from timeseries, core\n",
      "to the performance of various timeseries analytics tasks. However, the\n",
      "quadratic time and space complexities limit Transformers' scalability,\n",
      "especially for long timeseries. To address these issues, we develop a\n",
      "timeseries analytics tool, RITA, which uses a novel attention mechanism, named\n",
      "group attention, to address this scalability issue. Group attention dynamically\n",
      "clusters the objects based on their similarity into a small number of groups\n",
      "and approximately computes the attention at the coarse group granularity. It\n",
      "thus significantly reduces the time and space complexity, yet provides a\n",
      "theoretical guarantee on the quality of the computed attention. The dynamic\n",
      "scheduler of RITA continuously adapts the number of groups and the batch size\n",
      "in the training process, ensuring group attention always uses the fewest groups\n",
      "needed to meet the approximation quality requirement. Extensive experiments on\n",
      "various timeseries datasets and analytics tasks demonstrate that RITA\n",
      "outperforms the state-of-the-art in accuracy and is significantly faster --\n",
      "with speedups of up to 63X.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  create_gmail_draft (call_xgngnbPypLPRoU0ynHoxePrO)\n",
      " Call ID: call_xgngnbPypLPRoU0ynHoxePrO\n",
      "  Args:\n",
      "    message: 안녕하세요,\n",
      "\n",
      "다음은 'Attention Is All You Need' 논문의 요약입니다.\n",
      "\n",
      "'Attention Is All You Need' 논문은 자연어 처리에서 Transformer 모델의 핵심 개념인 어텐션 메커니즘에 대해 설명합니다. 이 논문은 전통적인 순환 신경망(RNN)과 달리 어텐션 메커니즘만을 사용하여 번역 작업을 수행하는 모델을 제안하며, 이는 계산 효율성과 성능 면에서 뛰어납니다.\n",
      "\n",
      "Transformer 모델은 인코더와 디코더 구조로 이루어져 있으며, 각 층은 셀프 어텐션과 피드 포워드 네트워크로 구성됩니다. 셀프 어텐션 메커니즘은 입력 시퀀스의 각 요소가 서로 얼마나 관련이 있는지를 계산하여 문맥을 이해하는 데 도움을 줍니다.\n",
      "\n",
      "이 논문은 어텐션 메커니즘이 다른 복잡한 구조 없이도 높은 성능을 낼 수 있음을 보여주며, 이후 다양한 자연어 처리 작업에서 널리 사용되는 계기가 됩니다.\n",
      "\n",
      "감사합니다.\n",
      "    to: ['sguys99@gmail.com']\n",
      "    subject: Attention Is All You Need 논문 요약\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: create_gmail_draft\n",
      "\n",
      "Draft created. Draft Id: r-7148326502654377785\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "\"Attention Is All You Need\" 논문의 요약을 포함한 이메일 초안을 작성했습니다. 초안은 수신자 sguys99@gmail.com에게 전송할 준비가 되어 있습니다. 추가 수정이 필요하시면 알려주세요.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "\"Attention Is All You Need\" 논문의 요약을 포함한 이메일 초안을 작성했습니다. 초안은 수신자 sguys99@gmail.com에게 전송할 준비가 되어 있습니다. 추가 수정이 필요하시면 알려주세요.\n"
     ]
    }
   ],
   "source": [
    "query = \"sguys99@gmail.com으로 Attention Is All You Need 논문을 요약해서 이메일 초안을 작성해주세요. 한글로 작성하세요.\"\n",
    "\n",
    "for chunk in graph.stream(\n",
    "    {\"messages\": [HumanMessage(query)]}, config=config, stream_mode=\"values\"\n",
    "):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "논문의 출처 url을 첨부해 주세요.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  arxiv (call_iM4uZoFj7gL34RwFakxTfKtP)\n",
      " Call ID: call_iM4uZoFj7gL34RwFakxTfKtP\n",
      "  Args:\n",
      "    query: Attention Is All You Need\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: arxiv\n",
      "\n",
      "Published: 2024-07-22\n",
      "Title: Attention Is All You Need But You Don't Need All Of It For Inference of Large Language Models\n",
      "Authors: Georgy Tyukin, Gbetondji J-S Dovonon, Jean Kaddour, Pasquale Minervini\n",
      "Summary: The inference demand for LLMs has skyrocketed in recent months, and serving\n",
      "models with low latencies remains challenging due to the quadratic input length\n",
      "complexity of the attention layers. In this work, we investigate the effect of\n",
      "dropping MLP and attention layers at inference time on the performance of\n",
      "Llama-v2 models. We find that dropping dreeper attention layers only marginally\n",
      "decreases performance but leads to the best speedups alongside dropping entire\n",
      "layers. For example, removing 33\\% of attention layers in a 13B Llama2 model\n",
      "results in a 1.8\\% drop in average performance over the OpenLLM benchmark. We\n",
      "also observe that skipping layers except the latter layers reduces performances\n",
      "for more layers skipped, except for skipping the attention layers.\n",
      "\n",
      "Published: 2021-07-16\n",
      "Title: All the attention you need: Global-local, spatial-channel attention for image retrieval\n",
      "Authors: Chull Hwan Song, Hye Joo Han, Yannis Avrithis\n",
      "Summary: We address representation learning for large-scale instance-level image\n",
      "retrieval. Apart from backbone, training pipelines and loss functions, popular\n",
      "approaches have focused on different spatial pooling and attention mechanisms,\n",
      "which are at the core of learning a powerful global image representation. There\n",
      "are different forms of attention according to the interaction of elements of\n",
      "the feature tensor (local and global) and the dimensions where it is applied\n",
      "(spatial and channel). Unfortunately, each study addresses only one or two\n",
      "forms of attention and applies it to different problems like classification,\n",
      "detection or retrieval.\n",
      "  We present global-local attention module (GLAM), which is attached at the end\n",
      "of a backbone network and incorporates all four forms of attention: local and\n",
      "global, spatial and channel. We obtain a new feature tensor and, by spatial\n",
      "pooling, we learn a powerful embedding for image retrieval. Focusing on global\n",
      "descriptors, we provide empirical evidence of the interaction of all forms of\n",
      "attention and improve the state of the art on standard benchmarks.\n",
      "\n",
      "Published: 2023-06-02\n",
      "Title: RITA: Group Attention is All You Need for Timeseries Analytics\n",
      "Authors: Jiaming Liang, Lei Cao, Samuel Madden, Zachary Ives, Guoliang Li\n",
      "Summary: Timeseries analytics is of great importance in many real-world applications.\n",
      "Recently, the Transformer model, popular in natural language processing, has\n",
      "been leveraged to learn high quality feature embeddings from timeseries, core\n",
      "to the performance of various timeseries analytics tasks. However, the\n",
      "quadratic time and space complexities limit Transformers' scalability,\n",
      "especially for long timeseries. To address these issues, we develop a\n",
      "timeseries analytics tool, RITA, which uses a novel attention mechanism, named\n",
      "group attention, to address this scalability issue. Group attention dynamically\n",
      "clusters the objects based on their similarity into a small number of groups\n",
      "and approximately computes the attention at the coarse group granularity. It\n",
      "thus significantly reduces the time and space complexity, yet provides a\n",
      "theoretical guarantee on the quality of the computed attention. The dynamic\n",
      "scheduler of RITA continuously adapts the number of groups and the batch size\n",
      "in the training process, ensuring group attention always uses the fewest groups\n",
      "needed to meet the approximation quality requirement. Extensive experiments on\n",
      "various timeseries datasets and analytics tasks demonstrate that RITA\n",
      "outperforms the state-of-the-art in accuracy and is significantly faster --\n",
      "with speedups of up to 63X.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "현재 제공된 검색 결과에는 \"Attention Is All You Need\" 논문의 정확한 URL이 포함되어 있지 않습니다. 대신 이 논문의 제목과 저자를 사용하여 직접 arXiv.org에서 찾을 수 있습니다. \"Attention Is All You Need\"의 원 논문은 Vaswani et al.에 의해 2017년에 발표되었습니다.\n",
      "\n",
      "arXiv 검색을 통해 [여기](https://arxiv.org/abs/1706.03762)에서 찾을 수 있습니다. 이 링크를 이메일 초안에 첨부하겠습니다.\n",
      "Tool Calls:\n",
      "  create_gmail_draft (call_bpP48ukefWrSnsu8aExUYZwD)\n",
      " Call ID: call_bpP48ukefWrSnsu8aExUYZwD\n",
      "  Args:\n",
      "    message: 안녕하세요,\n",
      "\n",
      "다음은 'Attention Is All You Need' 논문의 요약입니다.\n",
      "\n",
      "'Attention Is All You Need' 논문은 자연어 처리에서 Transformer 모델의 핵심 개념인 어텐션 메커니즘에 대해 설명합니다. 이 논문은 전통적인 순환 신경망(RNN)과 달리 어텐션 메커니즘만을 사용하여 번역 작업을 수행하는 모델을 제안하며, 이는 계산 효율성과 성능 면에서 뛰어납니다.\n",
      "\n",
      "Transformer 모델은 인코더와 디코더 구조로 이루어져 있으며, 각 층은 셀프 어텐션과 피드 포워드 네트워크로 구성됩니다. 셀프 어텐션 메커니즘은 입력 시퀀스의 각 요소가 서로 얼마나 관련이 있는지를 계산하여 문맥을 이해하는 데 도움을 줍니다.\n",
      "\n",
      "이 논문은 어텐션 메커니즘이 다른 복잡한 구조 없이도 높은 성능을 낼 수 있음을 보여주며, 이후 다양한 자연어 처리 작업에서 널리 사용되는 계기가 됩니다.\n",
      "\n",
      "논문의 출처 URL: [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n",
      "\n",
      "감사합니다.\n",
      "    to: ['sguys99@gmail.com']\n",
      "    subject: Attention Is All You Need 논문 요약\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: create_gmail_draft\n",
      "\n",
      "Draft created. Draft Id: r-6625443967060305574\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "논문의 출처 URL을 포함하여 이메일 초안을 업데이트했습니다. 초안은 수신자 sguys99@gmail.com에게 전송할 준비가 되어 있습니다. 추가 수정이 필요하시면 알려주세요.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "논문의 출처 URL을 포함하여 이메일 초안을 업데이트했습니다. 초안은 수신자 sguys99@gmail.com에게 전송할 준비가 되어 있습니다. 추가 수정이 필요하시면 알려주세요.\n"
     ]
    }
   ],
   "source": [
    "update_query = \"논문의 출처 url을 첨부해 주세요.\"\n",
    "\n",
    "for chunk in graph.stream(\n",
    "    {\"messages\": [HumanMessage(update_query)]}, config=config, stream_mode=\"values\"\n",
    "):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "초안이 좋습니다. 이메일을 전송해 주세요.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "Tool Calls:\n",
      "  send_gmail_message (call_MevD9ZaRa00Si8bvXWEHOwjN)\n",
      " Call ID: call_MevD9ZaRa00Si8bvXWEHOwjN\n",
      "  Args:\n",
      "    message: 안녕하세요,\n",
      "\n",
      "다음은 'Attention Is All You Need' 논문의 요약입니다.\n",
      "\n",
      "'Attention Is All You Need' 논문은 자연어 처리에서 Transformer 모델의 핵심 개념인 어텐션 메커니즘에 대해 설명합니다. 이 논문은 전통적인 순환 신경망(RNN)과 달리 어텐션 메커니즘만을 사용하여 번역 작업을 수행하는 모델을 제안하며, 이는 계산 효율성과 성능 면에서 뛰어납니다.\n",
      "\n",
      "Transformer 모델은 인코더와 디코더 구조로 이루어져 있으며, 각 층은 셀프 어텐션과 피드 포워드 네트워크로 구성됩니다. 셀프 어텐션 메커니즘은 입력 시퀀스의 각 요소가 서로 얼마나 관련이 있는지를 계산하여 문맥을 이해하는 데 도움을 줍니다.\n",
      "\n",
      "이 논문은 어텐션 메커니즘이 다른 복잡한 구조 없이도 높은 성능을 낼 수 있음을 보여주며, 이후 다양한 자연어 처리 작업에서 널리 사용되는 계기가 됩니다.\n",
      "\n",
      "논문의 출처 URL: [Attention Is All You Need](https://arxiv.org/abs/1706.03762)\n",
      "\n",
      "감사합니다.\n",
      "    to: sguys99@gmail.com\n",
      "    subject: Attention Is All You Need 논문 요약\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "Name: send_gmail_message\n",
      "\n",
      "Message sent. Message Id: 197beab791238336\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "이메일이 성공적으로 전송되었습니다. 추가로 도움이 필요하시면 언제든지 말씀해 주세요.\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "이메일이 성공적으로 전송되었습니다. 추가로 도움이 필요하시면 언제든지 말씀해 주세요.\n"
     ]
    }
   ],
   "source": [
    "update_query = \"초안이 좋습니다. 이메일을 전송해 주세요.\"\n",
    "\n",
    "for chunk in graph.stream(\n",
    "    {\"messages\": [HumanMessage(update_query)]}, config=config, stream_mode=\"values\"\n",
    "):\n",
    "    chunk[\"messages\"][-1].pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inflearn-langgraph",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
