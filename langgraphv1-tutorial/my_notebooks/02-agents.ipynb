{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2550920c-09d8-48b3-be2f-b36362c37989",
   "metadata": {},
   "source": [
    "## 2. Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a6b509",
   "metadata": {},
   "source": [
    "<div style=\"text-align: right\"> Initial issue : 2025.11.05 </div>\n",
    "<div style=\"text-align: right\"> last update : 2025.11.05 </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21a6c04d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240a1ed1",
   "metadata": {},
   "source": [
    "### ì—ì´ì „íŠ¸ ì‘ë™ë°©ì‹\n",
    "- ëª©í‘œ ë‹¬ì„±ì„ ìœ„í•´ ë„êµ¬ë¥¼ ë°˜ë³µì ìœ¼ë¡œ ì‹¤í–‰  \n",
    "- ì¦‰, ì¤‘ë‹¨ ì¡°ê±´ì´ ì¶©ì¡± ë ë•Œê¹Œì§€ ì‹¤í–‰ë˜ë©°, ëª¨ë¸ì´ ìµœì¢… ì¶œë ¥ì„ ìƒì„±í•˜ê±°ë‚˜ ë°˜ë³µ ì œí•œì— ë„ë‹¬í•˜ë©´ ì¤‘ë‹¨\n",
    "\n",
    "```\n",
    "ì…ë ¥ -> ëª¨ë¸ -> ë„êµ¬ í˜¸ì¶œ -> ë„êµ¬ ì‹¤í–‰ -> ëª¨ë¸ -> ... -> ìµœì¢… ì¶œë ¥\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef49bf4f",
   "metadata": {},
   "source": [
    "### ëª¨ë¸\n",
    "- ëª¨ë¸ì€ ì—ì´ì „íŠ¸ì˜ ì¶”ë¡ ì—”ì§„\n",
    "- ëª¨ë¸ì„ ì •ì  ë˜ëŠ” ë™ì ìœ¼ë¡œ ì§€ì •ê°€ëŠ¥"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca6ec69",
   "metadata": {},
   "source": [
    "ì •ì ëª¨ë¸: ì—ì´ì „íŠ¸ ìƒì„±ì‹œ í•œë²ˆ êµ¬ì„±ë˜ë©° ì‹¤í–‰ ì¤‘ ë³€ê²½ë˜ì§€ ì•ŠìŒ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19fb360f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain_openai import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf596770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°©ë²• 1: ëª¨ë¸ ì‹ë³„ì ë¬¸ìì—´ì„ ì‚¬ìš©í•œ ê°„ë‹¨í•œ ë°©ë²•\n",
    "agent = create_agent(\"openai:gpt-4.1-mini\", tools=[])\n",
    "\n",
    "# ë°©ë²• 2: ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì§ì ‘ ì´ˆê¸°í™”í•˜ì—¬ ë” ì„¸ë°€í•œ ì œì–´\n",
    "model = ChatOpenAI(\n",
    "    model = \"gpt-4.1-mini\",\n",
    "    temperature=0.1,\n",
    "    max_completion_tokens=1000,\n",
    "    timeout=30 # ì´ˆ\n",
    ")\n",
    "\n",
    "agent2 = create_agent(model=model, tools=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397478f8",
   "metadata": {},
   "source": [
    "ë™ì  ëª¨ë¸: ëŸ°íƒ€ì„ì— í˜„ì¬ ìƒíƒœì™€ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì„ íƒ\n",
    "- ì •êµí•œ ë¼ìš°íŒ… ë¡œì§ê³¼ ë¹„ìš© ìµœì í™” êµ¬í˜„ì´ ê°€ëŠ¥í•¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df9f0c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a57516b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ê¸°ë³¸ ëª¨ë¸ê³¼ ê³ ê¸‰ ëª¨ë¸ ì •ì˜\n",
    "basic_model = ChatOpenAI(model=\"gpt-4.1-mini\")\n",
    "advanced_model = ChatOpenAI(model=\"gpt-4.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6337de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "@wrap_model_call\n",
    "def dynamic_model_selection(request: ModelRequest, handler)-> ModelResponse:\n",
    "    \"\"\"ëŒ€í™” ë³µì¡ë„ì— ë”°ë¼ ëª¨ë¸ ì„ íƒ\"\"\"\n",
    "    print(request)\n",
    "    message_count = len(request.state[\"messages\"])\n",
    "\n",
    "    # ê¸´ ëŒ€í™”ì—ëŠ” ê³ ê¸‰ ëª¨ë¸ ì‚¬ìš©\n",
    "    if message_count > 10:\n",
    "        model = advanced_model\n",
    "    else:\n",
    "        model = basic_model\n",
    "\n",
    "    request.model = model\n",
    "    return handler(request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0693b1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_agent(\n",
    "    model = basic_model,\n",
    "    tools=[],\n",
    "    middleware=[dynamic_model_selection]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6e40b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelRequest(model=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x110b9b3e0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x110b9ba70>, root_client=<openai.OpenAI object at 0x110b991f0>, root_async_client=<openai.AsyncOpenAI object at 0x110b994c0>, model_name='gpt-4.1-mini', model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True), system_prompt=None, messages=[HumanMessage(content='ë¨¸ì‹ ëŸ¬ë‹ì˜ ë™ì‘ ì›ë¦¬ì— ëŒ€í•´ì„œ ì„¤ëª…í•´ì¤˜', additional_kwargs={}, response_metadata={}, id='78991d01-0e26-4c65-8116-3b5ecc31ae0b')], tool_choice=None, tools=[], response_format=None, state={'messages': [HumanMessage(content='ë¨¸ì‹ ëŸ¬ë‹ì˜ ë™ì‘ ì›ë¦¬ì— ëŒ€í•´ì„œ ì„¤ëª…í•´ì¤˜', additional_kwargs={}, response_metadata={}, id='78991d01-0e26-4c65-8116-3b5ecc31ae0b')]}, runtime=Runtime(context=None, store=None, stream_writer=<function Pregel.stream.<locals>.stream_writer at 0x110246de0>, previous=None), model_settings={})\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mmodel\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "ë¬¼ë¡ ì…ë‹ˆë‹¤! ë¨¸ì‹ ëŸ¬ë‹ì˜ ë™ì‘ ì›ë¦¬ë¥¼ ê°„ë‹¨í•˜ê²Œ ì„¤ëª…í•´ë“œë¦´ê²Œìš”.\n",
      "\n",
      "### ë¨¸ì‹ ëŸ¬ë‹ì´ë€?\n",
      "ë¨¸ì‹ ëŸ¬ë‹(Machine Learning)ì€ ì»´í“¨í„°ê°€ ëª…ì‹œì ì¸ í”„ë¡œê·¸ë˜ë° ì—†ì´ë„ ë°ì´í„°ë¥¼ í†µí•´ ìŠ¤ìŠ¤ë¡œ í•™ìŠµí•˜ê³ , íŒ¨í„´ì„ ì¸ì‹í•˜ë©°, ìƒˆë¡œìš´ ë°ì´í„°ì— ëŒ€í•´ ì˜ˆì¸¡í•˜ê±°ë‚˜ ê²°ì •ì„ ë‚´ë¦´ ìˆ˜ ìˆê²Œ í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤.\n",
      "\n",
      "### ë¨¸ì‹ ëŸ¬ë‹ì˜ ê¸°ë³¸ ë™ì‘ ì›ë¦¬\n",
      "1. **ë°ì´í„° ìˆ˜ì§‘**  \n",
      "   ëª¨ë¸ì´ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ ë‹¤ì–‘í•œ ì˜ˆì‹œ ë°ì´í„°ë¥¼ ì¤€ë¹„í•©ë‹ˆë‹¤. ë°ì´í„°ëŠ” íŠ¹ì§•(feature)ê³¼ ì •ë‹µ(label)ìœ¼ë¡œ êµ¬ì„±ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "2. **ë°ì´í„° ì „ì²˜ë¦¬**  \n",
      "   ì›ì‹œ ë°ì´í„°ë¥¼ ì •ì œí•˜ê³  ì •ê·œí™”í•˜ê±°ë‚˜ ê²°ì¸¡ì¹˜ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë“± ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì´ ì˜ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ ê°€ê³µí•©ë‹ˆë‹¤.\n",
      "\n",
      "3. **íŠ¹ì§• ì¶”ì¶œ ë° ì„ íƒ**  \n",
      "   ì…ë ¥ ë°ì´í„°ì—ì„œ ì˜ë¯¸ ìˆëŠ” ì •ë³´ë¥¼ ì¶”ì¶œí•˜ì—¬ ëª¨ë¸ì— ì…ë ¥í•  í•µì‹¬ íŠ¹ì„±(feature)ì„ ê²°ì •í•©ë‹ˆë‹¤.\n",
      "\n",
      "4. **ëª¨ë¸ ì„ íƒ**  \n",
      "   ë¬¸ì œ ìœ í˜•(ë¶„ë¥˜, íšŒê·€, êµ°ì§‘ ë“±)ì— ë§ëŠ” ì•Œê³ ë¦¬ì¦˜(ì˜ˆ: ì˜ì‚¬ê²°ì •íŠ¸ë¦¬, ì‹ ê²½ë§, ì„œí¬íŠ¸ ë²¡í„° ë¨¸ì‹  ë“±)ì„ ì„ íƒí•©ë‹ˆë‹¤.\n",
      "\n",
      "5. **í•™ìŠµ(Training)**  \n",
      "   ì¤€ë¹„ëœ ë°ì´í„°ë¥¼ ëª¨ë¸ì— ë„£ê³ , ëª¨ë¸ì˜ ë‚´ë¶€ íŒŒë¼ë¯¸í„°(ê°€ì¤‘ì¹˜ ë“±)ë¥¼ ìµœì í™”í•©ë‹ˆë‹¤.  \n",
      "   ì´ ê³¼ì •ì—ì„œëŠ” ì†ì‹¤ í•¨ìˆ˜(loss function)ë¥¼ ìµœì†Œí™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•©ë‹ˆë‹¤.  \n",
      "   ëŒ€í‘œì ì¸ ìµœì í™” ë°©ë²•ìœ¼ë¡œ ê²½ì‚¬ í•˜ê°•ë²•(Gradient Descent)ì´ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "6. **í‰ê°€(Evaluation)**  \n",
      "   í•™ìŠµëœ ëª¨ë¸ì„ ìƒˆë¡œìš´(ê²€ì¦ìš©) ë°ì´í„°ì— ì ìš©í•˜ì—¬ ì˜ˆì¸¡ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤. ì •í™•ë„, ì •ë°€ë„, ì¬í˜„ìœ¨ ë“± ì§€í‘œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
      "\n",
      "7. **ì˜ˆì¸¡(Prediction)**  \n",
      "   í•™ìŠµì´ ì™„ë£Œëœ ëª¨ë¸ì„ ì‚¬ìš©í•´ ì‹¤ì œ í™˜ê²½ì—ì„œ ìƒˆë¡œìš´ ë°ì´í„°ì— ëŒ€í•´ ì¶”ë¡ í•˜ê±°ë‚˜ ì˜ˆì¸¡í•©ë‹ˆë‹¤.\n",
      "\n",
      "---\n",
      "\n",
      "### ìš”ì•½í•˜ìë©´\n",
      "ë¨¸ì‹ ëŸ¬ë‹ì€ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ëª¨ë¸ì´ ìŠ¤ìŠ¤ë¡œ ê·œì¹™ì„ ì°¾ê³ , ì´ë¥¼ ì´ìš©í•´ ìƒˆë¡œìš´ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤. ì˜ˆì œë¥¼ í†µí•´ ë§ì´ ë°˜ë³µ í•™ìŠµí•˜ê³ , ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ë©° ì ì  ë” ì •í™•í•œ ì˜ˆì¸¡ì„ í•˜ë„ë¡ ë§Œë“­ë‹ˆë‹¤.\n",
      "\n",
      "í•„ìš”í•˜ë©´ íŠ¹ì • ì•Œê³ ë¦¬ì¦˜ì˜ ë™ì‘ ì›ë¦¬ë„ ìì„¸íˆ ì„¤ëª…í•´ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤!"
     ]
    }
   ],
   "source": [
    "from langchain_teddynote.messages import stream_graph\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "stream_graph(\n",
    "    agent,\n",
    "    inputs={\n",
    "        \"messages\": [HumanMessage(content=\"ë¨¸ì‹ ëŸ¬ë‹ì˜ ë™ì‘ ì›ë¦¬ì— ëŒ€í•´ì„œ ì„¤ëª…í•´ì¤˜\")]\n",
    "    },\n",
    "    config=RunnableConfig(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e838cb6",
   "metadata": {},
   "source": [
    "### ë„êµ¬\n",
    "- ì—ì´ì „íŠ¸ì—ê²Œ í–‰ë™(action)ì„ ì·¨í•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥ ë¶€ì—¬\n",
    "- ì—ì´ì „íŠ¸ëŠ” ë‹¤ìŒì„ ì§€ì›\n",
    "    - ìˆœì°¨ì ìœ¼ë¡œ ì—¬ëŸ¬ë„êµ¬ í˜¸ì¶œ\n",
    "    - ì ì ˆí•œ ê²½ìš° ë³‘ë ¬ ë„êµ¬ í˜¸ì¶œ\n",
    "    - ì´ì „ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë™ì  ë„êµ¬ ì„ íƒ\n",
    "    - ë„êµ¬ ì¬ì‹œë„ ë¡œì§ ë° ì˜¤ë¥˜ ì²˜ë¦¬\n",
    "    - ë„êµ¬ í˜¸ì¶œê°„ ìƒíƒœ ì§€ì†ì„±"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6aa566",
   "metadata": {},
   "source": [
    "ë„êµ¬ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8c8d7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "\n",
    "@tool\n",
    "def search(query: str) -> str:\n",
    "    \"\"\"Search for information.\"\"\"\n",
    "    return f\"Results for: {query}\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"Get weather information for a location.\"\"\"\n",
    "    # ì¼ë¶€ëŸ¬ ì˜¤ë¥˜ ë°œìƒ\n",
    "    raise Exception(\"ë‚ ì”¨ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ì˜€ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "\n",
    "# ë„êµ¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ì—ì´ì „íŠ¸ì— ì „ë‹¬\n",
    "agent = create_agent(\"openai:gpt-4.1-mini\", tools=[search, get_weather])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "591c72a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mmodel\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "ë‚ ì”¨ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ì˜€ìŠµë‹ˆë‹¤.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mException\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mstream_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mHumanMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mì„œìš¸ ë‚ ì”¨ ì¡°íšŒí•´ì¤˜\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/project/langchain-study/.venv/lib/python3.12/site-packages/langchain_teddynote/messages.py:369\u001b[39m, in \u001b[36mstream_graph\u001b[39m\u001b[34m(graph, inputs, config, context, node_names, callback)\u001b[39m\n\u001b[32m    366\u001b[39m prev_node = \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    368\u001b[39m \u001b[38;5;66;03m# LangGraph v1.0ì—ì„œ stream_mode=\"messages\"ëŠ” 2-íŠœí”Œ (message, metadata)ë¥¼ ë°˜í™˜\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m369\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk_msg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mgraph\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    370\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    371\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    372\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# metadataì—ì„œ ë…¸ë“œ ì •ë³´ ì¶”ì¶œ\u001b[39;49;00m\n\u001b[32m    373\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# v1.0ì—ì„œëŠ” 'langgraph_node' í‚¤ë¥¼ í†µí•´ ë…¸ë“œ ì´ë¦„ì— ì ‘ê·¼\u001b[39;49;00m\n\u001b[32m    374\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcurr_node\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlanggraph_node\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    376\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# node_namesê°€ ë¹„ì–´ìˆê±°ë‚˜ í˜„ì¬ ë…¸ë“œê°€ node_namesì— ìˆëŠ” ê²½ìš°ì—ë§Œ ì²˜ë¦¬\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/project/langchain-study/.venv/lib/python3.12/site-packages/langgraph/pregel/main.py:2679\u001b[39m, in \u001b[36mPregel.stream\u001b[39m\u001b[34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[39m\n\u001b[32m   2677\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop.match_cached_writes():\n\u001b[32m   2678\u001b[39m     loop.output_writes(task.id, task.writes, cached=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m2679\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrunner\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtick\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2680\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrites\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2681\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2682\u001b[39m \u001b[43m    \u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m=\u001b[49m\u001b[43mget_waiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2683\u001b[39m \u001b[43m    \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[43m=\u001b[49m\u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43maccept_push\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2684\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   2685\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# emit output\u001b[39;49;00m\n\u001b[32m   2686\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01myield from\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_output\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2687\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubgraphs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mqueue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEmpty\u001b[49m\n\u001b[32m   2688\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2689\u001b[39m loop.after_tick()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/project/langchain-study/.venv/lib/python3.12/site-packages/langgraph/pregel/_runner.py:258\u001b[39m, in \u001b[36mPregelRunner.tick\u001b[39m\u001b[34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[39m\n\u001b[32m    256\u001b[39m \u001b[38;5;66;03m# panic on failure or timeout\u001b[39;00m\n\u001b[32m    257\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m258\u001b[39m     \u001b[43m_panic_or_proceed\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    259\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdone\u001b[49m\u001b[43m.\u001b[49m\u001b[43munion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpanic\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    263\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m tb := exc.__traceback__:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/project/langchain-study/.venv/lib/python3.12/site-packages/langgraph/pregel/_runner.py:520\u001b[39m, in \u001b[36m_panic_or_proceed\u001b[39m\u001b[34m(futs, timeout_exc_cls, panic)\u001b[39m\n\u001b[32m    518\u001b[39m                 interrupts.append(exc)\n\u001b[32m    519\u001b[39m             \u001b[38;5;28;01melif\u001b[39;00m fut \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m SKIP_RERAISE_SET:\n\u001b[32m--> \u001b[39m\u001b[32m520\u001b[39m                 \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m    521\u001b[39m \u001b[38;5;66;03m# raise combined interrupts\u001b[39;00m\n\u001b[32m    522\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m interrupts:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/project/langchain-study/.venv/lib/python3.12/site-packages/langgraph/pregel/_executor.py:80\u001b[39m, in \u001b[36mBackgroundExecutor.done\u001b[39m\u001b[34m(self, task)\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Remove the task from the tasks dict when it's done.\"\"\"\u001b[39;00m\n\u001b[32m     79\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m     \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m GraphBubbleUp:\n\u001b[32m     82\u001b[39m     \u001b[38;5;66;03m# This exception is an interruption signal, not an error\u001b[39;00m\n\u001b[32m     83\u001b[39m     \u001b[38;5;66;03m# so we don't want to re-raise it on exit\u001b[39;00m\n\u001b[32m     84\u001b[39m     \u001b[38;5;28mself\u001b[39m.tasks.pop(task)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/concurrent/futures/thread.py:59\u001b[39m, in \u001b[36m_WorkItem.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     56\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     61\u001b[39m     \u001b[38;5;28mself\u001b[39m.future.set_exception(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/project/langchain-study/.venv/lib/python3.12/site-packages/langgraph/pregel/_retry.py:42\u001b[39m, in \u001b[36mrun_with_retry\u001b[39m\u001b[34m(task, retry_policy, configurable)\u001b[39m\n\u001b[32m     40\u001b[39m     task.writes.clear()\n\u001b[32m     41\u001b[39m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43mproc\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[43m.\u001b[49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     44\u001b[39m     ns: \u001b[38;5;28mstr\u001b[39m = config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/project/langchain-study/.venv/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:656\u001b[39m, in \u001b[36mRunnableSeq.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    654\u001b[39m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[32m    655\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[32m--> \u001b[39m\u001b[32m656\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    657\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    658\u001b[39m     \u001b[38;5;28minput\u001b[39m = step.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/project/langchain-study/.venv/lib/python3.12/site-packages/langgraph/_internal/_runnable.py:400\u001b[39m, in \u001b[36mRunnableCallable.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    398\u001b[39m         run_manager.on_chain_end(ret)\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m     ret = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    401\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.recurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ret.invoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/project/langchain-study/.venv/lib/python3.12/site-packages/langgraph/prebuilt/tool_node.py:716\u001b[39m, in \u001b[36mToolNode._func\u001b[39m\u001b[34m(self, input, config, runtime)\u001b[39m\n\u001b[32m    714\u001b[39m input_types = [input_type] * \u001b[38;5;28mlen\u001b[39m(tool_calls)\n\u001b[32m    715\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m get_executor_for_config(config) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[32m--> \u001b[39m\u001b[32m716\u001b[39m     outputs = \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    717\u001b[39m \u001b[43m        \u001b[49m\u001b[43mexecutor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_one\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_calls\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_types\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_runtimes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    718\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._combine_tool_outputs(outputs, input_type)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/concurrent/futures/_base.py:619\u001b[39m, in \u001b[36mExecutor.map.<locals>.result_iterator\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[32m    617\u001b[39m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[32m    618\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m619\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    620\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    621\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs.pop(), end_time - time.monotonic())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/concurrent/futures/_base.py:317\u001b[39m, in \u001b[36m_result_or_cancel\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    316\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m317\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    318\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    319\u001b[39m         fut.cancel()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/concurrent/futures/_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/concurrent/futures/_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.12.9-macos-aarch64-none/lib/python3.12/concurrent/futures/thread.py:59\u001b[39m, in \u001b[36m_WorkItem.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     56\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m     61\u001b[39m     \u001b[38;5;28mself\u001b[39m.future.set_exception(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/project/langchain-study/.venv/lib/python3.12/site-packages/langchain_core/runnables/config.py:546\u001b[39m, in \u001b[36mContextThreadPoolExecutor.map.<locals>._wrapped_fn\u001b[39m\u001b[34m(*args)\u001b[39m\n\u001b[32m    545\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_wrapped_fn\u001b[39m(*args: Any) -> T:\n\u001b[32m--> \u001b[39m\u001b[32m546\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcontexts\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/project/langchain-study/.venv/lib/python3.12/site-packages/langgraph/prebuilt/tool_node.py:931\u001b[39m, in \u001b[36mToolNode._run_one\u001b[39m\u001b[34m(self, call, input_type, tool_runtime)\u001b[39m\n\u001b[32m    927\u001b[39m config = tool_runtime.config\n\u001b[32m    929\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._wrap_tool_call \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    930\u001b[39m     \u001b[38;5;66;03m# No wrapper - execute directly\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m931\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_tool_sync\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtool_request\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    933\u001b[39m \u001b[38;5;66;03m# Define execute callable that can be called multiple times\u001b[39;00m\n\u001b[32m    934\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mexecute\u001b[39m(req: ToolCallRequest) -> ToolMessage | Command:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/project/langchain-study/.venv/lib/python3.12/site-packages/langgraph/prebuilt/tool_node.py:880\u001b[39m, in \u001b[36mToolNode._execute_tool_sync\u001b[39m\u001b[34m(self, request, input_type, config)\u001b[39m\n\u001b[32m    877\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m    879\u001b[39m     \u001b[38;5;66;03m# Error is handled - create error ToolMessage\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m880\u001b[39m     content = \u001b[43m_handle_tool_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflag\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_tool_errors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    881\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ToolMessage(\n\u001b[32m    882\u001b[39m         content=content,\n\u001b[32m    883\u001b[39m         name=call[\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    884\u001b[39m         tool_call_id=call[\u001b[33m\"\u001b[39m\u001b[33mid\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    885\u001b[39m         status=\u001b[33m\"\u001b[39m\u001b[33merror\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    886\u001b[39m     )\n\u001b[32m    888\u001b[39m \u001b[38;5;66;03m# Process successful response\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/project/langchain-study/.venv/lib/python3.12/site-packages/langgraph/prebuilt/tool_node.py:401\u001b[39m, in \u001b[36m_handle_tool_error\u001b[39m\u001b[34m(e, flag)\u001b[39m\n\u001b[32m    399\u001b[39m     content = flag\n\u001b[32m    400\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(flag):\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m     content = \u001b[43mflag\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore [assignment, call-arg]\u001b[39;00m\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    403\u001b[39m     msg = (\n\u001b[32m    404\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGot unexpected type of `handle_tool_error`. Expected bool, str \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    405\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mor callable. Received: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mflag\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    406\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/project/langchain-study/.venv/lib/python3.12/site-packages/langgraph/prebuilt/tool_node.py:358\u001b[39m, in \u001b[36m_default_handle_tool_errors\u001b[39m\u001b[34m(e)\u001b[39m\n\u001b[32m    356\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, ToolInvocationError):\n\u001b[32m    357\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m e.message\n\u001b[32m--> \u001b[39m\u001b[32m358\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/project/langchain-study/.venv/lib/python3.12/site-packages/langgraph/prebuilt/tool_node.py:833\u001b[39m, in \u001b[36mToolNode._execute_tool_sync\u001b[39m\u001b[34m(self, request, input_type, config)\u001b[39m\n\u001b[32m    831\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    832\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m         response = \u001b[43mtool\u001b[49m\u001b[43m.\u001b[49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcall_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    834\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m ValidationError \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    835\u001b[39m         \u001b[38;5;66;03m# Filter out errors for injected arguments\u001b[39;00m\n\u001b[32m    836\u001b[39m         filtered_errors = _filter_validation_errors(\n\u001b[32m    837\u001b[39m             exc,\n\u001b[32m    838\u001b[39m             \u001b[38;5;28mself\u001b[39m._tool_to_state_args.get(call[\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m], {}),\n\u001b[32m    839\u001b[39m             \u001b[38;5;28mself\u001b[39m._tool_to_store_arg.get(call[\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m]),\n\u001b[32m    840\u001b[39m             \u001b[38;5;28mself\u001b[39m._tool_to_runtime_arg.get(call[\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m]),\n\u001b[32m    841\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/project/langchain-study/.venv/lib/python3.12/site-packages/langchain_core/tools/base.py:598\u001b[39m, in \u001b[36mBaseTool.invoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m    590\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minvoke\u001b[39m(\n\u001b[32m    592\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    595\u001b[39m     **kwargs: Any,\n\u001b[32m    596\u001b[39m ) -> Any:\n\u001b[32m    597\u001b[39m     tool_input, kwargs = _prep_run_args(\u001b[38;5;28minput\u001b[39m, config, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m598\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtool_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/project/langchain-study/.venv/lib/python3.12/site-packages/langchain_core/tools/base.py:904\u001b[39m, in \u001b[36mBaseTool.run\u001b[39m\u001b[34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)\u001b[39m\n\u001b[32m    902\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m error_to_raise:\n\u001b[32m    903\u001b[39m     run_manager.on_tool_error(error_to_raise)\n\u001b[32m--> \u001b[39m\u001b[32m904\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m error_to_raise\n\u001b[32m    905\u001b[39m output = _format_output(content, artifact, tool_call_id, \u001b[38;5;28mself\u001b[39m.name, status)\n\u001b[32m    906\u001b[39m run_manager.on_tool_end(output, color=color, name=\u001b[38;5;28mself\u001b[39m.name, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/project/langchain-study/.venv/lib/python3.12/site-packages/langchain_core/tools/base.py:873\u001b[39m, in \u001b[36mBaseTool.run\u001b[39m\u001b[34m(self, tool_input, verbose, start_color, color, callbacks, tags, metadata, run_name, run_id, config, tool_call_id, **kwargs)\u001b[39m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m config_param := _get_runnable_config_param(\u001b[38;5;28mself\u001b[39m._run):\n\u001b[32m    872\u001b[39m         tool_kwargs |= {config_param: config}\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m     response = \u001b[43mcontext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mtool_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtool_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.response_format == \u001b[33m\"\u001b[39m\u001b[33mcontent_and_artifact\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    875\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(response) != \u001b[32m2\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/project/langchain-study/.venv/lib/python3.12/site-packages/langchain_core/tools/structured.py:90\u001b[39m, in \u001b[36mStructuredTool._run\u001b[39m\u001b[34m(self, config, run_manager, *args, **kwargs)\u001b[39m\n\u001b[32m     88\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m config_param := _get_runnable_config_param(\u001b[38;5;28mself\u001b[39m.func):\n\u001b[32m     89\u001b[39m         kwargs[config_param] = config\n\u001b[32m---> \u001b[39m\u001b[32m90\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     91\u001b[39m msg = \u001b[33m\"\u001b[39m\u001b[33mStructuredTool does not support sync invocation.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(msg)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 15\u001b[39m, in \u001b[36mget_weather\u001b[39m\u001b[34m(location)\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Get weather information for a location.\"\"\"\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# ì¼ë¶€ëŸ¬ ì˜¤ë¥˜ ë°œìƒ\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33më‚ ì”¨ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ì˜€ìŠµë‹ˆë‹¤.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mException\u001b[39m: ë‚ ì”¨ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ì˜€ìŠµë‹ˆë‹¤.",
      "During task with name 'tools' and id '02881af4-d8f1-80c1-6d91-c0d02714d9a5'"
     ]
    }
   ],
   "source": [
    "stream_graph(agent, inputs={\"messages\": [HumanMessage(content=\"ì„œìš¸ ë‚ ì”¨ ì¡°íšŒí•´ì¤˜\")]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec47551",
   "metadata": {},
   "source": [
    "ë„êµ¬ ì˜¤ë¥˜ ì²˜ë¦¬: ì˜¤ë¥˜ ì²˜ë¦¬ë¥¼ ì»¤ìŠ¤í„°ë§ˆì´ì§•í•˜ë ¤ë©´ `@wrap_tool_call` ë°ì½”ë ˆì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¯¸ë“¤ì›¨ì–´ë¥¼ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c1631c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import wrap_tool_call\n",
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "\n",
    "@wrap_tool_call\n",
    "def handle_tool_errors(request, handler):\n",
    "    \"\"\"ë„êµ¬ ì‹¤í–‰ ì˜¤ë¥˜ë¥¼ ì»¤ìŠ¤í…€ ë©”ì‹œì§€ë¡œ ì²˜ë¦¬\"\"\"\n",
    "    try:\n",
    "        return handler(request)\n",
    "    except Exception as e:\n",
    "        # ëª¨ë¸ì— ì»¤ìŠ¤í…€ ì˜¤ë¥˜ ë©”ì‹œì§€ ë°˜í™˜\n",
    "        return ToolMessage(\n",
    "            content=f\"[ì—ëŸ¬] ë„êµ¬ í˜¸ì¶œì‹œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì–´ìš”. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”. ({str(e)})\",\n",
    "            tool_call_id=request.tool_call[\"id\"],\n",
    "        )\n",
    "\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"openai:gpt-4.1-mini\",\n",
    "    tools=[search, get_weather],\n",
    "    middleware=[handle_tool_errors],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de07b8e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mmodel\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mtools\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "[ì—ëŸ¬] ë„êµ¬ í˜¸ì¶œì‹œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì–´ìš”. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”. (ë‚ ì”¨ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ì˜€ìŠµë‹ˆë‹¤.)\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mmodel\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "í˜„ì¬ ë‚ ì”¨ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ì— ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì‹œê² ì–´ìš”?"
     ]
    }
   ],
   "source": [
    "stream_graph(agent, inputs={\"messages\": [HumanMessage(content=\"ì„œìš¸ ë‚ ì”¨ ì¡°íšŒí•´ì¤˜\")]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40c74a8",
   "metadata": {},
   "source": [
    "### í”„ë¡¬í”„íŠ¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f7f7d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_agent(\n",
    "    \"openai:gpt-4.1-mini\",\n",
    "    tools=[search, get_weather],\n",
    "    system_prompt=\"You are a helpful assistant. Be concise and accurate.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45883f3",
   "metadata": {},
   "source": [
    "ë™ì  ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸: ëŸ°íƒ€ì„ ì»¨í…ìŠ¤íŠ¸ë‚˜ ì—ì´ì „íŠ¸ ìƒíƒœë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ìˆ˜ì •í•´ì•¼ í•˜ëŠ” ê²½ìš° ë¯¸ë“¤ì›¨ì–´ë¥¼ ì‚¬ìš© ê°€ëŠ¥\n",
    "\n",
    "- í•µì‹¬: `request.runtime.context`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "071a71b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import dynamic_prompt, ModelRequest\n",
    "\n",
    "\n",
    "class Context(TypedDict):\n",
    "    answer_type: str\n",
    "\n",
    "\n",
    "@dynamic_prompt\n",
    "def user_role_prompt(request: ModelRequest) -> str:\n",
    "    \"\"\"ì‚¬ìš©ì ì—­í• ì— ë”°ë¼ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±\"\"\"\n",
    "    answer_type = request.runtime.context.get(\"answer_type\", \"default\")\n",
    "    base_prompt = \"You are a helpful assistant.\"\n",
    "\n",
    "    if answer_type == \"default\":\n",
    "        return f\"{base_prompt} Answer in Korean. ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ì¤˜.\"\n",
    "    elif answer_type == \"sns\":\n",
    "        return f\"{base_prompt} Answer in Korean. SNS í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì¤˜.\"\n",
    "    elif answer_type == \"article\":\n",
    "        return f\"{base_prompt} Answer in Korean. ë‰´ìŠ¤ ê¸°ì‚¬ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì¤˜.\"\n",
    "\n",
    "    return base_prompt\n",
    "\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"openai:gpt-4.1-mini\",\n",
    "    tools=[search],\n",
    "    middleware=[user_role_prompt],\n",
    "    context_schema=Context,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ecae0e3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mmodel\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "ğŸ“¢ ë¨¸ì‹ ëŸ¬ë‹ ë™ì‘ ì›ë¦¬ ê°„ë‹¨ ì •ë¦¬!\n",
      "\n",
      "1ï¸âƒ£ ë°ì´í„° ìˆ˜ì§‘: í•™ìŠµí•  ë°ì´í„°ë¥¼ ëª¨ì•„ìš”  \n",
      "2ï¸âƒ£ ì „ì²˜ë¦¬: ë°ì´í„° ì •ë¦¬ ë° ê°€ê³µ  \n",
      "3ï¸âƒ£ ëª¨ë¸ ì„ íƒ: ë¬¸ì œì— ë§ëŠ” ì•Œê³ ë¦¬ì¦˜ ì„ íƒ  \n",
      "4ï¸âƒ£ í•™ìŠµ: ëª¨ë¸ì´ ë°ì´í„°ë¥¼ ë³´ê³  íŒ¨í„´ì„ ë°°ì›Œìš”  \n",
      "5ï¸âƒ£ í‰ê°€: ì„±ëŠ¥ í…ŒìŠ¤íŠ¸í•˜ì—¬ ì–¼ë§ˆë‚˜ ì˜ ë°°ì› ëŠ”ì§€ í™•ì¸  \n",
      "6ï¸âƒ£ ì˜ˆì¸¡/ì ìš©: ìƒˆë¡œìš´ ë°ì´í„°ì— ëª¨ë¸ì„ ì ìš©í•´ ê²°ê³¼ ë„ì¶œ!\n",
      "\n",
      "ì‰½ê²Œ ë§í•˜ë©´, ê³¼ê±° ë°ì´í„°ë¥¼ í†µí•´ ìŠ¤ìŠ¤ë¡œ í•™ìŠµí•˜ê³  ë¯¸ë˜ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê¸°ìˆ ! ğŸ’»âœ¨\n",
      "\n",
      "#ë¨¸ì‹ ëŸ¬ë‹ #ì¸ê³µì§€ëŠ¥ #ë°ì´í„°ê³¼í•™"
     ]
    }
   ],
   "source": [
    "# ì»¨í…ìŠ¤íŠ¸ì— ë”°ë¼ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ê°€ ë™ì ìœ¼ë¡œ ì„¤ì •ë©ë‹ˆë‹¤\n",
    "stream_graph(\n",
    "    agent,\n",
    "    inputs={\n",
    "        \"messages\": [HumanMessage(content=\"ë¨¸ì‹ ëŸ¬ë‹ì˜ ë™ì‘ ì›ë¦¬ì— ëŒ€í•´ì„œ ì„¤ëª…í•´ì¤˜\")]\n",
    "    },\n",
    "    context=Context(answer_type=\"sns\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9870bfa1",
   "metadata": {},
   "source": [
    "### ì—ì´ì „íŠ¸ í˜¸ì¶œ\n",
    "ì—ì´ì „íŠ¸ ìƒíƒœì— ì—…ë°ì´íŠ¸ë¥¼ ì „ë‹¬í•˜ì—¬ í˜¸ì¶œ ê°€ëŠ¥  \n",
    "ëª¨ë“  ì—ì´ì „íŠ¸ëŠ” ìƒíƒœì— ë©”ì‹œì§€ ì‹œí€€ìŠ¤ë¥¼ í¬í•¨??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ead6062f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content=\"What's the weather in San Fransisco?\", additional_kwargs={}, response_metadata={}, id='a1aa001d-9b4a-4402-9a18-c0f7847e74fb'), AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 15, 'prompt_tokens': 66, 'total_tokens': 81, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_4c2851f862', 'id': 'chatcmpl-CYoFAUrBZ9YROwiSUkrawwNqdrpoc', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--40d38361-4282-4bb3-bbd1-b103711471a5-0', tool_calls=[{'name': 'search', 'args': {'query': 'San Francisco weather'}, 'id': 'call_5dADlLdv6mowQgmk5WAbuzcA', 'type': 'tool_call'}], usage_metadata={'input_tokens': 66, 'output_tokens': 15, 'total_tokens': 81, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), ToolMessage(content='Results for: San Francisco weather', name='search', id='0e52aeec-0ab2-4ac1-a2d2-24be80b7ff01', tool_call_id='call_5dADlLdv6mowQgmk5WAbuzcA'), AIMessage(content='ìƒŒí”„ë€ì‹œìŠ¤ì½”ì˜ í˜„ì¬ ë‚ ì”¨ ì •ë³´ë¥¼ ì•Œë ¤ë©´ êµ¬ì²´ì ì¸ ë‚ ì§œì™€ ì‹œê°„ì„ ì•Œë ¤ì£¼ì„¸ìš”. ì¼ë°˜ì ì¸ ê¸°í›„ ì •ë³´ê°€ í•„ìš”í•˜ì‹œë©´ ë§ì”€í•´ ì£¼ì„¸ìš”.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 37, 'prompt_tokens': 94, 'total_tokens': 131, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4.1-mini-2025-04-14', 'system_fingerprint': 'fp_4c2851f862', 'id': 'chatcmpl-CYoFEVQA4yEo4sJvDOvKJdG5aieCe', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--50f5e792-ed95-48e8-8168-f9772f892c24-0', usage_metadata={'input_tokens': 94, 'output_tokens': 37, 'total_tokens': 131, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}\n"
     ]
    }
   ],
   "source": [
    "result = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"What's the weather in San Fransisco?\"}]},\n",
    "    context={\"user_role\": \"expert\"}\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e468aba5",
   "metadata": {},
   "source": [
    "### ê³ ê¸‰ ê°œë…"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58c9e97",
   "metadata": {},
   "source": [
    "### êµ¬ì¡°í™”ëœ ì¶œë ¥  \n",
    "- íŠ¹ì • í˜•ì‹ìœ¼ë¡œ ì—ì´ì „íŠ¸ ì¶œë ¥ì„ ë°˜í™˜í•˜ê³  ì‹¶ì„ë•Œ, response_format ë§¤ê°œë³€ìˆ˜ë¥¼ í†µí•´ êµ¬ì¡°í™”ëœ ì¶œë ¥ ì œê³µ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fe4a3a",
   "metadata": {},
   "source": [
    "ToolStrategy: ì¸ê³µì ì¸ ë„êµ¬ í˜¸ì¶œì„ í†µí•´ êµ¬ì¡°í™”ëœ ì¶œë ¥ ìƒì„±"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1489890",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from langchain.agents import create_agent\n",
    "from langchain.agents.structured_output import ToolStrategy\n",
    "\n",
    "# ì‘ë‹µ ìŠ¤í‚¤ë§ˆ ì •ì˜\n",
    "class ContactInfo(BaseModel):\n",
    "    name: str\n",
    "    email: str\n",
    "    phone: str\n",
    "\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"openai:gpt-4.1-mini\", tools=[], response_format=ToolStrategy(ContactInfo)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "960037a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='John Doe' email='john@example.com' phone='(555) 123-4567'\n"
     ]
    }
   ],
   "source": [
    "result = agent.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Extract contact info from: John Doe, john@example.com, (555) 123-4567\",\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "print(result[\"structured_response\"])\n",
    "# ContactInfo(name='John Doe', email='john@example.com', phone='(555) 123-4567')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0fb2d55",
   "metadata": {},
   "source": [
    "ProviderStrategy: ëª¨ë¸ ì œê³µìì˜ ë„¤ì´í‹°ë¸Œ êµ¬ì¡°í™”ëœ ì¶œë ¥ ì‚¬ìš©. ë” ì•ˆì •ì ì´ì§€ë§Œ êµ¬ì¡°í™”ëœ ì¶œë ¥ì„ ì§€ì›í•˜ëŠ” ëª¨ë¸ë§Œ ì‘ë™(ì˜ˆ: openai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9679e323",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.structured_output import ProviderStrategy\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"openai:gpt-4.1\", response_format=ProviderStrategy(ContactInfo)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "beb1a635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='John Doe' email='john@example.com' phone='(555) 123-4567'\n"
     ]
    }
   ],
   "source": [
    "result = agent.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Extract contact info from: John Doe, john@example.com, (555) 123-4567\",\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "print(result[\"structured_response\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952a6767",
   "metadata": {},
   "source": [
    "### ë©”ëª¨ë¦¬\n",
    "ì»¤ìŠ¤í…€ ìƒíƒœ ìŠ¤í‚¤ë§ˆë¥¼ ì‚¬ìš©í•˜ì—¬ ë©”ëª¨ë¦¬ ê´€ë¦¬ ê°€ëŠ¥"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8cb6af",
   "metadata": {},
   "source": [
    "### ë¯¸ë“¤ì›¨ì–´ë¥¼ í†µí•­ ìƒíƒœ ì •ì˜\n",
    "ì»¤ìŠ¤í…€ ìƒíƒœê°€ íŠ¹ì • ë¯¸ë“¤ì›¨ì–´ í›… ë° í•´ë‹¹ ë¯¸ë“¤ì›¨ì–´ì— ì—°ê²°ëœ ë„êµ¬ì—ì„œ ì•¡ì„¸ìŠ¤í•´ì•¼ í•˜ëŠ” ê²½ìš° ë¯¸ë“¤ì›¨ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ ì»¤ìŠ¤í…€ ìƒíƒœë¥¼ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9948793f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from langchain.agents import AgentState\n",
    "from langchain.agents.middleware import AgentMiddleware\n",
    "\n",
    "\n",
    "# ì»¤ìŠ¤í…€ ìƒíƒœ ìŠ¤í‚¤ë§ˆ ì •ì˜\n",
    "class CustomState(AgentState):\n",
    "    user_preferences: dict\n",
    "\n",
    "\n",
    "class CustomMiddleware(AgentMiddleware):\n",
    "    state_schema = CustomState\n",
    "    tools = []\n",
    "\n",
    "    def before_model(self, state: CustomState, runtime) -> dict[str, Any] | None:\n",
    "        # ëª¨ë¸ í˜¸ì¶œ ì „ ì»¤ìŠ¤í…€ ë¡œì§\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cb388706",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_agent(\"openai:gpt-4.1-mini\", tools=[], middleware=[CustomMiddleware()])\n",
    "\n",
    "# ì—ì´ì „íŠ¸ëŠ” ì´ì œ ë©”ì‹œì§€ ì™¸ì— ì¶”ê°€ ìƒíƒœë¥¼ ì¶”ì í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\n",
    "result = agent.invoke(\n",
    "    {\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"I prefer technical explanations\"}],\n",
    "        \"user_preferences\": {\"style\": \"technical\", \"verbosity\": \"detailed\"},\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7351af8",
   "metadata": {},
   "source": [
    "### State_schemaë¥¼ í†µí•œ ìƒíƒœ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3b35a389",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentState\n",
    "\n",
    "\n",
    "class CustomState(AgentState):\n",
    "    user_preferences: dict\n",
    "\n",
    "\n",
    "agent = create_agent(\"openai:gpt-4.1-mini\", tools=[], state_schema=CustomState)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e8394866",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_agent(\"openai:gpt-4.1-mini\", tools=[], state_schema=CustomState)\n",
    "\n",
    "result = agent.invoke(\n",
    "    {\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"I prefer technical explanations\"}],\n",
    "        \"user_preferences\": {\"style\": \"technical\", \"verbosity\": \"detailed\"},\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb6a20d",
   "metadata": {},
   "source": [
    "### ìŠ¤íŠ¸ë¦¬ë°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "02cc10d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent: Search for AI news and summarize the findings\n",
      "Agent: I don't have real-time internet access to search for the latest AI news. However, I can provide a summary of recent trends and developments in AI based on information available up to mid-2024. If you are looking for the very latest updates, I recommend checking trusted news sources like AI-focused blogs, technology news websites, or official announcements from leading AI companies.\n",
      "\n",
      "Hereâ€™s a summary of key recent trends and developments in AI:\n",
      "\n",
      "1. **Generative AI Advancements**: Large language models (LLMs) and generative AI tools continue to improve, with companies releasing more powerful and efficient versions. These models are being integrated into various applications, including coding assistants, content creation, and customer service.\n",
      "\n",
      "2. **AI in Healthcare**: AI is increasingly used for diagnostics, drug discovery, personalized medicine, and managing healthcare data. New studies show promising results in using AI to predict diseases earlier and more accurately.\n",
      "\n",
      "3. **Regulation and Ethics**: Governments worldwide are focusing on AI regulation, emphasizing transparency, fairness, and privacy. There is growing discussion on setting standards for AI accountability and mitigating biases.\n",
      "\n",
      "4. **AI and Robotics**: Integration of AI in robotics is enabling more sophisticated automation in industries such as manufacturing, logistics, and even home assistance.\n",
      "\n",
      "5. **Multimodal AI Systems**: AI systems capable of understanding and generating content across multiple modalities (text, image, audio, video) are gaining traction, leading to more versatile applications.\n",
      "\n",
      "If you want, I can provide detailed explanations or examples on any of these topics!\n"
     ]
    }
   ],
   "source": [
    "for chunk in agent.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"Search for AI news and summarize the findings\"}\n",
    "        ]\n",
    "    },\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    # ê° ì²­í¬ì—ëŠ” í•´ë‹¹ ì‹œì ì˜ ì „ì²´ ìƒíƒœê°€ í¬í•¨ë©ë‹ˆë‹¤\n",
    "    latest_message = chunk[\"messages\"][-1]\n",
    "    if latest_message.content:\n",
    "        print(f\"Agent: {latest_message.content}\")\n",
    "    elif hasattr(latest_message, \"tool_calls\") and latest_message.tool_calls:\n",
    "        print(f\"Calling tools: {[tc['name'] for tc in latest_message.tool_calls]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a968ab7d",
   "metadata": {},
   "source": [
    "### ë¯¸ë“¤ì›¨ì–´\n",
    "\n",
    "ë¯¸ë“¤ì›¨ì–´ëŠ” ì‹¤í–‰ì˜ ë‹¤ì–‘í•œ ë‹¨ê³„ì—ì„œ ì—ì´ì „íŠ¸ ë™ì‘ì„ ì»¤ìŠ¤í„°ë§ˆì´ì§•í•˜ê¸° ìœ„í•œ ê°•ë ¥í•œ í™•ì¥ì„±ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
    "\n",
    "**ë¯¸ë“¤ì›¨ì–´ ì‚¬ìš© ì‚¬ë¡€:**\n",
    "\n",
    "- ëª¨ë¸ í˜¸ì¶œ ì „ ìƒíƒœ ì²˜ë¦¬ (ì˜ˆ: ë©”ì‹œì§€ íŠ¸ë¦¬ë°, ì»¨í…ìŠ¤íŠ¸ ì£¼ì…)\n",
    "- ëª¨ë¸ ì‘ë‹µ ìˆ˜ì • ë˜ëŠ” ê²€ì¦ (ì˜ˆ: ê°€ë“œë ˆì¼, ì½˜í…ì¸  í•„í„°ë§)\n",
    "- ì»¤ìŠ¤í…€ ë¡œì§ìœ¼ë¡œ ë„êµ¬ ì‹¤í–‰ ì˜¤ë¥˜ ì²˜ë¦¬\n",
    "- ìƒíƒœ ë˜ëŠ” ì»¨í…ìŠ¤íŠ¸ì— ë”°ë¥¸ ë™ì  ëª¨ë¸ ì„ íƒ\n",
    "- ì»¤ìŠ¤í…€ ë¡œê¹…, ëª¨ë‹ˆí„°ë§ ë˜ëŠ” ë¶„ì„ ì¶”ê°€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a8d9321",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
