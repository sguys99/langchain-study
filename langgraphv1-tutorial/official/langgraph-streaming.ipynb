{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LangChain 스트리밍\n",
        "\n",
        "스트리밍을 사용하면 LLM의 출력이 생성되는 대로 점진적으로 표시할 수 있어 더 나은 사용자 경험을 제공합니다. LangChain 에이전트는 기본적으로 스트리밍 모드로 실행되며, 실시간으로 응답을 제공합니다.\n",
        "\n",
        "여러 가지 스트리밍 모드가 있으며, 각각 다른 종류의 정보를 제공합니다:\n",
        "\n",
        "- **`updates`**: 에이전트의 진행 상황 (기본값)\n",
        "- **`messages`**: LLM의 토큰 스트리밍\n",
        "- **`custom`**: 커스텀 업데이트"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 사전 준비\n",
        "\n",
        "환경 변수를 설정합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv(override=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 에이전트 진행 상황 스트리밍\n",
        "\n",
        "`stream_mode=\"updates\"`는 에이전트의 진행 상황을 추적하는 기본 스트리밍 모드입니다. 각 노드가 실행을 완료할 때마다 업데이트를 생성합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.agents import create_agent\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.tools import tool\n",
        "\n",
        "@tool\n",
        "def get_weather(city: str) -> str:\n",
        "    \"\"\"Get the weather for a specific city.\"\"\"\n",
        "    return f\"The weather in {city} is sunny!\"\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4.1-mini\")\n",
        "agent = create_agent(model=model, tools=[get_weather])\n",
        "\n",
        "# 기본 스트리밍 (updates 모드)\n",
        "for chunk in agent.stream({\"messages\": [{\"role\": \"user\", \"content\": \"What's the weather in Seoul?\"}]}):\n",
        "    print(chunk)\n",
        "    print(\"---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LLM 토큰 스트리밍\n",
        "\n",
        "`stream_mode=\"messages\"`를 사용하면 LLM이 생성하는 토큰을 실시간으로 스트리밍할 수 있습니다. 이는 사용자에게 즉각적인 피드백을 제공하는 데 유용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# messages 스트리밍 모드\n",
        "for chunk in agent.stream(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": \"Tell me a short story about a robot.\"}]},\n",
        "    stream_mode=\"messages\"  # LLM 토큰 스트리밍\n",
        "):\n",
        "    print(chunk, end=\"\", flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 실용적인 예제: 타자기 효과\n",
        "\n",
        "LLM 토큰을 스트리밍하여 타자기처럼 텍스트를 출력하는 실용적인 예제입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import time\n",
        "\n",
        "print(\"AI: \", end=\"\", flush=True)\n",
        "\n",
        "for chunk in agent.stream(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": \"Write a haiku about technology.\"}]},\n",
        "    stream_mode=\"messages\"\n",
        "):\n",
        "    # AIMessageChunk에서 텍스트 추출\n",
        "    if hasattr(chunk, 'content'):\n",
        "        print(chunk.content, end=\"\", flush=True)\n",
        "        time.sleep(0.02)  # 타자기 효과를 위한 약간의 지연\n",
        "\n",
        "print()  # 줄바꿈"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 커스텀 업데이트\n",
        "\n",
        "`get_stream_writer()`를 사용하면 에이전트 실행 중 커스텀 업데이트를 스트리밍할 수 있습니다. 이는 진행 상황, 중간 결과 또는 디버그 정보를 전송하는 데 유용합니다.\n",
        "\n",
        "커스텀 업데이트는 `stream_mode=\"custom\"`으로 수신할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.tools import tool, ToolRuntime\n",
        "\n",
        "@tool\n",
        "def process_data(data_size: int, runtime: ToolRuntime) -> str:\n",
        "    \"\"\"Process data with progress updates.\"\"\"\n",
        "    writer = runtime.get_stream_writer()\n",
        "    \n",
        "    # 진행 상황을 커스텀 업데이트로 전송\n",
        "    for i in range(0, data_size, 10):\n",
        "        progress = min(i + 10, data_size)\n",
        "        writer({\"progress\": progress, \"total\": data_size})\n",
        "    \n",
        "    return f\"Processed {data_size} items successfully!\"\n",
        "\n",
        "agent = create_agent(model=model, tools=[process_data])\n",
        "\n",
        "# 커스텀 스트리밍 모드로 진행 상황 추적\n",
        "for chunk in agent.stream(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": \"Process 50 items of data\"}]},\n",
        "    stream_mode=\"custom\"  # 커스텀 업데이트 수신\n",
        "):\n",
        "    if \"progress\" in chunk:\n",
        "        percentage = (chunk[\"progress\"] / chunk[\"total\"]) * 100\n",
        "        print(f\"Progress: {chunk['progress']}/{chunk['total']} ({percentage:.0f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 다중 스트리밍 모드\n",
        "\n",
        "여러 스트리밍 모드를 동시에 사용할 수 있습니다. 리스트로 전달하면 각 모드의 업데이트를 모두 받을 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 여러 스트리밍 모드 동시 사용\n",
        "for chunk in agent.stream(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": \"Process 30 items\"}]},\n",
        "    stream_mode=[\"updates\", \"custom\"]  # 여러 모드 동시 사용\n",
        "):\n",
        "    # chunk는 (stream_mode, data) 튜플 형태\n",
        "    mode, data = chunk\n",
        "    \n",
        "    if mode == \"updates\":\n",
        "        print(f\"[UPDATE] Node completed: {list(data.keys())}\")\n",
        "    elif mode == \"custom\":\n",
        "        if \"progress\" in data:\n",
        "            print(f\"[PROGRESS] {data['progress']}/{data['total']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 스트리밍 비활성화\n",
        "\n",
        "개별 모델 또는 도구에 대해 스트리밍을 비활성화하려면 해당 객체를 생성할 때 `streaming=False`를 설정합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "# 스트리밍 비활성화된 모델\n",
        "non_streaming_model = ChatOpenAI(model=\"gpt-4.1-mini\", streaming=False)\n",
        "\n",
        "agent = create_agent(model=non_streaming_model, tools=[get_weather])\n",
        "\n",
        "# messages 모드를 사용해도 토큰이 스트리밍되지 않음\n",
        "for chunk in agent.stream(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": \"What's the weather in Tokyo?\"}]},\n",
        "    stream_mode=\"messages\"\n",
        "):\n",
        "    # 전체 응답이 한 번에 전달됨\n",
        "    print(chunk)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 종합 예제: 진행률 바가 있는 데이터 처리\n",
        "\n",
        "여러 스트리밍 기능을 결합한 실용적인 예제입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "from langchain.tools import tool, ToolRuntime\n",
        "from langchain.agents import create_agent\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "@tool\n",
        "def analyze_data(dataset_name: str, num_records: int, runtime: ToolRuntime) -> str:\n",
        "    \"\"\"Analyze a dataset with detailed progress reporting.\"\"\"\n",
        "    writer = runtime.get_stream_writer()\n",
        "    \n",
        "    # 단계별 분석 프로세스\n",
        "    steps = [\n",
        "        (\"loading\", \"Loading data\", 0.2),\n",
        "        (\"cleaning\", \"Cleaning data\", 0.3),\n",
        "        (\"processing\", \"Processing data\", 0.3),\n",
        "        (\"finalizing\", \"Finalizing results\", 0.2)\n",
        "    ]\n",
        "    \n",
        "    for step_name, step_desc, duration in steps:\n",
        "        writer({\n",
        "            \"step\": step_name,\n",
        "            \"description\": step_desc,\n",
        "            \"status\": \"started\"\n",
        "        })\n",
        "        \n",
        "        time.sleep(duration)  # 작업 시뮬레이션\n",
        "        \n",
        "        writer({\n",
        "            \"step\": step_name,\n",
        "            \"description\": step_desc,\n",
        "            \"status\": \"completed\"\n",
        "        })\n",
        "    \n",
        "    return f\"Successfully analyzed {num_records} records from {dataset_name}!\"\n",
        "\n",
        "@tool\n",
        "def get_summary(analysis_result: str, runtime: ToolRuntime) -> str:\n",
        "    \"\"\"Generate a summary of the analysis.\"\"\"\n",
        "    return f\"Analysis complete. {analysis_result}\"\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4.1-mini\")\n",
        "agent = create_agent(model=model, tools=[analyze_data, get_summary])\n",
        "\n",
        "print(\"Starting data analysis...\\n\")\n",
        "\n",
        "# 다중 스트리밍 모드로 실행\n",
        "for chunk in agent.stream(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": \"Analyze the sales dataset with 1000 records\"}]},\n",
        "    stream_mode=[\"custom\", \"updates\"]\n",
        "):\n",
        "    mode, data = chunk\n",
        "    \n",
        "    if mode == \"custom\":\n",
        "        # 커스텀 진행 상황 표시\n",
        "        if \"step\" in data:\n",
        "            status_icon = \"✓\" if data[\"status\"] == \"completed\" else \"→\"\n",
        "            print(f\"{status_icon} {data['description']}... {data['status']}\")\n",
        "    \n",
        "    elif mode == \"updates\":\n",
        "        # 노드 완료 정보 (선택적으로 표시)\n",
        "        if \"messages\" in data:\n",
        "            last_msg = data[\"messages\"][-1]\n",
        "            if hasattr(last_msg, \"content\") and last_msg.content:\n",
        "                print(f\"\\n[Result] {last_msg.content}\")\n",
        "\n",
        "print(\"\\nAnalysis finished!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 실전 팁\n",
        "\n",
        "### 1. 적절한 스트리밍 모드 선택\n",
        "\n",
        "- **`updates`**: 에이전트의 전체적인 진행 상황을 추적할 때\n",
        "- **`messages`**: 사용자에게 LLM의 응답을 실시간으로 보여줄 때\n",
        "- **`custom`**: 복잡한 작업의 세부 진행 상황을 보고할 때"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 채팅 인터페이스에 적합한 설정\n",
        "def chat_interface():\n",
        "    for chunk in agent.stream(\n",
        "        {\"messages\": [{\"role\": \"user\", \"content\": \"Hello!\"}]},\n",
        "        stream_mode=\"messages\"  # 실시간 응답 표시\n",
        "    ):\n",
        "        if hasattr(chunk, 'content'):\n",
        "            yield chunk.content\n",
        "\n",
        "# 백그라운드 작업 모니터링에 적합한 설정\n",
        "def background_task():\n",
        "    for chunk in agent.stream(\n",
        "        {\"messages\": [{\"role\": \"user\", \"content\": \"Process data\"}]},\n",
        "        stream_mode=[\"updates\", \"custom\"]  # 진행 상황 추적\n",
        "    ):\n",
        "        mode, data = chunk\n",
        "        # 진행 상황을 데이터베이스나 로그에 기록\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. 에러 처리\n",
        "\n",
        "스트리밍 중 에러가 발생할 수 있으므로 적절한 에러 처리가 중요합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "try:\n",
        "    for chunk in agent.stream(\n",
        "        {\"messages\": [{\"role\": \"user\", \"content\": \"Test query\"}]},\n",
        "        stream_mode=\"messages\"\n",
        "    ):\n",
        "        print(chunk, end=\"\", flush=True)\n",
        "except Exception as e:\n",
        "    print(f\"\\nError during streaming: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. 성능 최적화\n",
        "\n",
        "불필요한 스트리밍 모드를 사용하지 않으면 성능이 향상됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 좋은 예: 필요한 모드만 사용\n",
        "for chunk in agent.stream(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": \"Query\"}]},\n",
        "    stream_mode=\"messages\"  # 필요한 모드만\n",
        "):\n",
        "    pass\n",
        "\n",
        "# 나쁜 예: 모든 모드 사용 (불필요한 오버헤드)\n",
        "# for chunk in agent.stream(\n",
        "#     {\"messages\": [{\"role\": \"user\", \"content\": \"Query\"}]},\n",
        "#     stream_mode=[\"updates\", \"messages\", \"custom\"]\n",
        "# ):\n",
        "#     pass"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
