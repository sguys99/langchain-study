{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain ì—ì´ì „íŠ¸\n",
    "\n",
    "ì—ì´ì „íŠ¸ëŠ” ì–¸ì–´ ëª¨ë¸ê³¼ ë„êµ¬ë¥¼ ê²°í•©í•˜ì—¬ ì‘ì—…ì— ëŒ€í•´ ì¶”ë¡ í•˜ê³ , ì‚¬ìš©í•  ë„êµ¬ë¥¼ ê²°ì •í•˜ë©°, ì†”ë£¨ì…˜ì„ í–¥í•´ ë°˜ë³µì ìœ¼ë¡œ ì‘ì—…í•  ìˆ˜ ìˆëŠ” ì‹œìŠ¤í…œì„ ë§Œë“­ë‹ˆë‹¤.\n",
    "\n",
    "`create_agent`ëŠ” í”„ë¡œë•ì…˜ ìˆ˜ì¤€ì˜ ì—ì´ì „íŠ¸ êµ¬í˜„ì„ ì œê³µí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì‚¬ì „ ì¤€ë¹„\n",
    "\n",
    "í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith ì¶”ì ì„ ì‹œì‘í•©ë‹ˆë‹¤.\n",
      "[í”„ë¡œì íŠ¸ëª…]\n",
      "LangChain-V1-Tutorial\n"
     ]
    }
   ],
   "source": [
    "from langchain_teddynote import logging\n",
    "\n",
    "logging.langsmith(\"LangChain-V1-Tutorial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì—ì´ì „íŠ¸ì˜ ì‘ë™ ë°©ì‹\n",
    "\n",
    "ì—ì´ì „íŠ¸ëŠ” ëª©í‘œë¥¼ ë‹¬ì„±í•˜ê¸° ìœ„í•´ ë„êµ¬ë¥¼ ë°˜ë³µì ìœ¼ë¡œ ì‹¤í–‰í•©ë‹ˆë‹¤. ì—ì´ì „íŠ¸ëŠ” ì¤‘ë‹¨ ì¡°ê±´ì´ ì¶©ì¡±ë  ë•Œê¹Œì§€ ì‹¤í–‰ë˜ë©°, ëª¨ë¸ì´ ìµœì¢… ì¶œë ¥ì„ ìƒì„±í•˜ê±°ë‚˜ ë°˜ë³µ ì œí•œì— ë„ë‹¬í•˜ë©´ ì¤‘ë‹¨ë©ë‹ˆë‹¤.\n",
    "\n",
    "```\n",
    "ì…ë ¥ -> ëª¨ë¸ -> ë„êµ¬ í˜¸ì¶œ -> ë„êµ¬ ì‹¤í–‰ -> ëª¨ë¸ -> ... -> ìµœì¢… ì¶œë ¥\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ëª¨ë¸ (Model)\n",
    "\n",
    "ëª¨ë¸ì€ ì—ì´ì „íŠ¸ì˜ ì¶”ë¡  ì—”ì§„ì…ë‹ˆë‹¤. ì •ì  ë˜ëŠ” ë™ì  ë°©ì‹ìœ¼ë¡œ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ì •ì  ëª¨ë¸\n",
    "\n",
    "ì •ì  ëª¨ë¸ì€ ì—ì´ì „íŠ¸ ìƒì„± ì‹œ í•œ ë²ˆ êµ¬ì„±ë˜ë©° ì‹¤í–‰ ì¤‘ì—ëŠ” ë³€ê²½ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "# ëª¨ë¸ ì‹ë³„ì ë¬¸ìì—´ì„ ì‚¬ìš©í•œ ê°„ë‹¨í•œ ë°©ë²•\n",
    "agent = create_agent(\"openai:gpt-4.1-mini\", tools=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# ëª¨ë¸ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì§ì ‘ ì´ˆê¸°í™”í•˜ì—¬ ë” ì„¸ë°€í•œ ì œì–´\n",
    "model = ChatOpenAI(\n",
    "    model=\"gpt-4.1-mini\",\n",
    "    temperature=0.1,  # ì‘ë‹µì˜ ë¬´ì‘ìœ„ì„± ì œì–´\n",
    "    max_tokens=1000,  # ìµœëŒ€ ìƒì„± í† í° ìˆ˜\n",
    "    timeout=30,  # ìš”ì²­ íƒ€ì„ì•„ì›ƒ(ì´ˆ)\n",
    ")\n",
    "\n",
    "agent = create_agent(model, tools=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ë™ì  ëª¨ë¸\n",
    "\n",
    "ë™ì  ëª¨ë¸ì€ ëŸ°íƒ€ì„ì— í˜„ì¬ ìƒíƒœì™€ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì„ íƒë©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ì •êµí•œ ë¼ìš°íŒ… ë¡œì§ê³¼ ë¹„ìš© ìµœì í™”ê°€ ê°€ëŠ¥í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import wrap_model_call, ModelRequest, ModelResponse\n",
    "\n",
    "# ê¸°ë³¸ ëª¨ë¸ê³¼ ê³ ê¸‰ ëª¨ë¸ ì •ì˜\n",
    "basic_model = ChatOpenAI(model=\"gpt-4.1-mini\")\n",
    "advanced_model = ChatOpenAI(model=\"gpt-4.1\")\n",
    "\n",
    "\n",
    "@wrap_model_call\n",
    "def dynamic_model_selection(request: ModelRequest, handler) -> ModelResponse:\n",
    "    \"\"\"ëŒ€í™” ë³µì¡ë„ì— ë”°ë¼ ëª¨ë¸ ì„ íƒ\"\"\"\n",
    "    print(request)\n",
    "    message_count = len(request.state[\"messages\"])\n",
    "\n",
    "    # ê¸´ ëŒ€í™”ì—ëŠ” ê³ ê¸‰ ëª¨ë¸ ì‚¬ìš©\n",
    "    if message_count > 10:\n",
    "        model = advanced_model\n",
    "    else:\n",
    "        model = basic_model\n",
    "\n",
    "    request.model = model\n",
    "    return handler(request)\n",
    "\n",
    "\n",
    "agent = create_agent(\n",
    "    model=basic_model, tools=[], middleware=[dynamic_model_selection]  # ê¸°ë³¸ ëª¨ë¸\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelRequest(model=ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x11700b890>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x11700db50>, root_client=<openai.OpenAI object at 0x11700a310>, root_async_client=<openai.AsyncOpenAI object at 0x11700ced0>, model_name='gpt-4.1-mini', model_kwargs={}, openai_api_key=SecretStr('**********'), stream_usage=True), system_prompt=None, messages=[HumanMessage(content='ë¨¸ì‹ ëŸ¬ë‹ì˜ ë™ì‘ ì›ë¦¬ì— ëŒ€í•´ì„œ ì„¤ëª…í•´ì¤˜', additional_kwargs={}, response_metadata={}, id='68a7d2b8-fdc5-497c-ad3e-657a1daef059')], tool_choice=None, tools=[], response_format=None, state={'messages': [HumanMessage(content='ë¨¸ì‹ ëŸ¬ë‹ì˜ ë™ì‘ ì›ë¦¬ì— ëŒ€í•´ì„œ ì„¤ëª…í•´ì¤˜', additional_kwargs={}, response_metadata={}, id='68a7d2b8-fdc5-497c-ad3e-657a1daef059')]}, runtime=Runtime(context=None, store=None, stream_writer=<function Pregel.stream.<locals>.stream_writer at 0x114dfee80>, previous=None), model_settings={})\n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mmodel\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "ë¨¸ì‹ ëŸ¬ë‹(Machine Learning)ì€ ì»´í“¨í„°ê°€ ëª…ì‹œì ìœ¼ë¡œ í”„ë¡œê·¸ë˜ë°ë˜ì§€ ì•Šê³ ë„ ë°ì´í„°ë¡œë¶€í„° íŒ¨í„´ì„ í•™ìŠµí•˜ì—¬ ìŠ¤ìŠ¤ë¡œ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ê¸°ìˆ ì…ë‹ˆë‹¤. ë¨¸ì‹ ëŸ¬ë‹ì˜ ë™ì‘ ì›ë¦¬ëŠ” í¬ê²Œ ë‹¤ìŒê³¼ ê°™ì€ ë‹¨ê³„ë¡œ ìš”ì•½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
      "\n",
      "1. **ë°ì´í„° ìˆ˜ì§‘ ë° ì¤€ë¹„**  \n",
      "   ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì€ ë°ì´í„°ë¥¼ í†µí•´ í•™ìŠµí•˜ê¸° ë•Œë¬¸ì— ì–‘ì§ˆì˜ ë°ì´í„°ê°€ í•„ìˆ˜ì ì…ë‹ˆë‹¤. ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ê³ , ê²°ì¸¡ì¹˜ ì²˜ë¦¬, ì •ê·œí™”, íŠ¹ì„± ì„ íƒ ë“±ì˜ ì „ì²˜ë¦¬ ê³¼ì •ì„ ê±°ì¹©ë‹ˆë‹¤.\n",
      "\n",
      "2. **íŠ¹ì„± ì¶”ì¶œ ë° ì„ íƒ**  \n",
      "   ì›ì‹œ ë°ì´í„°ì—ì„œ ëª¨ë¸ì´ í•™ìŠµí•˜ê¸° ì í•©í•œ í˜•íƒœì˜ íŠ¹ì„±(feature)ì„ ì¶”ì¶œí•©ë‹ˆë‹¤. ì ì ˆí•œ íŠ¹ì„±ì„ ì„ íƒí•˜ëŠ” ê³¼ì •ì€ ëª¨ë¸ì˜ ì„±ëŠ¥ì— í° ì˜í–¥ì„ ë¯¸ì¹©ë‹ˆë‹¤.\n",
      "\n",
      "3. **ëª¨ë¸ ì„ íƒ**  \n",
      "   ë¬¸ì œì— ë§ëŠ” ì•Œê³ ë¦¬ì¦˜(ëª¨ë¸)ì„ ì„ íƒí•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ë¶„ë¥˜ ë¬¸ì œì—ëŠ” ì˜ì‚¬ê²°ì •ë‚˜ë¬´, SVM, ì‹ ê²½ë§ ë“±ì´ ì‚¬ìš©ë˜ê³ , íšŒê·€ ë¬¸ì œì—ëŠ” ì„ í˜• íšŒê·€, ë¦¿ì§€ íšŒê·€ ë“±ì´ ì‚¬ìš©ë©ë‹ˆë‹¤.\n",
      "\n",
      "4. **í•™ìŠµ(Training)**  \n",
      "   ì„ íƒí•œ ëª¨ë¸ì— í•™ìŠµ ë°ì´í„°ë¥¼ ì…ë ¥í•˜ì—¬ íŒ¨í„´ì„ í•™ìŠµí•©ë‹ˆë‹¤. ì´ ê³¼ì •ì—ì„œ ëª¨ë¸ì˜ ë‚´ë¶€ íŒŒë¼ë¯¸í„°(ì˜ˆ: ê°€ì¤‘ì¹˜)ê°€ ë°ì´í„°ì™€ ëª©í‘œ ì¶œë ¥ ê°„ì˜ ì˜¤ë¥˜ë¥¼ ìµœì†Œí™”í•˜ë„ë¡ ì¡°ì •ë©ë‹ˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ì†ì‹¤ í•¨ìˆ˜(loss function)ë¥¼ ì •ì˜í•˜ê³ , ì´ë¥¼ ìµœì†Œí™”í•˜ê¸° ìœ„í•´ ê²½ì‚¬ í•˜ê°•ë²•(gradient descent) ë“±ì˜ ìµœì í™” ê¸°ë²•ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n",
      "\n",
      "5. **í‰ê°€(Evaluation)**  \n",
      "   í•™ìŠµëœ ëª¨ë¸ì„ ê²€ì¦ ë°ì´í„° ë˜ëŠ” í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ì ìš©í•´ ì„±ëŠ¥ì„ í‰ê°€í•©ë‹ˆë‹¤. ì •í™•ë„, ì •ë°€ë„, ì¬í˜„ìœ¨, F1 ì ìˆ˜ ë“± ë‹¤ì–‘í•œ í‰ê°€ ì§€í‘œë¥¼ í™œìš©í•©ë‹ˆë‹¤.\n",
      "\n",
      "6. **ë°°í¬ ë° ì˜ˆì¸¡(Prediction)**  \n",
      "   í•™ìŠµì´ ì™„ë£Œëœ ëª¨ë¸ì„ ì‹¤ì œ í™˜ê²½ì— ì ìš©í•˜ì—¬ ìƒˆë¡œìš´ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ ë˜ëŠ” ì˜ì‚¬ê²°ì •ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
      "\n",
      "7. **ëª¨ë¸ ê°œì„ **  \n",
      "   ì‹¤ì œ ì‚¬ìš© ê³¼ì •ì—ì„œ ì„±ëŠ¥ì„ ëª¨ë‹ˆí„°ë§í•˜ê³ , í•„ìš” ì‹œ ì¶”ê°€ ë°ì´í„°ë¡œ ì¬í•™ìŠµí•˜ê±°ë‚˜ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ë“±ì„ í†µí•´ ëª¨ë¸ì„ ê°œì„ í•©ë‹ˆë‹¤.\n",
      "\n",
      "ìš”ì•½í•˜ë©´, ë¨¸ì‹ ëŸ¬ë‹ì€ ë°ì´í„°ë¥¼ í†µí•´ íŒ¨í„´ì„ ì°¾ê³  ì´ë¥¼ ì¼ë°˜í™”í•˜ì—¬ ìƒˆë¡œìš´ ë°ì´í„°ì— ì ìš©í•¨ìœ¼ë¡œì¨ ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤. ì´ ê³¼ì •ì—ì„œ ëª¨ë¸ì€ ì…ë ¥ ë°ì´í„°ì™€ ëª©í‘œ ê°’ ê°„ì˜ ê´€ê³„ë¥¼ ìˆ˜í•™ì ìœ¼ë¡œ í‘œí˜„í•˜ëŠ” í•¨ìˆ˜ë¥¼ í•™ìŠµí•˜ê²Œ ë©ë‹ˆë‹¤."
     ]
    }
   ],
   "source": [
    "from langchain_teddynote.messages import stream_graph\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "stream_graph(\n",
    "    agent,\n",
    "    inputs={\n",
    "        \"messages\": [HumanMessage(content=\"ë¨¸ì‹ ëŸ¬ë‹ì˜ ë™ì‘ ì›ë¦¬ì— ëŒ€í•´ì„œ ì„¤ëª…í•´ì¤˜\")]\n",
    "    },\n",
    "    config=RunnableConfig(),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë„êµ¬ (Tools)\n",
    "\n",
    "ë„êµ¬ëŠ” ì—ì´ì „íŠ¸ì—ê²Œ í–‰ë™ì„ ì·¨í•  ìˆ˜ ìˆëŠ” ëŠ¥ë ¥ì„ ë¶€ì—¬í•©ë‹ˆë‹¤. \n",
    "\n",
    "ì—ì´ì „íŠ¸ëŠ” ë‹¤ìŒì„ ì§€ì›í•©ë‹ˆë‹¤.\n",
    "- ìˆœì°¨ì ìœ¼ë¡œ ì—¬ëŸ¬ ë„êµ¬ í˜¸ì¶œ\n",
    "- ì ì ˆí•œ ê²½ìš° ë³‘ë ¬ ë„êµ¬ í˜¸ì¶œ\n",
    "- ì´ì „ ê²°ê³¼ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ë™ì  ë„êµ¬ ì„ íƒ\n",
    "- ë„êµ¬ ì¬ì‹œë„ ë¡œì§ ë° ì˜¤ë¥˜ ì²˜ë¦¬\n",
    "- ë„êµ¬ í˜¸ì¶œ ê°„ ìƒíƒœ ì§€ì†ì„±"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ë„êµ¬ ì •ì˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "from langchain.agents import create_agent\n",
    "\n",
    "\n",
    "@tool\n",
    "def search(query: str) -> str:\n",
    "    \"\"\"Search for information.\"\"\"\n",
    "    return f\"Results for: {query}\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"Get weather information for a location.\"\"\"\n",
    "    # ì¼ë¶€ëŸ¬ ì˜¤ë¥˜ ë°œìƒ\n",
    "    raise Exception(\"ë‚ ì”¨ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ì˜€ìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "\n",
    "# ë„êµ¬ ë¦¬ìŠ¤íŠ¸ë¥¼ ì—ì´ì „íŠ¸ì— ì „ë‹¬\n",
    "agent = create_agent(\"openai:gpt-4.1-mini\", tools=[search, get_weather])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ë„êµ¬ ì˜¤ë¥˜ ì²˜ë¦¬\n",
    "\n",
    "ë„êµ¬ ì˜¤ë¥˜ ì²˜ë¦¬ë¥¼ ì»¤ìŠ¤í„°ë§ˆì´ì§•í•˜ë ¤ë©´ `@wrap_tool_call` ë°ì½”ë ˆì´í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¯¸ë“¤ì›¨ì–´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import wrap_tool_call\n",
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "\n",
    "@wrap_tool_call\n",
    "def handle_tool_errors(request, handler):\n",
    "    \"\"\"ë„êµ¬ ì‹¤í–‰ ì˜¤ë¥˜ë¥¼ ì»¤ìŠ¤í…€ ë©”ì‹œì§€ë¡œ ì²˜ë¦¬\"\"\"\n",
    "    try:\n",
    "        return handler(request)\n",
    "    except Exception as e:\n",
    "        # ëª¨ë¸ì— ì»¤ìŠ¤í…€ ì˜¤ë¥˜ ë©”ì‹œì§€ ë°˜í™˜\n",
    "        return ToolMessage(\n",
    "            content=f\"[ì—ëŸ¬] ë„êµ¬ í˜¸ì¶œì‹œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì–´ìš”. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”. ({str(e)})\",\n",
    "            tool_call_id=request.tool_call[\"id\"],\n",
    "        )\n",
    "\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"openai:gpt-4.1-mini\",\n",
    "    tools=[search, get_weather],\n",
    "    middleware=[handle_tool_errors],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mmodel\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mtools\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "[ì—ëŸ¬] ë„êµ¬ í˜¸ì¶œì‹œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆì–´ìš”. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”. (ë‚ ì”¨ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ì˜€ìŠµë‹ˆë‹¤.)\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mmodel\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "ì£„ì†¡í•©ë‹ˆë‹¤. í˜„ì¬ ì„œìš¸ì˜ ë‚ ì”¨ ì •ë³´ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë° ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ì— ë‹¤ì‹œ ì‹œë„í•´ ì£¼ì‹œê² ì–´ìš”?"
     ]
    }
   ],
   "source": [
    "stream_graph(agent, inputs={\"messages\": [HumanMessage(content=\"ì„œìš¸ ë‚ ì”¨ ì¡°íšŒí•´ì¤˜\")]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## í”„ë¡¬í”„íŠ¸"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸\n",
    "\n",
    "ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ì œê³µí•˜ì—¬ ì—ì´ì „íŠ¸ê°€ ì‘ì—…ì— ì ‘ê·¼í•˜ëŠ” ë°©ì‹ì„ í˜•ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_agent(\n",
    "    \"openai:gpt-4.1-mini\",\n",
    "    tools=[search, get_weather],\n",
    "    system_prompt=\"You are a helpful assistant. Be concise and accurate.\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ë™ì  ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸\n",
    "\n",
    "ëŸ°íƒ€ì„ ì»¨í…ìŠ¤íŠ¸ë‚˜ ì—ì´ì „íŠ¸ ìƒíƒœë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ë¥¼ ìˆ˜ì •í•´ì•¼ í•˜ëŠ” ê³ ê¸‰ ì‚¬ìš© ì‚¬ë¡€ì˜ ê²½ìš° ë¯¸ë“¤ì›¨ì–´ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "- í•µì‹¬: `request.runtime.context`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict\n",
    "from langchain.agents import create_agent\n",
    "from langchain.agents.middleware import dynamic_prompt, ModelRequest\n",
    "\n",
    "\n",
    "class Context(TypedDict):\n",
    "    answer_type: str\n",
    "\n",
    "\n",
    "@dynamic_prompt\n",
    "def user_role_prompt(request: ModelRequest) -> str:\n",
    "    \"\"\"ì‚¬ìš©ì ì—­í• ì— ë”°ë¼ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±\"\"\"\n",
    "    answer_type = request.runtime.context.get(\"answer_type\", \"default\")\n",
    "    base_prompt = \"You are a helpful assistant.\"\n",
    "\n",
    "    if answer_type == \"default\":\n",
    "        return f\"{base_prompt} Answer in Korean. ê°„ê²°í•˜ê²Œ ë‹µë³€í•´ì¤˜.\"\n",
    "    elif answer_type == \"sns\":\n",
    "        return f\"{base_prompt} Answer in Korean. SNS í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì¤˜.\"\n",
    "    elif answer_type == \"article\":\n",
    "        return f\"{base_prompt} Answer in Korean. ë‰´ìŠ¤ ê¸°ì‚¬ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ì¤˜.\"\n",
    "\n",
    "    return base_prompt\n",
    "\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"openai:gpt-4.1-mini\",\n",
    "    tools=[search],\n",
    "    middleware=[user_role_prompt],\n",
    "    context_schema=Context,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "ğŸ”„ Node: \u001b[1;36mmodel\u001b[0m ğŸ”„\n",
      "- - - - - - - - - - - - - - - - - - - - - - - - - \n",
      "[ë¨¸ì‹ ëŸ¬ë‹ ë™ì‘ ì›ë¦¬]ğŸ¤–âœ¨\n",
      "\n",
      "ë¨¸ì‹ ëŸ¬ë‹ì€ 'ë°ì´í„°'ë¥¼ í†µí•´ ìŠ¤ìŠ¤ë¡œ í•™ìŠµí•˜ê³  ì˜ˆì¸¡í•˜ëŠ” ê¸°ìˆ ì´ì—ìš”!\n",
      "\n",
      "1ï¸âƒ£ ë°ì´í„° ìˆ˜ì§‘: ë‹¤ì–‘í•œ ë°ì´í„°ë¥¼ ëª¨ì•„ìš”.\n",
      "2ï¸âƒ£ ì „ì²˜ë¦¬: ë°ì´í„°ë¥¼ ê¹”ë”í•˜ê²Œ ì •ë¦¬í•´ìš”.\n",
      "3ï¸âƒ£ ëª¨ë¸ í•™ìŠµ: ì•Œê³ ë¦¬ì¦˜ì´ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ë©° ê·œì¹™ì„ ì°¾ì•„ìš”.\n",
      "4ï¸âƒ£ í‰ê°€: ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì²´í¬í•´ìš”.\n",
      "5ï¸âƒ£ ì˜ˆì¸¡: ìƒˆë¡œìš´ ë°ì´í„°ì— ëŒ€í•´ ê²°ê³¼ë¥¼ ì˜ˆì¸¡í•´ìš”.\n",
      "\n",
      "ì‰½ê²Œ ë§í•´, ê²½í—˜ì„ í†µí•´ ë” ë˜‘ë˜‘í•´ì§€ëŠ” ì»´í“¨í„°ë¼ê³  ë³´ë©´ ë¼ìš”! #ë¨¸ì‹ ëŸ¬ë‹ #ì¸ê³µì§€ëŠ¥ #ë°ì´í„°ì‚¬ì´ì–¸ìŠ¤"
     ]
    }
   ],
   "source": [
    "# ì»¨í…ìŠ¤íŠ¸ì— ë”°ë¼ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ê°€ ë™ì ìœ¼ë¡œ ì„¤ì •ë©ë‹ˆë‹¤\n",
    "stream_graph(\n",
    "    agent,\n",
    "    inputs={\n",
    "        \"messages\": [HumanMessage(content=\"ë¨¸ì‹ ëŸ¬ë‹ì˜ ë™ì‘ ì›ë¦¬ì— ëŒ€í•´ì„œ ì„¤ëª…í•´ì¤˜\")]\n",
    "    },\n",
    "    context=Context(answer_type=\"sns\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ì—ì´ì „íŠ¸ í˜¸ì¶œ\n",
    "\n",
    "ì—ì´ì „íŠ¸ì˜ ìƒíƒœì— ì—…ë°ì´íŠ¸ë¥¼ ì „ë‹¬í•˜ì—¬ ì—ì´ì „íŠ¸ë¥¼ í˜¸ì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ëª¨ë“  ì—ì´ì „íŠ¸ëŠ” ìƒíƒœì— ë©”ì‹œì§€ ì‹œí€€ìŠ¤ë¥¼ í¬í•¨í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": \"What's the weather in San Francisco?\"}]},\n",
    "    context={\"user_role\": \"expert\"},\n",
    ")\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ê³ ê¸‰ ê°œë…\n",
    "\n",
    "### êµ¬ì¡°í™”ëœ ì¶œë ¥\n",
    "\n",
    "íŠ¹ì • í˜•ì‹ìœ¼ë¡œ ì—ì´ì „íŠ¸ì˜ ì¶œë ¥ì„ ë°˜í™˜í•˜ê³  ì‹¶ì„ ë•Œê°€ ìˆìŠµë‹ˆë‹¤. LangChainì€ `response_format` ë§¤ê°œë³€ìˆ˜ë¥¼ í†µí•´ êµ¬ì¡°í™”ëœ ì¶œë ¥ ì „ëµì„ ì œê³µí•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ToolStrategy\n",
    "\n",
    "`ToolStrategy`ëŠ” ì¸ê³µì ì¸ ë„êµ¬ í˜¸ì¶œì„ ì‚¬ìš©í•˜ì—¬ êµ¬ì¡°í™”ëœ ì¶œë ¥ì„ ìƒì„±í•©ë‹ˆë‹¤. ë„êµ¬ í˜¸ì¶œì„ ì§€ì›í•˜ëŠ” ëª¨ë“  ëª¨ë¸ì—ì„œ ì‘ë™í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='John Doe' email='john@example.com' phone='(555) 123-4567'\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "from langchain.agents import create_agent\n",
    "from langchain.agents.structured_output import ToolStrategy\n",
    "\n",
    "\n",
    "# ì‘ë‹µ ìŠ¤í‚¤ë§ˆ ì •ì˜\n",
    "class ContactInfo(BaseModel):\n",
    "    name: str\n",
    "    email: str\n",
    "    phone: str\n",
    "\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"openai:gpt-4.1-mini\", tools=[], response_format=ToolStrategy(ContactInfo)\n",
    ")\n",
    "\n",
    "result = agent.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Extract contact info from: John Doe, john@example.com, (555) 123-4567\",\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "print(result[\"structured_response\"])\n",
    "# ContactInfo(name='John Doe', email='john@example.com', phone='(555) 123-4567')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ProviderStrategy\n",
    "\n",
    "`ProviderStrategy`ëŠ” ëª¨ë¸ ì œê³µìì˜ ë„¤ì´í‹°ë¸Œ êµ¬ì¡°í™”ëœ ì¶œë ¥ ìƒì„±ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ë” ì•ˆì •ì ì´ì§€ë§Œ ë„¤ì´í‹°ë¸Œ êµ¬ì¡°í™”ëœ ì¶œë ¥ì„ ì§€ì›í•˜ëŠ” ì œê³µì(ì˜ˆ: OpenAI)ì—ì„œë§Œ ì‘ë™í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.structured_output import ProviderStrategy\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"openai:gpt-4.1\", response_format=ProviderStrategy(ContactInfo)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = agent.invoke(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Extract contact info from: John Doe, john@example.com, (555) 123-4567\",\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ContactInfo(name='John Doe', email='john@example.com', phone='(555) 123-4567')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[\"structured_response\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë©”ëª¨ë¦¬\n",
    "\n",
    "ì—ì´ì „íŠ¸ëŠ” ë©”ì‹œì§€ ìƒíƒœë¥¼ í†µí•´ ëŒ€í™” ê¸°ë¡ì„ ìë™ìœ¼ë¡œ ìœ ì§€í•©ë‹ˆë‹¤. ëŒ€í™” ì¤‘ì— ì¶”ê°€ ì •ë³´ë¥¼ ê¸°ì–µí•˜ê¸° ìœ„í•´ ì»¤ìŠ¤í…€ ìƒíƒœ ìŠ¤í‚¤ë§ˆë¥¼ ì‚¬ìš©í•˜ë„ë¡ ì—ì´ì „íŠ¸ë¥¼ êµ¬ì„±í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\n",
    "\n",
    "ìƒíƒœì— ì €ì¥ëœ ì •ë³´ëŠ” ì—ì´ì „íŠ¸ì˜ ë‹¨ê¸° ë©”ëª¨ë¦¬ë¡œ ìƒê°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ë¯¸ë“¤ì›¨ì–´ë¥¼ í†µí•œ ìƒíƒœ ì •ì˜\n",
    "\n",
    "ì»¤ìŠ¤í…€ ìƒíƒœê°€ íŠ¹ì • ë¯¸ë“¤ì›¨ì–´ í›… ë° í•´ë‹¹ ë¯¸ë“¤ì›¨ì–´ì— ì—°ê²°ëœ ë„êµ¬ì—ì„œ ì•¡ì„¸ìŠ¤í•´ì•¼ í•˜ëŠ” ê²½ìš° ë¯¸ë“¤ì›¨ì–´ë¥¼ ì‚¬ìš©í•˜ì—¬ ì»¤ìŠ¤í…€ ìƒíƒœë¥¼ ì •ì˜í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "from langchain.agents import AgentState\n",
    "from langchain.agents.middleware import AgentMiddleware\n",
    "\n",
    "\n",
    "# ì»¤ìŠ¤í…€ ìƒíƒœ ìŠ¤í‚¤ë§ˆ ì •ì˜\n",
    "class CustomState(AgentState):\n",
    "    user_preferences: dict\n",
    "\n",
    "\n",
    "class CustomMiddleware(AgentMiddleware):\n",
    "    state_schema = CustomState\n",
    "    tools = []\n",
    "\n",
    "    def before_model(self, state: CustomState, runtime) -> dict[str, Any] | None:\n",
    "        # ëª¨ë¸ í˜¸ì¶œ ì „ ì»¤ìŠ¤í…€ ë¡œì§\n",
    "        pass\n",
    "\n",
    "\n",
    "agent = create_agent(\"openai:gpt-4.1-mini\", tools=[], middleware=[CustomMiddleware()])\n",
    "\n",
    "# ì—ì´ì „íŠ¸ëŠ” ì´ì œ ë©”ì‹œì§€ ì™¸ì— ì¶”ê°€ ìƒíƒœë¥¼ ì¶”ì í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤\n",
    "result = agent.invoke(\n",
    "    {\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"I prefer technical explanations\"}],\n",
    "        \"user_preferences\": {\"style\": \"technical\", \"verbosity\": \"detailed\"},\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `state_schema`ë¥¼ í†µí•œ ìƒíƒœ ì •ì˜\n",
    "\n",
    "ë„êµ¬ì—ì„œë§Œ ì‚¬ìš©ë˜ëŠ” ì»¤ìŠ¤í…€ ìƒíƒœë¥¼ ì •ì˜í•˜ëŠ” ë‹¨ì¶• ë°©ë²•ìœ¼ë¡œ `state_schema` ë§¤ê°œë³€ìˆ˜ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import AgentState\n",
    "\n",
    "\n",
    "class CustomState(AgentState):\n",
    "    user_preferences: dict\n",
    "\n",
    "\n",
    "agent = create_agent(\"openai:gpt-4.1-mini\", tools=[], state_schema=CustomState)\n",
    "\n",
    "result = agent.invoke(\n",
    "    {\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": \"I prefer technical explanations\"}],\n",
    "        \"user_preferences\": {\"style\": \"technical\", \"verbosity\": \"detailed\"},\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ìŠ¤íŠ¸ë¦¬ë°\n",
    "\n",
    "ì—ì´ì „íŠ¸ê°€ ì—¬ëŸ¬ ë‹¨ê³„ë¥¼ ì‹¤í–‰í•˜ëŠ” ê²½ìš° ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¤‘ê°„ ì§„í–‰ ìƒí™©ì„ í‘œì‹œí•˜ê¸° ìœ„í•´ ë©”ì‹œì§€ê°€ ë°œìƒí•˜ëŠ” ëŒ€ë¡œ ìŠ¤íŠ¸ë¦¬ë°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for chunk in agent.stream(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"Search for AI news and summarize the findings\"}\n",
    "        ]\n",
    "    },\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    # ê° ì²­í¬ì—ëŠ” í•´ë‹¹ ì‹œì ì˜ ì „ì²´ ìƒíƒœê°€ í¬í•¨ë©ë‹ˆë‹¤\n",
    "    latest_message = chunk[\"messages\"][-1]\n",
    "    if latest_message.content:\n",
    "        print(f\"Agent: {latest_message.content}\")\n",
    "    elif hasattr(latest_message, \"tool_calls\") and latest_message.tool_calls:\n",
    "        print(f\"Calling tools: {[tc['name'] for tc in latest_message.tool_calls]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ë¯¸ë“¤ì›¨ì–´\n",
    "\n",
    "ë¯¸ë“¤ì›¨ì–´ëŠ” ì‹¤í–‰ì˜ ë‹¤ì–‘í•œ ë‹¨ê³„ì—ì„œ ì—ì´ì „íŠ¸ ë™ì‘ì„ ì»¤ìŠ¤í„°ë§ˆì´ì§•í•˜ê¸° ìœ„í•œ ê°•ë ¥í•œ í™•ì¥ì„±ì„ ì œê³µí•©ë‹ˆë‹¤.\n",
    "\n",
    "**ë¯¸ë“¤ì›¨ì–´ ì‚¬ìš© ì‚¬ë¡€:**\n",
    "\n",
    "- ëª¨ë¸ í˜¸ì¶œ ì „ ìƒíƒœ ì²˜ë¦¬ (ì˜ˆ: ë©”ì‹œì§€ íŠ¸ë¦¬ë°, ì»¨í…ìŠ¤íŠ¸ ì£¼ì…)\n",
    "- ëª¨ë¸ ì‘ë‹µ ìˆ˜ì • ë˜ëŠ” ê²€ì¦ (ì˜ˆ: ê°€ë“œë ˆì¼, ì½˜í…ì¸  í•„í„°ë§)\n",
    "- ì»¤ìŠ¤í…€ ë¡œì§ìœ¼ë¡œ ë„êµ¬ ì‹¤í–‰ ì˜¤ë¥˜ ì²˜ë¦¬\n",
    "- ìƒíƒœ ë˜ëŠ” ì»¨í…ìŠ¤íŠ¸ì— ë”°ë¥¸ ë™ì  ëª¨ë¸ ì„ íƒ\n",
    "- ì»¤ìŠ¤í…€ ë¡œê¹…, ëª¨ë‹ˆí„°ë§ ë˜ëŠ” ë¶„ì„ ì¶”ê°€"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
