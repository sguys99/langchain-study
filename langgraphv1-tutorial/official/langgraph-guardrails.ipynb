{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LangChain 가드레일\n",
        "\n",
        "가드레일(Guardrails)은 에이전트 실행 중 주요 지점에서 콘텐츠를 검증하고 필터링하여 안전하고 규정을 준수하는 AI 애플리케이션을 구축할 수 있도록 도와줍니다.\n",
        "\n",
        "## 주요 사용 사례\n",
        "\n",
        "- 개인정보(PII) 유출 방지\n",
        "- 프롬프트 인젝션 공격 탐지 및 차단\n",
        "- 부적절하거나 유해한 콘텐츠 차단\n",
        "- 비즈니스 규칙 및 규정 준수 요구사항 강제\n",
        "- 출력 품질 및 정확성 검증\n",
        "\n",
        "가드레일은 미들웨어를 사용하여 구현되며, 에이전트 시작 전, 완료 후, 또는 모델 및 도구 호출 주변에서 실행을 가로챌 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 사전 준비\n",
        "\n",
        "환경 변수를 설정합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv(override=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 가드레일의 두 가지 접근 방식\n",
        "\n",
        "### 1. 결정론적 가드레일 (Deterministic)\n",
        "\n",
        "정규 표현식, 키워드 매칭, 명시적 검사 등 규칙 기반 로직을 사용합니다. 빠르고 예측 가능하며 비용 효율적이지만, 미묘한 위반 사항을 놓칠 수 있습니다.\n",
        "\n",
        "### 2. 모델 기반 가드레일 (Model-based)\n",
        "\n",
        "LLM 또는 분류기를 사용하여 의미론적 이해로 콘텐츠를 평가합니다. 규칙이 놓치는 미묘한 문제를 포착할 수 있지만, 느리고 비용이 더 많이 듭니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 내장 가드레일\n",
        "\n",
        "### PII 탐지\n",
        "\n",
        "LangChain은 대화에서 개인 식별 정보(PII)를 탐지하고 처리하기 위한 내장 미들웨어를 제공합니다.\n",
        "\n",
        "PII 미들웨어는 다양한 전략을 지원합니다:\n",
        "\n",
        "- **`redact`**: `[REDACTED_TYPE]`으로 대체\n",
        "- **`mask`**: 부분적으로 가림 (예: 마지막 4자리만 표시)\n",
        "- **`hash`**: 결정론적 해시로 대체\n",
        "- **`block`**: 탐지 시 예외 발생"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.agents import create_agent\n",
        "from langchain.agents.middleware import PIIMiddleware\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.tools import tool\n",
        "\n",
        "@tool\n",
        "def customer_service_tool(query: str) -> str:\n",
        "    \"\"\"Handle customer service queries.\"\"\"\n",
        "    return f\"Processing customer query: {query}\"\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4.1-mini\")\n",
        "\n",
        "agent = create_agent(\n",
        "    model=model,\n",
        "    tools=[customer_service_tool],\n",
        "    middleware=[\n",
        "        # 사용자 입력에서 이메일 수정\n",
        "        PIIMiddleware(\n",
        "            \"email\",\n",
        "            strategy=\"redact\",\n",
        "            apply_to_input=True,\n",
        "        ),\n",
        "        # 사용자 입력에서 신용카드 마스킹\n",
        "        PIIMiddleware(\n",
        "            \"credit_card\",\n",
        "            strategy=\"mask\",\n",
        "            apply_to_input=True,\n",
        "        ),\n",
        "    ],\n",
        ")\n",
        "\n",
        "# PII가 포함된 메시지 테스트\n",
        "result = agent.invoke({\n",
        "    \"messages\": [{\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"My email is john.doe@example.com and card is 4532-1234-5678-9010\"\n",
        "    }]\n",
        "})\n",
        "\n",
        "print(result[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 커스텀 PII 탐지기\n",
        "\n",
        "정규 표현식을 사용하여 커스텀 PII 패턴을 정의할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.agents.middleware import PIIMiddleware\n",
        "\n",
        "agent = create_agent(\n",
        "    model=model,\n",
        "    tools=[customer_service_tool],\n",
        "    middleware=[\n",
        "        # API 키 탐지 - 발견 시 에러 발생\n",
        "        PIIMiddleware(\n",
        "            \"api_key\",\n",
        "            detector=r\"sk-[a-zA-Z0-9]{32}\",  # 커스텀 정규 표현식\n",
        "            strategy=\"block\",\n",
        "            apply_to_input=True,\n",
        "        ),\n",
        "        # 한국 전화번호 패턴 탐지\n",
        "        PIIMiddleware(\n",
        "            \"phone_number\",\n",
        "            detector=r\"010-\\d{4}-\\d{4}\",\n",
        "            strategy=\"redact\",\n",
        "            apply_to_input=True,\n",
        "        ),\n",
        "    ],\n",
        ")\n",
        "\n",
        "# 전화번호가 포함된 메시지 테스트\n",
        "result = agent.invoke({\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"My phone is 010-1234-5678\"}]\n",
        "})\n",
        "\n",
        "print(result[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 내장 PII 타입\n",
        "\n",
        "LangChain은 다음과 같은 내장 PII 타입을 제공합니다:\n",
        "\n",
        "- `email`: 이메일 주소\n",
        "- `credit_card`: 신용카드 번호 (Luhn 검증)\n",
        "- `ip`: IP 주소\n",
        "- `mac_address`: MAC 주소\n",
        "- `url`: URL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 여러 PII 타입 동시 적용\n",
        "agent = create_agent(\n",
        "    model=model,\n",
        "    tools=[customer_service_tool],\n",
        "    middleware=[\n",
        "        PIIMiddleware(\"email\", strategy=\"redact\", apply_to_input=True),\n",
        "        PIIMiddleware(\"ip\", strategy=\"hash\", apply_to_input=True),\n",
        "        PIIMiddleware(\"url\", strategy=\"redact\", apply_to_input=True),\n",
        "    ],\n",
        ")\n",
        "\n",
        "result = agent.invoke({\n",
        "    \"messages\": [{\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"Contact me at user@test.com from IP 192.168.1.1 or visit https://example.com\"\n",
        "    }]\n",
        "})\n",
        "\n",
        "print(result[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Human-in-the-Loop\n",
        "\n",
        "민감한 작업을 실행하기 전에 사람의 승인을 요구하는 내장 미들웨어입니다. 이는 고위험 결정에 가장 효과적인 가드레일 중 하나입니다.\n",
        "\n",
        "다음과 같은 경우에 유용합니다:\n",
        "- 금융 거래 및 송금\n",
        "- 프로덕션 데이터 삭제 또는 수정\n",
        "- 외부 당사자에게 통신 전송\n",
        "- 비즈니스에 중요한 영향을 미치는 모든 작업"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.agents import create_agent\n",
        "from langchain.agents.middleware import HumanInTheLoopMiddleware\n",
        "from langgraph.checkpoint.memory import InMemorySaver\n",
        "from langgraph.types import Command\n",
        "from langchain.tools import tool\n",
        "\n",
        "@tool\n",
        "def search_tool(query: str) -> str:\n",
        "    \"\"\"Search for information.\"\"\"\n",
        "    return f\"Search results for: {query}\"\n",
        "\n",
        "@tool\n",
        "def send_email_tool(recipient: str, subject: str, body: str) -> str:\n",
        "    \"\"\"Send an email. This is a sensitive operation.\"\"\"\n",
        "    return f\"Email sent to {recipient}\"\n",
        "\n",
        "@tool\n",
        "def delete_database_tool(database_name: str) -> str:\n",
        "    \"\"\"Delete a database. This is a critical operation.\"\"\"\n",
        "    return f\"Database {database_name} deleted\"\n",
        "\n",
        "agent = create_agent(\n",
        "    model=model,\n",
        "    tools=[search_tool, send_email_tool, delete_database_tool],\n",
        "    middleware=[\n",
        "        HumanInTheLoopMiddleware(\n",
        "            interrupt_on={\n",
        "                # 민감한 작업에 대해 승인 필요\n",
        "                \"send_email_tool\": True,\n",
        "                \"delete_database_tool\": True,\n",
        "                # 안전한 작업은 자동 승인\n",
        "                \"search_tool\": False,\n",
        "            }\n",
        "        ),\n",
        "    ],\n",
        "    checkpointer=InMemorySaver(),  # 상태 지속성 필요\n",
        ")\n",
        "\n",
        "# thread_id 필요\n",
        "config = {\"configurable\": {\"thread_id\": \"some_id\"}}\n",
        "\n",
        "# 이메일 전송 전 일시 중지됨\n",
        "result = agent.invoke(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": \"Send an email to the team\"}]},\n",
        "    config=config\n",
        ")\n",
        "\n",
        "print(\"Agent paused for approval. Approving...\")\n",
        "\n",
        "# 승인 후 재개\n",
        "result = agent.invoke(\n",
        "    Command(resume={\"decisions\": [{\"type\": \"approve\"}]}),\n",
        "    config=config  # 동일한 thread_id로 재개\n",
        ")\n",
        "\n",
        "print(result[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 커스텀 가드레일\n",
        "\n",
        "더 정교한 가드레일을 위해 에이전트 실행 전후에 실행되는 커스텀 미들웨어를 생성할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Before Agent 가드레일\n",
        "\n",
        "\"Before Agent\" 훅을 사용하여 각 호출 시작 시 요청을 검증합니다. 인증, 속도 제한 또는 처리가 시작되기 전에 부적절한 요청을 차단하는 세션 수준 검사에 유용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Any\n",
        "from langchain.agents.middleware import before_agent, AgentState, hook_config\n",
        "from langgraph.runtime import Runtime\n",
        "\n",
        "@before_agent(can_jump_to=[\"end\"])\n",
        "def content_filter(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n",
        "    \"\"\"결정론적 가드레일: 금지된 키워드가 포함된 요청 차단\"\"\"\n",
        "    banned_keywords = [\"hack\", \"exploit\", \"malware\", \"해킹\", \"악성코드\"]\n",
        "    \n",
        "    # 첫 번째 사용자 메시지 가져오기\n",
        "    if not state[\"messages\"]:\n",
        "        return None\n",
        "\n",
        "    first_message = state[\"messages\"][0]\n",
        "    if first_message.type != \"human\":\n",
        "        return None\n",
        "\n",
        "    content = first_message.content.lower()\n",
        "\n",
        "    # 금지된 키워드 확인\n",
        "    for keyword in banned_keywords:\n",
        "        if keyword in content:\n",
        "            # 처리 전에 실행 차단\n",
        "            return {\n",
        "                \"messages\": [{\n",
        "                    \"role\": \"assistant\",\n",
        "                    \"content\": \"부적절한 콘텐츠가 포함된 요청은 처리할 수 없습니다. 요청을 다시 작성해주세요.\"\n",
        "                }],\n",
        "                \"jump_to\": \"end\"\n",
        "            }\n",
        "\n",
        "    return None\n",
        "\n",
        "# 커스텀 가드레일 사용\n",
        "agent = create_agent(\n",
        "    model=model,\n",
        "    tools=[search_tool],\n",
        "    middleware=[content_filter],\n",
        ")\n",
        "\n",
        "# 이 요청은 처리 전에 차단됨\n",
        "result = agent.invoke({\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"How do I hack into a database?\"}]\n",
        "})\n",
        "\n",
        "print(result[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 클래스 기반 Before Agent 가드레일\n",
        "\n",
        "더 복잡한 로직의 경우 클래스 기반 미들웨어를 사용할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.agents.middleware import AgentMiddleware, AgentState, hook_config\n",
        "from langgraph.runtime import Runtime\n",
        "from typing import Any\n",
        "\n",
        "class ContentFilterMiddleware(AgentMiddleware):\n",
        "    \"\"\"결정론적 가드레일: 금지된 키워드가 포함된 요청 차단\"\"\"\n",
        "\n",
        "    def __init__(self, banned_keywords: list[str]):\n",
        "        super().__init__()\n",
        "        self.banned_keywords = [kw.lower() for kw in banned_keywords]\n",
        "\n",
        "    @hook_config(can_jump_to=[\"end\"])\n",
        "    def before_agent(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n",
        "        if not state[\"messages\"]:\n",
        "            return None\n",
        "\n",
        "        first_message = state[\"messages\"][0]\n",
        "        if first_message.type != \"human\":\n",
        "            return None\n",
        "\n",
        "        content = first_message.content.lower()\n",
        "\n",
        "        for keyword in self.banned_keywords:\n",
        "            if keyword in content:\n",
        "                return {\n",
        "                    \"messages\": [{\n",
        "                        \"role\": \"assistant\",\n",
        "                        \"content\": \"부적절한 콘텐츠가 포함된 요청은 처리할 수 없습니다.\"\n",
        "                    }],\n",
        "                    \"jump_to\": \"end\"\n",
        "                }\n",
        "\n",
        "        return None\n",
        "\n",
        "agent = create_agent(\n",
        "    model=model,\n",
        "    tools=[search_tool],\n",
        "    middleware=[\n",
        "        ContentFilterMiddleware(\n",
        "            banned_keywords=[\"hack\", \"exploit\", \"malware\", \"해킹\"]\n",
        "        ),\n",
        "    ],\n",
        ")\n",
        "\n",
        "result = agent.invoke({\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"데이터베이스를 해킹하는 방법은?\"}]\n",
        "})\n",
        "\n",
        "print(result[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### After Agent 가드레일\n",
        "\n",
        "\"After Agent\" 훅을 사용하여 사용자에게 반환하기 전에 최종 출력을 검증합니다. 모델 기반 안전성 검사, 품질 검증 또는 전체 에이전트 응답에 대한 최종 규정 준수 스캔에 유용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.agents.middleware import after_agent, AgentState, hook_config\n",
        "from langgraph.runtime import Runtime\n",
        "from langchain_core.messages import AIMessage\n",
        "from langchain.chat_models import init_chat_model\n",
        "from typing import Any\n",
        "\n",
        "safety_model = init_chat_model(\"openai:gpt-4.1-mini\")\n",
        "\n",
        "@after_agent(can_jump_to=[\"end\"])\n",
        "def safety_guardrail(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n",
        "    \"\"\"모델 기반 가드레일: LLM을 사용하여 응답 안전성 평가\"\"\"\n",
        "    # 최종 AI 응답 가져오기\n",
        "    if not state[\"messages\"]:\n",
        "        return None\n",
        "\n",
        "    last_message = state[\"messages\"][-1]\n",
        "    if not isinstance(last_message, AIMessage):\n",
        "        return None\n",
        "\n",
        "    # 모델을 사용하여 안전성 평가\n",
        "    safety_prompt = f\"\"\"이 응답이 안전하고 적절한지 평가하세요.\n",
        "    'SAFE' 또는 'UNSAFE'로만 응답하세요.\n",
        "\n",
        "    응답: {last_message.content}\"\"\"\n",
        "\n",
        "    result = safety_model.invoke([{\"role\": \"user\", \"content\": safety_prompt}])\n",
        "\n",
        "    if \"UNSAFE\" in result.content:\n",
        "        return {\n",
        "            \"messages\": [{\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": \"해당 응답을 제공할 수 없습니다. 요청을 다시 작성해주세요.\"\n",
        "            }],\n",
        "            \"jump_to\": \"end\"\n",
        "        }\n",
        "\n",
        "    return None\n",
        "\n",
        "# 안전 가드레일 사용\n",
        "agent = create_agent(\n",
        "    model=model,\n",
        "    tools=[search_tool],\n",
        "    middleware=[safety_guardrail],\n",
        ")\n",
        "\n",
        "result = agent.invoke({\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"Tell me about Python programming\"}]\n",
        "})\n",
        "\n",
        "print(result[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 여러 가드레일 결합\n",
        "\n",
        "미들웨어 배열에 여러 가드레일을 추가하여 계층화된 보호를 구축할 수 있습니다. 가드레일은 순서대로 실행됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.agents import create_agent\n",
        "from langchain.agents.middleware import PIIMiddleware, HumanInTheLoopMiddleware\n",
        "from langgraph.checkpoint.memory import InMemorySaver\n",
        "\n",
        "agent = create_agent(\n",
        "    model=model,\n",
        "    tools=[search_tool, send_email_tool],\n",
        "    middleware=[\n",
        "        # 계층 1: 결정론적 입력 필터 (before agent)\n",
        "        ContentFilterMiddleware(banned_keywords=[\"hack\", \"exploit\"]),\n",
        "\n",
        "        # 계층 2: PII 보호 (before and after model)\n",
        "        PIIMiddleware(\"email\", strategy=\"redact\", apply_to_input=True),\n",
        "        PIIMiddleware(\"email\", strategy=\"redact\", apply_to_output=True),\n",
        "\n",
        "        # 계층 3: 민감한 도구에 대한 사람의 승인\n",
        "        HumanInTheLoopMiddleware(interrupt_on={\"send_email_tool\": True}),\n",
        "\n",
        "        # 계층 4: 모델 기반 안전성 검사 (after agent)\n",
        "        safety_guardrail,\n",
        "    ],\n",
        "    checkpointer=InMemorySaver(),\n",
        ")\n",
        "\n",
        "config = {\"configurable\": {\"thread_id\": \"secure_thread\"}}\n",
        "\n",
        "# 모든 계층의 가드레일을 통과해야 함\n",
        "result = agent.invoke(\n",
        "    {\"messages\": [{\"role\": \"user\", \"content\": \"Send an email to team@example.com\"}]},\n",
        "    config=config\n",
        ")\n",
        "\n",
        "print(result[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 종합 예제: 보안 고객 서비스 에이전트\n",
        "\n",
        "여러 가드레일을 결합한 실용적인 예제입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.agents import create_agent\n",
        "from langchain.agents.middleware import PIIMiddleware, AgentMiddleware, hook_config\n",
        "from langchain.tools import tool\n",
        "from typing import Any\n",
        "\n",
        "@tool\n",
        "def lookup_customer(customer_id: str) -> str:\n",
        "    \"\"\"Look up customer information.\"\"\"\n",
        "    return f\"Customer {customer_id}: John Doe, Premium member\"\n",
        "\n",
        "@tool\n",
        "def process_refund(order_id: str, amount: float) -> str:\n",
        "    \"\"\"Process a refund for an order.\"\"\"\n",
        "    return f\"Refund of ${amount} processed for order {order_id}\"\n",
        "\n",
        "@tool\n",
        "def send_notification(customer_id: str, message: str) -> str:\n",
        "    \"\"\"Send notification to customer.\"\"\"\n",
        "    return f\"Notification sent to customer {customer_id}\"\n",
        "\n",
        "# 속도 제한 가드레일\n",
        "class RateLimitMiddleware(AgentMiddleware):\n",
        "    def __init__(self, max_requests: int = 10):\n",
        "        super().__init__()\n",
        "        self.request_count = 0\n",
        "        self.max_requests = max_requests\n",
        "\n",
        "    @hook_config(can_jump_to=[\"end\"])\n",
        "    def before_agent(self, state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n",
        "        self.request_count += 1\n",
        "        if self.request_count > self.max_requests:\n",
        "            return {\n",
        "                \"messages\": [{\n",
        "                    \"role\": \"assistant\",\n",
        "                    \"content\": \"요청 한도를 초과했습니다. 잠시 후 다시 시도해주세요.\"\n",
        "                }],\n",
        "                \"jump_to\": \"end\"\n",
        "            }\n",
        "        return None\n",
        "\n",
        "# 출력 품질 검증 가드레일\n",
        "@after_agent\n",
        "def validate_output(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n",
        "    \"\"\"응답이 충분히 도움이 되는지 확인\"\"\"\n",
        "    if not state[\"messages\"]:\n",
        "        return None\n",
        "    \n",
        "    last_message = state[\"messages\"][-1]\n",
        "    if isinstance(last_message, AIMessage):\n",
        "        # 응답이 너무 짧으면 경고\n",
        "        if len(last_message.content) < 20:\n",
        "            print(\"Warning: Response may be too brief\")\n",
        "    \n",
        "    return None\n",
        "\n",
        "# 보안 고객 서비스 에이전트 생성\n",
        "secure_agent = create_agent(\n",
        "    model=model,\n",
        "    tools=[lookup_customer, process_refund, send_notification],\n",
        "    middleware=[\n",
        "        # 1. 입력 검증\n",
        "        RateLimitMiddleware(max_requests=100),\n",
        "        ContentFilterMiddleware(banned_keywords=[\"hack\", \"fraud\"]),\n",
        "        \n",
        "        # 2. PII 보호\n",
        "        PIIMiddleware(\"email\", strategy=\"redact\", apply_to_input=True),\n",
        "        PIIMiddleware(\"credit_card\", strategy=\"mask\", apply_to_input=True),\n",
        "        PIIMiddleware(\"email\", strategy=\"redact\", apply_to_output=True),\n",
        "        \n",
        "        # 3. 출력 검증\n",
        "        validate_output,\n",
        "    ],\n",
        ")\n",
        "\n",
        "# 테스트\n",
        "print(\"=== Test 1: Normal query ===\")\n",
        "result = secure_agent.invoke({\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"Look up customer CUST123\"}]\n",
        "})\n",
        "print(result[\"messages\"][-1].content)\n",
        "\n",
        "print(\"\\n=== Test 2: Query with PII ===\")\n",
        "result = secure_agent.invoke({\n",
        "    \"messages\": [{\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"Customer email is john@example.com and card is 4532-1234-5678-9010\"\n",
        "    }]\n",
        "})\n",
        "print(result[\"messages\"][-1].content)\n",
        "\n",
        "print(\"\\n=== Test 3: Blocked content ===\")\n",
        "result = secure_agent.invoke({\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"How to hack the system?\"}]\n",
        "})\n",
        "print(result[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 가드레일 설계 모범 사례\n",
        "\n",
        "### 1. 계층화된 방어 (Defense in Depth)\n",
        "\n",
        "여러 가드레일을 결합하여 다층 보호를 구현합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 좋은 예: 다층 보호\n",
        "agent = create_agent(\n",
        "    model=model,\n",
        "    tools=[search_tool],\n",
        "    middleware=[\n",
        "        # 입력 단계: 빠른 규칙 기반 필터\n",
        "        ContentFilterMiddleware(banned_keywords=[\"hack\"]),\n",
        "        PIIMiddleware(\"email\", strategy=\"redact\"),\n",
        "        \n",
        "        # 출력 단계: 느리지만 정확한 모델 기반 검증\n",
        "        safety_guardrail,\n",
        "    ],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. 성능 고려사항\n",
        "\n",
        "빠른 검사를 먼저 실행하여 불필요한 비용을 줄입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 좋은 예: 빠른 검사가 먼저\n",
        "agent = create_agent(\n",
        "    model=model,\n",
        "    tools=[search_tool],\n",
        "    middleware=[\n",
        "        ContentFilterMiddleware(banned_keywords=[\"hack\"]),  # 빠름\n",
        "        safety_guardrail,  # 느림 (LLM 호출)\n",
        "    ],\n",
        ")\n",
        "\n",
        "# 나쁜 예: 느린 검사가 먼저\n",
        "# agent = create_agent(\n",
        "#     model=model,\n",
        "#     tools=[search_tool],\n",
        "#     middleware=[\n",
        "#         safety_guardrail,  # 느림 - 모든 요청에 대해 실행됨\n",
        "#         ContentFilterMiddleware(banned_keywords=[\"hack\"]),  # 빠름\n",
        "#     ],\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. 명확한 에러 메시지\n",
        "\n",
        "사용자에게 왜 요청이 차단되었는지 명확하게 알려줍니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@before_agent(can_jump_to=[\"end\"])\n",
        "def clear_error_messages(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n",
        "    \"\"\"명확한 에러 메시지를 제공하는 가드레일\"\"\"\n",
        "    if not state[\"messages\"]:\n",
        "        return None\n",
        "\n",
        "    first_message = state[\"messages\"][0]\n",
        "    if first_message.type != \"human\":\n",
        "        return None\n",
        "\n",
        "    content = first_message.content.lower()\n",
        "\n",
        "    # 구체적인 피드백 제공\n",
        "    if \"password\" in content or \"비밀번호\" in content:\n",
        "        return {\n",
        "            \"messages\": [{\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": \"보안상의 이유로 비밀번호 관련 질문은 처리할 수 없습니다. 계정 설정에서 비밀번호를 재설정해주세요.\"\n",
        "            }],\n",
        "            \"jump_to\": \"end\"\n",
        "        }\n",
        "\n",
        "    return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. 로깅 및 모니터링\n",
        "\n",
        "가드레일이 트리거될 때 로그를 남겨 보안 위협을 추적합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import logging\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "@before_agent(can_jump_to=[\"end\"])\n",
        "def monitored_guardrail(state: AgentState, runtime: Runtime) -> dict[str, Any] | None:\n",
        "    \"\"\"로깅이 포함된 가드레일\"\"\"\n",
        "    if not state[\"messages\"]:\n",
        "        return None\n",
        "\n",
        "    first_message = state[\"messages\"][0]\n",
        "    if first_message.type != \"human\":\n",
        "        return None\n",
        "\n",
        "    content = first_message.content.lower()\n",
        "\n",
        "    if \"hack\" in content:\n",
        "        # 보안 이벤트 로깅\n",
        "        logger.warning(f\"Security violation detected: {content[:50]}...\")\n",
        "        \n",
        "        return {\n",
        "            \"messages\": [{\n",
        "                \"role\": \"assistant\",\n",
        "                \"content\": \"요청이 보안 정책에 위배됩니다.\"\n",
        "            }],\n",
        "            \"jump_to\": \"end\"\n",
        "        }\n",
        "\n",
        "    return None"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
