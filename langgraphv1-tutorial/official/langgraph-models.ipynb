{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# LangChain 모델\n",
                "\n",
                "LLM(대규모 언어 모델)은 사람처럼 텍스트를 해석하고 생성할 수 있는 강력한 AI 도구입니다. 각 작업에 대한 전문적인 훈련 없이도 콘텐츠 작성, 언어 번역, 요약 및 질문 응답에 사용할 수 있습니다.\n",
                "\n",
                "텍스트 생성 외에도 많은 모델이 다음을 지원합니다:\n",
                "\n",
                "- 도구 호출 - 외부 도구(데이터베이스 쿼리 또는 API 호출 등)를 호출하고 결과를 응답에 사용\n",
                "- 구조화된 출력 - 모델의 응답이 정의된 형식을 따르도록 제한\n",
                "- 멀티모달리티 - 이미지, 오디오, 비디오 등 텍스트가 아닌 데이터 처리 및 반환\n",
                "- 추론 - 결론에 도달하기 위한 다단계 추론 수행\n",
                "\n",
                "모델은 에이전트의 추론 엔진입니다. 에이전트의 의사 결정 프로세스를 주도하여 어떤 도구를 호출할지, 결과를 해석하는 방법, 최종 답변을 제공할 시기를 결정합니다."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 사전 준비\n",
                "\n",
                "환경 변수를 설정합니다."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from dotenv import load_dotenv\n",
                "\n",
                "load_dotenv(override=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 기본 사용법\n",
                "\n",
                "모델은 두 가지 방식으로 활용할 수 있습니다:\n",
                "\n",
                "1. **에이전트와 함께** - 에이전트를 생성할 때 모델을 동적으로 지정\n",
                "2. **독립 실행형** - 에이전트 프레임워크 없이 텍스트 생성, 분류 또는 추출 작업을 위해 모델을 직접 호출\n",
                "\n",
                "동일한 모델 인터페이스가 두 컨텍스트에서 모두 작동하므로 간단하게 시작하여 필요에 따라 더 복잡한 에이전트 기반 워크플로로 확장할 수 있습니다."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 모델 초기화\n",
                "\n",
                "LangChain에서 독립 실행형 모델을 시작하는 가장 쉬운 방법은 `init_chat_model`을 사용하여 선택한 채팅 모델 제공자로부터 모델을 초기화하는 것입니다."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### OpenAI"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "from langchain.chat_models import init_chat_model\n",
                "\n",
                "# init_chat_model 사용\n",
                "model = init_chat_model(\"openai:gpt-4.1-mini\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 또는 모델 클래스 직접 사용\n",
                "from langchain_openai import ChatOpenAI\n",
                "\n",
                "model = ChatOpenAI(model=\"gpt-4.1-mini\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Anthropic"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain.chat_models import init_chat_model\n",
                "\n",
                "model = init_chat_model(\"anthropic:claude-sonnet-4-5\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 또는 모델 클래스 직접 사용\n",
                "from langchain_anthropic import ChatAnthropic\n",
                "\n",
                "model = ChatAnthropic(model=\"claude-sonnet-4-5\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "#### Google Gemini"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain.chat_models import init_chat_model\n",
                "\n",
                "model = init_chat_model(\"google_genai:gemini-2.5-flash-lite\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 모델 호출"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "response = model.invoke(\"Why do parrots talk?\")\n",
                "print(response)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 주요 메서드\n",
                "\n",
                "1. **Invoke** - 모델이 메시지를 입력으로 받아 완전한 응답을 생성한 후 메시지를 출력합니다.\n",
                "2. **Stream** - 모델을 호출하지만 실시간으로 생성되는 출력을 스트리밍합니다.\n",
                "3. **Batch** - 더 효율적인 처리를 위해 여러 요청을 일괄로 모델에 전송합니다."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 매개변수\n",
                "\n",
                "채팅 모델은 동작을 구성하는 데 사용할 수 있는 매개변수를 받습니다. 지원되는 전체 매개변수 세트는 모델 및 제공자에 따라 다르지만 표준 매개변수는 다음과 같습니다:\n",
                "\n",
                "- `model`: 사용하려는 특정 모델의 이름 또는 식별자\n",
                "- `api_key`: 모델 제공자와 인증하는 데 필요한 키\n",
                "- `temperature`: 모델 출력의 무작위성 제어 (높을수록 더 창의적)\n",
                "- `timeout`: 요청을 취소하기 전에 모델 응답을 기다리는 최대 시간(초)\n",
                "- `max_tokens`: 응답의 총 토큰 수 제한\n",
                "- `max_retries`: 실패 시 요청을 재전송하는 최대 시도 횟수"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 매개변수를 사용하여 모델 초기화\n",
                "model = init_chat_model(\n",
                "    \"anthropic:claude-sonnet-4-5\",\n",
                "    temperature=0.7,    # 응답의 창의성 제어\n",
                "    timeout=30,         # 요청 타임아웃\n",
                "    max_tokens=1000,    # 최대 생성 토큰 수\n",
                ")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 호출 방법\n",
                "\n",
                "### Invoke\n",
                "\n",
                "모델을 호출하는 가장 간단한 방법은 단일 메시지 또는 메시지 목록과 함께 `invoke()`를 사용하는 것입니다."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 단일 메시지\n",
                "response = model.invoke(\"Why do parrots have colorful feathers?\")\n",
                "print(response)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 대화 기록을 나타내는 메시지 목록\n",
                "conversation = [\n",
                "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that translates English to French.\"},\n",
                "    {\"role\": \"user\", \"content\": \"Translate: I love programming.\"},\n",
                "    {\"role\": \"assistant\", \"content\": \"J'adore la programmation.\"},\n",
                "    {\"role\": \"user\", \"content\": \"Translate: I love building applications.\"}\n",
                "]\n",
                "\n",
                "response = model.invoke(conversation)\n",
                "print(response)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 메시지 객체 사용\n",
                "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
                "\n",
                "conversation = [\n",
                "    SystemMessage(\"You are a helpful assistant that translates English to French.\"),\n",
                "    HumanMessage(\"Translate: I love programming.\"),\n",
                "    AIMessage(\"J'adore la programmation.\"),\n",
                "    HumanMessage(\"Translate: I love building applications.\")\n",
                "]\n",
                "\n",
                "response = model.invoke(conversation)\n",
                "print(response)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Stream\n",
                "\n",
                "대부분의 모델은 생성되는 동안 출력 콘텐츠를 스트리밍할 수 있습니다. 출력을 점진적으로 표시함으로써 스트리밍은 특히 긴 응답에 대해 사용자 경험을 크게 향상시킵니다."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 기본 텍스트 스트리밍\n",
                "for chunk in model.stream(\"Why do parrots have colorful feathers?\"):\n",
                "    print(chunk.content, end=\"\", flush=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 전체 메시지를 구성하기 위해 청크 누적\n",
                "full = None\n",
                "for chunk in model.stream(\"What color is the sky?\"):\n",
                "    full = chunk if full is None else full + chunk\n",
                "    print(full.content)\n",
                "\n",
                "# 결과 메시지는 invoke()로 생성된 메시지와 동일하게 처리될 수 있습니다\n",
                "print(\"\\n최종 메시지:\")\n",
                "print(full)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Batch\n",
                "\n",
                "독립적인 요청 모음을 일괄 처리하면 처리가 병렬로 수행될 수 있으므로 성능이 크게 향상되고 비용이 절감됩니다."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 여러 요청을 일괄 처리\n",
                "responses = model.batch([\n",
                "    \"Why do parrots have colorful feathers?\",\n",
                "    \"How do airplanes fly?\",\n",
                "    \"What is quantum computing?\"\n",
                "])\n",
                "\n",
                "for response in responses:\n",
                "    print(response)\n",
                "    print(\"---\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 완료 시 일괄 응답 산출\n",
                "for response in model.batch_as_completed([\n",
                "    \"Why do parrots have colorful feathers?\",\n",
                "    \"How do airplanes fly?\",\n",
                "    \"What is quantum computing?\"\n",
                "]):\n",
                "    print(response)\n",
                "    print(\"---\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 도구 호출\n",
                "\n",
                "모델은 데이터베이스에서 데이터를 가져오거나, 웹을 검색하거나, 코드를 실행하는 등의 작업을 수행하는 도구를 호출하도록 요청할 수 있습니다.\n",
                "\n",
                "정의한 도구를 모델에서 사용할 수 있도록 하려면 `bind_tools()`를 사용하여 바인딩해야 합니다."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain.tools import tool\n",
                "\n",
                "@tool\n",
                "def get_weather(location: str) -> str:\n",
                "    \"\"\"Get the weather at a location.\"\"\"\n",
                "    return f\"It's sunny in {location}.\"\n",
                "\n",
                "# 도구를 모델에 바인딩\n",
                "model_with_tools = model.bind_tools([get_weather])\n",
                "\n",
                "response = model_with_tools.invoke(\"What's the weather like in Boston?\")\n",
                "\n",
                "# 모델이 수행한 도구 호출 확인\n",
                "for tool_call in response.tool_calls:\n",
                "    print(f\"Tool: {tool_call['name']}\")\n",
                "    print(f\"Args: {tool_call['args']}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 도구 실행 루프\n",
                "\n",
                "모델이 도구 호출을 반환하면 도구를 실행하고 결과를 모델에 다시 전달해야 합니다. 이렇게 하면 모델이 도구 결과를 사용하여 최종 응답을 생성할 수 있는 대화 루프가 생성됩니다."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 도구를 모델에 바인딩\n",
                "model_with_tools = model.bind_tools([get_weather])\n",
                "\n",
                "# 1단계: 모델이 도구 호출 생성\n",
                "messages = [{\"role\": \"user\", \"content\": \"What's the weather in Boston?\"}]\n",
                "ai_msg = model_with_tools.invoke(messages)\n",
                "messages.append(ai_msg)\n",
                "\n",
                "# 2단계: 도구 실행 및 결과 수집\n",
                "for tool_call in ai_msg.tool_calls:\n",
                "    # 생성된 인수로 도구 실행\n",
                "    tool_result = get_weather.invoke(tool_call)\n",
                "    messages.append(tool_result)\n",
                "\n",
                "# 3단계: 최종 응답을 위해 결과를 모델에 다시 전달\n",
                "final_response = model_with_tools.invoke(messages)\n",
                "print(final_response.content)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 병렬 도구 호출\n",
                "\n",
                "많은 모델이 적절한 경우 여러 도구를 병렬로 호출하는 것을 지원합니다. 이를 통해 모델은 서로 다른 소스에서 동시에 정보를 수집할 수 있습니다."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "model_with_tools = model.bind_tools([get_weather])\n",
                "\n",
                "response = model_with_tools.invoke(\n",
                "    \"What's the weather in Boston and Tokyo?\"\n",
                ")\n",
                "\n",
                "# 모델이 여러 도구 호출을 생성할 수 있습니다\n",
                "print(response.tool_calls)\n",
                "# [\n",
                "#   {'name': 'get_weather', 'args': {'location': 'Boston'}, 'id': 'call_1'},\n",
                "#   {'name': 'get_weather', 'args': {'location': 'Tokyo'}, 'id': 'call_2'},\n",
                "# ]\n",
                "\n",
                "# 모든 도구 실행 (비동기로 병렬 처리 가능)\n",
                "results = []\n",
                "for tool_call in response.tool_calls:\n",
                "    if tool_call['name'] == 'get_weather':\n",
                "        result = get_weather.invoke(tool_call)\n",
                "        results.append(result)\n",
                "\n",
                "print(results)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 구조화된 출력\n",
                "\n",
                "모델이 주어진 스키마와 일치하는 형식으로 응답을 제공하도록 요청할 수 있습니다. 이는 출력을 쉽게 구문 분석하고 후속 처리에 사용할 수 있도록 하는 데 유용합니다."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Pydantic 모델"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pydantic import BaseModel, Field\n",
                "\n",
                "class Movie(BaseModel):\n",
                "    \"\"\"A movie with details.\"\"\"\n",
                "    title: str = Field(..., description=\"The title of the movie\")\n",
                "    year: int = Field(..., description=\"The year the movie was released\")\n",
                "    director: str = Field(..., description=\"The director of the movie\")\n",
                "    rating: float = Field(..., description=\"The movie's rating out of 10\")\n",
                "\n",
                "model_with_structure = model.with_structured_output(Movie)\n",
                "response = model_with_structure.invoke(\"Provide details about the movie Inception\")\n",
                "print(response)\n",
                "# Movie(title=\"Inception\", year=2010, director=\"Christopher Nolan\", rating=8.8)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### TypedDict"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from typing_extensions import TypedDict, Annotated\n",
                "\n",
                "class MovieDict(TypedDict):\n",
                "    \"\"\"A movie with details.\"\"\"\n",
                "    title: Annotated[str, ..., \"The title of the movie\"]\n",
                "    year: Annotated[int, ..., \"The year the movie was released\"]\n",
                "    director: Annotated[str, ..., \"The director of the movie\"]\n",
                "    rating: Annotated[float, ..., \"The movie's rating out of 10\"]\n",
                "\n",
                "model_with_structure = model.with_structured_output(MovieDict)\n",
                "response = model_with_structure.invoke(\"Provide details about the movie Inception\")\n",
                "print(response)\n",
                "# {'title': 'Inception', 'year': 2010, 'director': 'Christopher Nolan', 'rating': 8.8}"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 중첩된 구조"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pydantic import BaseModel, Field\n",
                "\n",
                "class Actor(BaseModel):\n",
                "    name: str\n",
                "    role: str\n",
                "\n",
                "class MovieDetails(BaseModel):\n",
                "    title: str\n",
                "    year: int\n",
                "    cast: list[Actor]\n",
                "    genres: list[str]\n",
                "    budget: float | None = Field(None, description=\"Budget in millions USD\")\n",
                "\n",
                "model_with_structure = model.with_structured_output(MovieDetails)\n",
                "response = model_with_structure.invoke(\n",
                "    \"Provide detailed information about the movie Inception including cast and genres\"\n",
                ")\n",
                "print(response)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 고급 주제\n",
                "\n",
                "### 멀티모달\n",
                "\n",
                "특정 모델은 이미지, 오디오, 비디오 등 텍스트가 아닌 데이터를 처리하고 반환할 수 있습니다. 콘텐츠 블록을 제공하여 텍스트가 아닌 데이터를 모델에 전달할 수 있습니다."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 멀티모달 입력 예시 (이미지 URL 포함)\n",
                "message = [\n",
                "    {\n",
                "        \"role\": \"user\",\n",
                "        \"content\": [\n",
                "            {\"type\": \"text\", \"text\": \"What's in this image?\"},\n",
                "            {\"type\": \"image_url\", \"image_url\": {\"url\": \"https://example.com/image.jpg\"}}\n",
                "        ]\n",
                "    }\n",
                "]\n",
                "\n",
                "# response = model.invoke(message)\n",
                "# print(response.content)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 추론\n",
                "\n",
                "최신 모델은 결론에 도달하기 위해 다단계 추론을 수행할 수 있습니다. 기본 모델에서 지원되는 경우 이 추론 프로세스를 표시하여 모델이 최종 답변에 도달한 방법을 더 잘 이해할 수 있습니다."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 추론 출력 스트리밍\n",
                "for chunk in model.stream(\"Why do parrots have colorful feathers?\"):\n",
                "    reasoning_steps = [r for r in chunk.content_blocks if r.get(\"type\") == \"reasoning\"]\n",
                "    if reasoning_steps:\n",
                "        print(reasoning_steps)\n",
                "    else:\n",
                "        print(chunk.content)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 토큰 사용량\n",
                "\n",
                "많은 모델 제공자가 호출 응답의 일부로 토큰 사용량 정보를 반환합니다. 사용 가능한 경우 이 정보는 해당 모델이 생성한 AIMessage 객체에 포함됩니다."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain.chat_models import init_chat_model\n",
                "from langchain_core.callbacks import UsageMetadataCallbackHandler\n",
                "\n",
                "model_1 = init_chat_model(model=\"openai:gpt-4.1-mini\")\n",
                "model_2 = init_chat_model(model=\"anthropic:claude-3-5-haiku-latest\")\n",
                "\n",
                "# 콜백 핸들러를 사용하여 토큰 사용량 추적\n",
                "callback = UsageMetadataCallbackHandler()\n",
                "result_1 = model_1.invoke(\"Hello\", config={\"callbacks\": [callback]})\n",
                "result_2 = model_2.invoke(\"Hello\", config={\"callbacks\": [callback]})\n",
                "\n",
                "print(callback.usage_metadata)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 호출 구성\n",
                "\n",
                "모델을 호출할 때 `config` 매개변수를 통해 추가 구성을 전달할 수 있습니다. 이를 통해 실행 동작, 콜백 및 메타데이터 추적을 런타임에 제어할 수 있습니다."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# 구성을 사용한 호출\n",
                "response = model.invoke(\n",
                "    \"Tell me a joke\",\n",
                "    config={\n",
                "        \"run_name\": \"joke_generation\",      # 이 실행의 커스텀 이름\n",
                "        \"tags\": [\"humor\", \"demo\"],          # 분류를 위한 태그\n",
                "        \"metadata\": {\"user_id\": \"123\"},     # 커스텀 메타데이터\n",
                "    }\n",
                ")\n",
                "\n",
                "print(response.content)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 구성 가능한 모델\n",
                "\n",
                "`configurable_fields`를 지정하여 런타임 구성 가능한 모델을 만들 수 있습니다. 모델 값을 지정하지 않으면 `model` 및 `model_provider`가 기본적으로 구성 가능합니다."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from langchain.chat_models import init_chat_model\n",
                "\n",
                "# 구성 가능한 모델 생성\n",
                "configurable_model = init_chat_model(temperature=0)\n",
                "\n",
                "# 다른 모델로 실행\n",
                "response1 = configurable_model.invoke(\n",
                "    \"what's your name\",\n",
                "    config={\"configurable\": {\"model\": \"gpt-4.1-mini\"}},\n",
                ")\n",
                "print(\"gpt-4.1-mini:\", response1.content)\n",
                "\n",
                "response2 = configurable_model.invoke(\n",
                "    \"what's your name\",\n",
                "    config={\"configurable\": {\"model\": \"claude-sonnet-4-5\"}},\n",
                ")\n",
                "print(\"Claude:\", response2.content)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}
