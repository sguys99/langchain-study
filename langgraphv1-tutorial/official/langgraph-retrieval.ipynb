{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Retrieval (검색)\n",
        "\n",
        "대형 언어 모델(LLM)은 강력하지만 두 가지 주요 한계가 있습니다:\n",
        "\n",
        "- **제한된 컨텍스트**: 전체 코퍼스를 한 번에 처리할 수 없음\n",
        "- **정적 지식**: 훈련 데이터가 특정 시점에 고정됨\n",
        "\n",
        "**Retrieval(검색)**은 쿼리 시점에 관련 외부 지식을 가져와서 이러한 문제를 해결합니다. 이것이 **Retrieval-Augmented Generation (RAG)**의 기초입니다: 컨텍스트별 정보로 LLM의 답변을 향상시킵니다.\n",
        "\n",
        "## RAG의 핵심 개념\n",
        "\n",
        "RAG는 검색과 생성을 결합하여 근거 있고 컨텍스트를 인식하는 답변을 생성합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 사전 준비\n",
        "\n",
        "환경 변수를 설정합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "\n",
        "load_dotenv(override=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 필요한 라이브러리 설치\n",
        "\n",
        "```bash\n",
        "pip install langchain-openai faiss-cpu langchain-community\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 지식 베이스 구축\n",
        "\n",
        "**지식 베이스**는 검색 중에 사용되는 문서 또는 구조화된 데이터의 저장소입니다.\n",
        "\n",
        "### 검색 파이프라인\n",
        "\n",
        "일반적인 검색 워크플로우:\n",
        "\n",
        "1. **문서 로드**: 외부 소스에서 데이터 수집\n",
        "2. **청크로 분할**: 큰 문서를 작은 조각으로 분할\n",
        "3. **임베딩 생성**: 텍스트를 벡터로 변환\n",
        "4. **벡터 저장소에 저장**: 벡터를 검색 가능한 데이터베이스에 저장\n",
        "5. **쿼리 임베딩**: 사용자 질문을 벡터로 변환\n",
        "6. **검색**: 유사한 벡터 찾기\n",
        "7. **LLM에 전달**: 검색된 정보로 답변 생성"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 간단한 지식 베이스 생성"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "# 샘플 문서\n",
        "documents = [\n",
        "    Document(\n",
        "        page_content=\"LangChain은 LLM 애플리케이션 개발을 위한 프레임워크입니다. 도구, 에이전트, 메모리 등을 제공합니다.\",\n",
        "        metadata={\"source\": \"docs\", \"topic\": \"langchain\"}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"LangGraph는 상태 기반 에이전트를 구축하기 위한 라이브러리입니다. LangChain 위에 구축되었습니다.\",\n",
        "        metadata={\"source\": \"docs\", \"topic\": \"langgraph\"}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"RAG는 Retrieval-Augmented Generation의 약자로, 외부 지식을 검색하여 LLM 응답을 향상시킵니다.\",\n",
        "        metadata={\"source\": \"docs\", \"topic\": \"rag\"}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"벡터 데이터베이스는 임베딩을 저장하고 검색하는 데 특화된 데이터베이스입니다. FAISS, Pinecone 등이 있습니다.\",\n",
        "        metadata={\"source\": \"docs\", \"topic\": \"vectordb\"}\n",
        "    ),\n",
        "]\n",
        "\n",
        "# 텍스트 분할기 (큰 문서의 경우)\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=50\n",
        ")\n",
        "splits = text_splitter.split_documents(documents)\n",
        "\n",
        "# 임베딩 및 벡터 저장소 생성\n",
        "embeddings = OpenAIEmbeddings()\n",
        "vectorstore = FAISS.from_documents(splits, embeddings)\n",
        "\n",
        "print(f\"Created vector store with {len(splits)} documents\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 벡터 저장소에서 검색"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 검색 테스트\n",
        "query = \"LangGraph란 무엇인가?\"\n",
        "docs = vectorstore.similarity_search(query, k=2)\n",
        "\n",
        "print(f\"Query: {query}\\n\")\n",
        "for i, doc in enumerate(docs, 1):\n",
        "    print(f\"Result {i}:\")\n",
        "    print(f\"Content: {doc.page_content}\")\n",
        "    print(f\"Metadata: {doc.metadata}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RAG 아키텍처\n",
        "\n",
        "RAG는 시스템의 필요에 따라 여러 방식으로 구현할 수 있습니다.\n",
        "\n",
        "| 아키텍처 | 설명 | 제어 | 유연성 | 지연 시간 | 사용 사례 |\n",
        "|---------|------|------|-------|----------|----------|\n",
        "| **2-Step RAG** | 검색이 항상 생성 전에 발생. 간단하고 예측 가능 | ✅ 높음 | ❌ 낮음 | ⚡ 빠름 | FAQ, 문서 봇 |\n",
        "| **Agentic RAG** | LLM 기반 에이전트가 추론 중 검색 시점과 방법을 결정 | ❌ 낮음 | ✅ 높음 | ⏳ 가변적 | 다중 도구를 사용하는 연구 어시스턴트 |\n",
        "| **Hybrid** | 검증 단계를 포함하여 두 접근 방식의 특성을 결합 | ⚖️ 중간 | ⚖️ 중간 | ⏳ 가변적 | 품질 검증이 필요한 도메인별 Q&A |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2-Step RAG\n",
        "\n",
        "**2-Step RAG**에서는 검색 단계가 항상 생성 단계 전에 실행됩니다. 이 아키텍처는 간단하고 예측 가능합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "# Retriever 생성\n",
        "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
        "\n",
        "# 프롬프트 템플릿\n",
        "template = \"\"\"다음 컨텍스트를 사용하여 질문에 답하세요:\n",
        "\n",
        "컨텍스트:\n",
        "{context}\n",
        "\n",
        "질문: {question}\n",
        "\n",
        "답변:\"\"\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "# LLM\n",
        "llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0)\n",
        "\n",
        "# 문서를 문자열로 포맷팅하는 함수\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "# RAG 체인 구성\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# 테스트\n",
        "question = \"LangGraph는 무엇이고 어떻게 사용하나요?\"\n",
        "answer = rag_chain.invoke(question)\n",
        "\n",
        "print(f\"Question: {question}\\n\")\n",
        "print(f\"Answer: {answer}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2-Step RAG의 장단점\n",
        "\n",
        "**장점**:\n",
        "- 간단하고 예측 가능\n",
        "- 빠른 응답 시간\n",
        "- 구현이 쉬움\n",
        "\n",
        "**단점**:\n",
        "- 유연성 부족\n",
        "- 항상 검색을 수행 (불필요한 경우에도)\n",
        "- 다중 단계 추론 제한적"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Agentic RAG\n",
        "\n",
        "**Agentic RAG**는 에이전트가 추론 과정에서 **언제** 그리고 **어떻게** 정보를 검색할지 결정합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.agents import create_agent\n",
        "from langchain.tools import tool\n",
        "\n",
        "# 검색 도구 생성\n",
        "@tool\n",
        "def search_knowledge_base(query: str) -> str:\n",
        "    \"\"\"Search the knowledge base for relevant information.\"\"\"\n",
        "    docs = vectorstore.similarity_search(query, k=2)\n",
        "    return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
        "\n",
        "# Agentic RAG 에이전트\n",
        "agent = create_agent(\n",
        "    model=llm,\n",
        "    tools=[search_knowledge_base],\n",
        "    system_prompt=\"\"\"당신은 도움이 되는 AI 어시스턴트입니다.\n",
        "    질문에 답하기 위해 외부 정보가 필요하면 search_knowledge_base 도구를 사용하세요.\n",
        "    검색 결과를 기반으로 명확하고 정확한 답변을 제공하세요.\"\"\"\n",
        ")\n",
        "\n",
        "# 테스트\n",
        "result = agent.invoke({\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"RAG에 대해 설명해주세요.\"}]\n",
        "})\n",
        "\n",
        "print(result[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Agentic RAG의 장단점\n",
        "\n",
        "**장점**:\n",
        "- 높은 유연성\n",
        "- 필요할 때만 검색\n",
        "- 다중 단계 추론 가능\n",
        "- 여러 도구 조합 가능\n",
        "\n",
        "**단점**:\n",
        "- 예측 불가능한 지연 시간\n",
        "- 더 많은 LLM 호출\n",
        "- 복잡한 구현"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 다중 도구를 사용하는 Agentic RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@tool\n",
        "def calculate(expression: str) -> str:\n",
        "    \"\"\"Calculate mathematical expressions.\"\"\"\n",
        "    try:\n",
        "        result = eval(expression)\n",
        "        return str(result)\n",
        "    except Exception as e:\n",
        "        return f\"Error: {str(e)}\"\n",
        "\n",
        "@tool\n",
        "def get_current_date() -> str:\n",
        "    \"\"\"Get the current date.\"\"\"\n",
        "    from datetime import datetime\n",
        "    return datetime.now().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "# 여러 도구를 가진 에이전트\n",
        "multi_tool_agent = create_agent(\n",
        "    model=llm,\n",
        "    tools=[search_knowledge_base, calculate, get_current_date],\n",
        "    system_prompt=\"\"\"당신은 다재다능한 AI 어시스턴트입니다.\n",
        "    - 지식 기반 질문: search_knowledge_base 사용\n",
        "    - 계산: calculate 사용\n",
        "    - 날짜 정보: get_current_date 사용\n",
        "    \n",
        "    적절한 도구를 선택하여 사용자 질문에 답하세요.\"\"\"\n",
        ")\n",
        "\n",
        "# 복합 질문 테스트\n",
        "questions = [\n",
        "    \"LangChain이 무엇인지 설명하고, 오늘 날짜를 알려주세요.\",\n",
        "    \"벡터 데이터베이스에 대해 설명하고, 10 * 25를 계산해주세요.\",\n",
        "]\n",
        "\n",
        "for question in questions:\n",
        "    print(f\"\\n=== Question: {question} ===\")\n",
        "    result = multi_tool_agent.invoke({\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": question}]\n",
        "    })\n",
        "    print(f\"Answer: {result['messages'][-1].content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Hybrid RAG\n",
        "\n",
        "Hybrid RAG는 2-Step과 Agentic RAG의 특성을 결합합니다. 쿼리 전처리, 검색 검증, 생성 후 검사 등의 중간 단계를 도입합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "# 1. 쿼리 향상\n",
        "query_enhancement_prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"사용자 질문을 더 명확하고 구체적으로 재작성하세요.\n",
        "    검색에 최적화된 형태로 만드세요.\n",
        "    \n",
        "    원본 질문: {question}\n",
        "    \n",
        "    향상된 질문:\"\"\"\n",
        ")\n",
        "\n",
        "# 2. 검색 검증\n",
        "validation_prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"다음 검색 결과가 질문에 답하기에 충분한지 평가하세요.\n",
        "    \n",
        "    질문: {question}\n",
        "    \n",
        "    검색 결과:\n",
        "    {results}\n",
        "    \n",
        "    충분하면 'YES', 불충분하면 'NO'라고만 답하세요.\"\"\"\n",
        ")\n",
        "\n",
        "# 3. 답변 생성\n",
        "answer_prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"검색된 정보를 바탕으로 질문에 답하세요.\n",
        "    \n",
        "    질문: {question}\n",
        "    \n",
        "    컨텍스트:\n",
        "    {context}\n",
        "    \n",
        "    답변:\"\"\"\n",
        ")\n",
        "\n",
        "def hybrid_rag(question: str, max_iterations: int = 2) -> str:\n",
        "    \"\"\"Hybrid RAG with query enhancement and validation.\"\"\"\n",
        "    \n",
        "    # 1. 쿼리 향상\n",
        "    enhanced_query = (query_enhancement_prompt | llm | StrOutputParser()).invoke(\n",
        "        {\"question\": question}\n",
        "    )\n",
        "    print(f\"Enhanced query: {enhanced_query}\\n\")\n",
        "    \n",
        "    for i in range(max_iterations):\n",
        "        # 2. 검색\n",
        "        docs = vectorstore.similarity_search(enhanced_query, k=3)\n",
        "        context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
        "        \n",
        "        print(f\"Iteration {i+1}: Retrieved {len(docs)} documents\")\n",
        "        \n",
        "        # 3. 검색 검증\n",
        "        validation = (validation_prompt | llm | StrOutputParser()).invoke({\n",
        "            \"question\": question,\n",
        "            \"results\": context\n",
        "        })\n",
        "        \n",
        "        if \"YES\" in validation.upper():\n",
        "            print(\"Validation: PASSED\\n\")\n",
        "            break\n",
        "        else:\n",
        "            print(\"Validation: FAILED - Refining query\\n\")\n",
        "            # 쿼리 재작성 로직 (단순화)\n",
        "            enhanced_query = f\"{enhanced_query} (more specific)\"\n",
        "    \n",
        "    # 4. 답변 생성\n",
        "    answer = (answer_prompt | llm | StrOutputParser()).invoke({\n",
        "        \"question\": question,\n",
        "        \"context\": context\n",
        "    })\n",
        "    \n",
        "    return answer\n",
        "\n",
        "# 테스트\n",
        "question = \"LangChain과 관련된 기술들은?\"\n",
        "answer = hybrid_rag(question)\n",
        "\n",
        "print(f\"\\nFinal Answer:\\n{answer}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 실용적인 예제: 문서 Q&A 시스템\n",
        "\n",
        "실제 문서를 로드하고 검색하는 완전한 RAG 시스템을 구축해봅시다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 더 많은 문서 추가\n",
        "extended_documents = [\n",
        "    Document(\n",
        "        page_content=\"\"\"Python은 1991년 Guido van Rossum이 개발한 고수준 프로그래밍 언어입니다.\n",
        "        간결하고 읽기 쉬운 문법으로 유명하며, 데이터 과학, 웹 개발, 자동화 등 다양한 분야에서 사용됩니다.\"\"\",\n",
        "        metadata={\"source\": \"programming\", \"topic\": \"python\"}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"\"\"머신러닝은 데이터로부터 학습하는 알고리즘을 연구하는 인공지능의 한 분야입니다.\n",
        "        지도 학습, 비지도 학습, 강화 학습 등의 방법이 있으며, 이미지 인식, 자연어 처리 등에 활용됩니다.\"\"\",\n",
        "        metadata={\"source\": \"ai\", \"topic\": \"machine-learning\"}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"\"\"Transformer는 2017년 Google이 발표한 딥러닝 아키텍처입니다.\n",
        "        Self-attention 메커니즘을 사용하며, GPT, BERT 등 현대 LLM의 기초가 되었습니다.\"\"\",\n",
        "        metadata={\"source\": \"ai\", \"topic\": \"transformer\"}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"\"\"임베딩(Embedding)은 텍스트를 고차원 벡터 공간의 점으로 표현하는 기법입니다.\n",
        "        의미적으로 유사한 텍스트는 벡터 공간에서 가까이 위치합니다. Word2Vec, BERT embeddings 등이 있습니다.\"\"\",\n",
        "        metadata={\"source\": \"nlp\", \"topic\": \"embeddings\"}\n",
        "    ),\n",
        "]\n",
        "\n",
        "# 확장된 벡터 저장소 생성\n",
        "all_documents = documents + extended_documents\n",
        "extended_vectorstore = FAISS.from_documents(all_documents, embeddings)\n",
        "\n",
        "print(f\"Extended knowledge base with {len(all_documents)} documents\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 대화형 Q&A 시스템"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.agents import create_agent\n",
        "\n",
        "@tool\n",
        "def search_documents(query: str) -> str:\n",
        "    \"\"\"Search documents in the knowledge base.\"\"\"\n",
        "    docs = extended_vectorstore.similarity_search(query, k=3)\n",
        "    results = []\n",
        "    for doc in docs:\n",
        "        results.append(f\"[{doc.metadata['topic']}] {doc.page_content}\")\n",
        "    return \"\\n\\n\".join(results)\n",
        "\n",
        "# 문서 Q&A 에이전트\n",
        "qa_agent = create_agent(\n",
        "    model=llm,\n",
        "    tools=[search_documents],\n",
        "    system_prompt=\"\"\"당신은 지식 기반 Q&A 어시스턴트입니다.\n",
        "    \n",
        "    사용자 질문에 답할 때:\n",
        "    1. search_documents 도구를 사용하여 관련 정보를 찾으세요\n",
        "    2. 검색된 문서를 바탕으로 명확하고 정확한 답변을 제공하세요\n",
        "    3. 답변의 출처(토픽)를 언급하세요\n",
        "    4. 검색 결과가 불충분하면 이를 사용자에게 알리세요\n",
        "    \n",
        "    한국어로 답변하세요.\"\"\"\n",
        ")\n",
        "\n",
        "# 테스트 질문들\n",
        "test_questions = [\n",
        "    \"Python에 대해 설명해주세요.\",\n",
        "    \"Transformer 아키텍처는 무엇인가요?\",\n",
        "    \"임베딩이 무엇이고 어떻게 사용되나요?\",\n",
        "    \"RAG와 머신러닝의 관계는?\",\n",
        "]\n",
        "\n",
        "for question in test_questions:\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Q: {question}\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    result = qa_agent.invoke({\n",
        "        \"messages\": [{\"role\": \"user\", \"content\": question}]\n",
        "    })\n",
        "    \n",
        "    print(f\"A: {result['messages'][-1].content}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 고급 기능: 메타데이터 필터링\n",
        "\n",
        "메타데이터를 사용하여 검색을 더 정확하게 만들 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@tool\n",
        "def search_by_topic(query: str, topic: str) -> str:\n",
        "    \"\"\"Search documents filtered by topic.\n",
        "    \n",
        "    Available topics: python, machine-learning, transformer, embeddings, langchain, langgraph, rag, vectordb\n",
        "    \"\"\"\n",
        "    # 메타데이터 필터링\n",
        "    docs = extended_vectorstore.similarity_search(\n",
        "        query,\n",
        "        k=3,\n",
        "        filter={\"topic\": topic}\n",
        "    )\n",
        "    \n",
        "    if not docs:\n",
        "        return f\"No documents found for topic: {topic}\"\n",
        "    \n",
        "    results = []\n",
        "    for doc in docs:\n",
        "        results.append(doc.page_content)\n",
        "    return \"\\n\\n\".join(results)\n",
        "\n",
        "# 토픽 필터링 에이전트\n",
        "filtered_agent = create_agent(\n",
        "    model=llm,\n",
        "    tools=[search_by_topic],\n",
        "    system_prompt=\"\"\"당신은 토픽별 문서 검색 어시스턴트입니다.\n",
        "    \n",
        "    사용자 질문에서 관련 토픽을 파악하고 search_by_topic을 사용하세요.\n",
        "    가능한 토픽: python, machine-learning, transformer, embeddings, langchain, langgraph, rag, vectordb\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "# 테스트\n",
        "result = filtered_agent.invoke({\n",
        "    \"messages\": [{\"role\": \"user\", \"content\": \"Python에 대한 정보를 찾아주세요.\"}]\n",
        "})\n",
        "\n",
        "print(result[\"messages\"][-1].content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 모범 사례\n",
        "\n",
        "### 1. 적절한 청크 크기 선택\n",
        "\n",
        "- 너무 작으면: 컨텍스트 부족\n",
        "- 너무 크면: 노이즈 증가, 검색 정확도 감소\n",
        "- 일반적으로 500-1000 토큰이 적절"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 좋은 예\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=500,      # 적절한 크기\n",
        "    chunk_overlap=50     # 컨텍스트 연속성 유지\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. 검색 결과 수 조정\n",
        "\n",
        "- k 값이 너무 작으면: 중요한 정보 누락\n",
        "- k 값이 너무 크면: 노이즈 증가, 비용 증가"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 질문 유형에 따라 k 조정\n",
        "simple_query_k = 2  # 간단한 질문\n",
        "complex_query_k = 5  # 복잡한 질문"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. 프롬프트 최적화\n",
        "\n",
        "명확한 지시사항을 제공하세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 좋은 프롬프트\n",
        "good_prompt = \"\"\"다음 컨텍스트를 사용하여 질문에 답하세요.\n",
        "컨텍스트에 답이 없으면 '정보가 부족합니다'라고 답하세요.\n",
        "추측하지 마세요.\n",
        "\n",
        "컨텍스트: {context}\n",
        "질문: {question}\n",
        "답변:\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. 메타데이터 활용\n",
        "\n",
        "메타데이터로 검색 품질을 향상시키세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 유용한 메타데이터\n",
        "metadata = {\n",
        "    \"source\": \"document_name.pdf\",\n",
        "    \"page\": 5,\n",
        "    \"topic\": \"machine-learning\",\n",
        "    \"date\": \"2024-01-01\",\n",
        "    \"author\": \"John Doe\"\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 요약\n",
        "\n",
        "### RAG 아키텍처 선택 가이드\n",
        "\n",
        "**2-Step RAG를 선택하는 경우**:\n",
        "- 간단한 FAQ 시스템\n",
        "- 예측 가능한 지연 시간이 중요\n",
        "- 구현 복잡도를 최소화하고 싶을 때\n",
        "\n",
        "**Agentic RAG를 선택하는 경우**:\n",
        "- 복잡한 다단계 추론 필요\n",
        "- 여러 도구와 데이터 소스 활용\n",
        "- 유연성이 중요할 때\n",
        "\n",
        "**Hybrid RAG를 선택하는 경우**:\n",
        "- 품질 검증이 중요\n",
        "- 쿼리 향상이 필요\n",
        "- 중간 수준의 복잡도와 제어가 필요할 때\n",
        "\n",
        "### 핵심 요점\n",
        "\n",
        "1. **지식 베이스 구축**: 문서 로드 → 청크 분할 → 임베딩 → 벡터 저장소\n",
        "2. **검색**: 쿼리 임베딩 → 유사도 검색 → 관련 문서 반환\n",
        "3. **생성**: 검색된 컨텍스트 + 질문 → LLM → 답변\n",
        "4. **최적화**: 청크 크기, k 값, 프롬프트, 메타데이터 활용\n",
        "\n",
        "RAG는 LLM의 지식을 확장하고 최신 정보를 제공하는 강력한 패턴입니다."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
