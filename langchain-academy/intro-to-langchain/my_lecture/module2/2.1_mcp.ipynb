{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "717edf63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(\"../../.env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db64c0fd-9355-49e4-8290-2f2ae08eeef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import asyncio\n",
    "\n",
    "# Fix for Windows issues in Jupyter notebooks\n",
    "if sys.platform == \"win32\":\n",
    "    # 1. Use ProactorEventLoop for subprocess support\n",
    "    if not isinstance(asyncio.get_event_loop_policy(), asyncio.WindowsProactorEventLoopPolicy):\n",
    "        asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())\n",
    "    \n",
    "    # 2. Redirect stderr to avoid fileno() error when launching MCP servers\n",
    "    if \"ipykernel\" in sys.modules:\n",
    "        sys.stderr = sys.__stderr__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d701224",
   "metadata": {},
   "source": [
    "## Local MCP server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11678d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "\n",
    "# ì—¬ëŸ¬ ì„œë²„ë¥¼ ë‹¤ë£° ìˆ˜ ìˆëŠ” mcp í´ë¼ì´ì–¸íŠ¸ë¥¼ ìƒì„±\n",
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"local_server\": { # resourceì— ìˆëŠ” ì„œë²„ì˜ ë³„ì¹­\n",
    "                \"transport\": \"stdio\", # í‘œì¤€ ì…ì¶œë ¥ìœ¼ë¡œ MCPì™€ í†µì‹ , ë³„ë„ ë„¤íŠ¸ì›Œí¬ í¬íŠ¸ê°€ ì•„ë‹ˆë¼ í”„ë¡œì„¸ìŠ¤ê°„ IOë¡œ ì—°ê²°, stdio ì•„ë‹ˆë©´ http ì¤‘ í•˜ë‚˜ì¼ ê²ƒì´ë‹¤.\n",
    "                \"command\": \"python\", # ì„œë²„ë¥¼ ì‹¤í–‰í•  ëª…ë ¹ì–´\n",
    "                \"args\": [\"resources/2.1_mcp_server.py\"], # ì‹¤í–‰í•  ìŠ¤í¬ë¦½íŠ¸ ê²½ë¡œ\n",
    "            } # ì‹¤í–‰ê³¼ ë™ì‹œì— ì„œë²„ê°€ ì‹¤í–‰ë  ê²ƒ ì´ë‹¤.\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "184db1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get tools\n",
    "tools = await client.get_tools()\n",
    "\n",
    "# get resources\n",
    "resources = await client.get_resources(\"local_server\")\n",
    "\n",
    "# get prompts\n",
    "prompt = await client.get_prompt(\"local_server\", \"prompt\")\n",
    "prompt = prompt[0].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5ba7143",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    ë‹¹ì‹ ì€ Langchain, LangGraph, LangSmithì— ëŒ€í•œ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ì¹œì ˆí•œ ì–´ì‹œìŠ¤í„´íŠ¸ ì…ë‹ˆë‹¤.\\n\\n    ì§ˆë¬¸ì— ë‹µë³€í•˜ê¸° ìœ„í•´ ë‹¤ìŒ tools/resourcesë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n    - search_web: Search the web for information\\n    - github_file: Access the langchain-ai repo files\\n    \\n    ì‚¬ìš©ìê°€ LangChain, LangGraph or LangSmithì™€ ê´€ë ¨ ì—†ëŠ” ì§ˆë¬¸ì„ í•˜ë©´, \"ì£„ì†¡í•©ë‹ˆë‹¤. ì €ëŠ” ë­ì²´ì¸, ë­ê·¸ë ˆí”„, ë­ìŠ¤ë¯¸ìŠ¤ì— ëŒ€í•´ì„œë§Œ ë‹µë³€í• ìˆ˜ ìˆì–´ìš”.\" ë¼ê³  ë‹µë³€í•©ë‹ˆë‹¤.\\n    \\n    ë‹¹ì‹ ì€ ì‚¬ìš©ì ì§ˆë¬¸ì— ë‹µë³€ì„ ìœ„í•´ ë‹¤ìˆ˜ì˜ toolê³¼ resouceë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\\n    \\n    ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë³´ë‹¤ ì˜ ì´í•´í•˜ê¸° ìœ„í•´ ì‚¬ìš©ìì—ê²Œ ì§ˆë¬¸ì˜ ëª…í™•í•˜ê²Œ ì‹ë³„í•˜ê¸° ìœ„í•œ ì§ˆë¬¸ì„ í• ìˆ˜ ìˆìŠµë‹ˆë‹¤.(reasking)\\n    '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d548fad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"gpt-5-nano\",\n",
    "    tools=tools,\n",
    "    system_prompt=prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5256ae3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.messages import HumanMessage\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "response = await agent.ainvoke(\n",
    "    {\"messages\": [HumanMessage(content=\"langchain-mcp-adaptersì— ëŒ€í•´ì„œ ì•Œë ¤ì¤˜.\")]},\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3efb5bbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='langchain-mcp-adaptersì— ëŒ€í•´ì„œ ì•Œë ¤ì¤˜.', additional_kwargs={}, response_metadata={}, id='22bc9269-c318-4a90-a7d3-acb382e1e4e9'),\n",
      "              AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 156, 'prompt_tokens': 319, 'total_tokens': 475, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 128, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-D5KrkTmEFGDtEvlF8TRaC2gtbiUcO', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019c2602-6bef-77a2-af36-5cfc88a5c2cc-0', tool_calls=[{'name': 'search_web', 'args': {'query': 'langchain-mcp-adapters'}, 'id': 'call_jdyJEPgMVne0jLBpdmP2kXdb', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 319, 'output_tokens': 156, 'total_tokens': 475, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 128}}),\n",
      "              ToolMessage(content=[{'type': 'text', 'text': '{\\n  \"query\": \"langchain-mcp-adapters\",\\n  \"follow_up_questions\": null,\\n  \"answer\": null,\\n  \"images\": [],\\n  \"results\": [\\n    {\\n      \"url\": \"https://changelog.langchain.com/announcements/mcp-adapters-for-langchain-and-langgraph\",\\n      \"title\": \"MCP Adapters for LangChain and LangGraph\",\\n      \"content\": \"# LangChain Changelog. Sign up for our newsletter to stay up to date. # MCP Adapters for LangChain and LangGraph. The **LangChain MCP Adapters** is a package that makes it easy to use **Anthropic Model Context Protocol (MCP) tools** with LangChain & LangGraph. * Converts **MCP tools** into **LangChain- & LangGraph-compatible tools**. * Enables interaction with tools across multiple **MCP servers**. * Seamlessly integrates the **hundreds of tool servers** already published into LangGraph Agents. ### Why use MCP Adapters:. This adapter makes it simple to connect LangChain and LangGraph with the growing ecosystem of MCP tool servers. Instead of manually adapting each tool, you can now integrate them seamlessly. It also allows agents to pull from multiple MCP servers at once, making it easier to combine different tools for more powerful applications. MCP is gaining serious traction, and this adapter helps LangGraph agents take full advantage. ##### Subscribe to updates.\",\\n      \"score\": 0.93206495,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://docs.langchain.com/oss/javascript/langchain/mcp\",\\n      \"title\": \"Model Context Protocol (MCP) - Docs by LangChain\",\\n      \"content\": \"@langchain/mcp-adapters enables agents to use tools defined across one or more MCP servers. MultiServerMCPClient is stateless by default. Each tool invocation\",\\n      \"score\": 0.8896154,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://reference.langchain.com/python/langchain_mcp_adapters/\",\\n      \"title\": \"langchain-mcp-adapters\",\\n      \"content\": \"This module provides functionality to convert MCP tools into LangChain-compatible tools, handle tool execution, and manage tool conversion between the two\",\\n      \"score\": 0.86303437,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://github.com/langchain-ai/langchain-mcp-adapters\",\\n      \"title\": \"langchain-ai/langchain-mcp-adapters: LangChain ğŸ”Œ MCP\",\\n      \"content\": \"from langchain_mcp_adapters client import MultiServerMCPClient from langchain agents import create_agent client = MultiServerMCPClient \\\\\"math\\\\\" \\\\\"command\\\\\" \\\\\"python\\\\\"# Make sure to update to the full absolute path to your math_server.py file \\\\\"args\\\\\"\\\\\"/path/to/math_server.py\\\\\" \\\\\"transport\\\\\" \\\\\"stdio\\\\\" \\\\\"weather\\\\\" # Make sure you start your weather server on port 8000 \\\\\"url\\\\\"\\\\\"http://localhost:8000/mcp\\\\\" \\\\\"transport\\\\\" \\\\\"http\\\\\" tools = await client get_tools agent = create_agent\\\\\"openai:gpt-4.1\\\\\" tools math_response = await agent ainvoke \\\\\"messages\\\\\"\\\\\"what\\'s (3 + 5) x 12?\\\\\" weather_response = await agent ainvoke \\\\\"messages\\\\\" \\\\\"what is the weather in nyc?\\\\\". from langchain_mcp_adapters client import MultiServerMCPClient from langgraph graph import StateGraph MessagesState START from langgraph prebuilt import ToolNode tools_condition from langchain chat_models import init_chat_model model = init_chat_model\\\\\"openai:gpt-4.1\\\\\" client = MultiServerMCPClient \\\\\"math\\\\\" \\\\\"command\\\\\" \\\\\"python\\\\\"# Make sure to update to the full absolute path to your math_server.py file \\\\\"args\\\\\"\\\\\"./examples/math_server.py\\\\\" \\\\\"transport\\\\\" \\\\\"stdio\\\\\" \\\\\"weather\\\\\" # make sure you start your weather server on port 8000 \\\\\"url\\\\\"\\\\\"http://localhost:8000/mcp\\\\\" \\\\\"transport\\\\\" \\\\\"http\\\\\" tools = await client get_tools def call_model state MessagesState response = model bind_tools tools invoke state \\\\\"messages\\\\\" return \\\\\"messages\\\\\" response builder = StateGraph MessagesState builder add_node call_model builder add_node ToolNode tools builder add_edge START \\\\\"call_model\\\\\" builder add_conditional_edges \\\\\"call_model\\\\\" tools_condition builder add_edge \\\\\"tools\\\\\" \\\\\"call_model\\\\\" graph = builder compile math_response = await graph ainvoke \\\\\"messages\\\\\"\\\\\"what\\'s (3 + 5) x 12?\\\\\" weather_response = await graph ainvoke \\\\\"messages\\\\\" \\\\\"what is the weather in nyc?\\\\\".\",\\n      \"score\": 0.8565368,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://pypi.org/project/langchain-mcp-tools/\",\\n      \"title\": \"langchain-mcp-tools\",\\n      \"content\": \"LangChain\\'s official LangChain MCP Adapters library, which supports comprehensive integration with LangChain, has been released. You may want to consider using\",\\n      \"score\": 0.8473754,\\n      \"raw_content\": null\\n    }\\n  ],\\n  \"response_time\": 0.97,\\n  \"request_id\": \"f2c52e82-5b13-4438-9fe7-95e2f95b86d9\"\\n}', 'id': 'lc_9893ca26-2f1a-460f-b2d8-038c648843e1'}], name='search_web', id='24ffef6e-e1c9-43c9-bbdc-f4a29b590e57', tool_call_id='call_jdyJEPgMVne0jLBpdmP2kXdb', artifact={'structured_content': {'result': {'query': 'langchain-mcp-adapters', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://changelog.langchain.com/announcements/mcp-adapters-for-langchain-and-langgraph', 'title': 'MCP Adapters for LangChain and LangGraph', 'content': '# LangChain Changelog. Sign up for our newsletter to stay up to date. # MCP Adapters for LangChain and LangGraph. The **LangChain MCP Adapters** is a package that makes it easy to use **Anthropic Model Context Protocol (MCP) tools** with LangChain & LangGraph. * Converts **MCP tools** into **LangChain- & LangGraph-compatible tools**. * Enables interaction with tools across multiple **MCP servers**. * Seamlessly integrates the **hundreds of tool servers** already published into LangGraph Agents. ### Why use MCP Adapters:. This adapter makes it simple to connect LangChain and LangGraph with the growing ecosystem of MCP tool servers. Instead of manually adapting each tool, you can now integrate them seamlessly. It also allows agents to pull from multiple MCP servers at once, making it easier to combine different tools for more powerful applications. MCP is gaining serious traction, and this adapter helps LangGraph agents take full advantage. ##### Subscribe to updates.', 'score': 0.93206495, 'raw_content': None}, {'url': 'https://docs.langchain.com/oss/javascript/langchain/mcp', 'title': 'Model Context Protocol (MCP) - Docs by LangChain', 'content': '@langchain/mcp-adapters enables agents to use tools defined across one or more MCP servers. MultiServerMCPClient is stateless by default. Each tool invocation', 'score': 0.8896154, 'raw_content': None}, {'url': 'https://reference.langchain.com/python/langchain_mcp_adapters/', 'title': 'langchain-mcp-adapters', 'content': 'This module provides functionality to convert MCP tools into LangChain-compatible tools, handle tool execution, and manage tool conversion between the two', 'score': 0.86303437, 'raw_content': None}, {'url': 'https://github.com/langchain-ai/langchain-mcp-adapters', 'title': 'langchain-ai/langchain-mcp-adapters: LangChain ğŸ”Œ MCP', 'content': 'from langchain_mcp_adapters client import MultiServerMCPClient from langchain agents import create_agent client = MultiServerMCPClient \"math\" \"command\" \"python\"# Make sure to update to the full absolute path to your math_server.py file \"args\"\"/path/to/math_server.py\" \"transport\" \"stdio\" \"weather\" # Make sure you start your weather server on port 8000 \"url\"\"http://localhost:8000/mcp\" \"transport\" \"http\" tools = await client get_tools agent = create_agent\"openai:gpt-4.1\" tools math_response = await agent ainvoke \"messages\"\"what\\'s (3 + 5) x 12?\" weather_response = await agent ainvoke \"messages\" \"what is the weather in nyc?\". from langchain_mcp_adapters client import MultiServerMCPClient from langgraph graph import StateGraph MessagesState START from langgraph prebuilt import ToolNode tools_condition from langchain chat_models import init_chat_model model = init_chat_model\"openai:gpt-4.1\" client = MultiServerMCPClient \"math\" \"command\" \"python\"# Make sure to update to the full absolute path to your math_server.py file \"args\"\"./examples/math_server.py\" \"transport\" \"stdio\" \"weather\" # make sure you start your weather server on port 8000 \"url\"\"http://localhost:8000/mcp\" \"transport\" \"http\" tools = await client get_tools def call_model state MessagesState response = model bind_tools tools invoke state \"messages\" return \"messages\" response builder = StateGraph MessagesState builder add_node call_model builder add_node ToolNode tools builder add_edge START \"call_model\" builder add_conditional_edges \"call_model\" tools_condition builder add_edge \"tools\" \"call_model\" graph = builder compile math_response = await graph ainvoke \"messages\"\"what\\'s (3 + 5) x 12?\" weather_response = await graph ainvoke \"messages\" \"what is the weather in nyc?\".', 'score': 0.8565368, 'raw_content': None}, {'url': 'https://pypi.org/project/langchain-mcp-tools/', 'title': 'langchain-mcp-tools', 'content': \"LangChain's official LangChain MCP Adapters library, which supports comprehensive integration with LangChain, has been released. You may want to consider using\", 'score': 0.8473754, 'raw_content': None}], 'response_time': 0.97, 'request_id': 'f2c52e82-5b13-4438-9fe7-95e2f95b86d9'}}}),\n",
      "              AIMessage(content='ìš”ì•½\\n- LangChain MCP AdaptersëŠ” MCP(Anthropic Model Context Protocol) ë„êµ¬ë“¤ì„ LangChainê³¼ LangGraphì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ë³€í™˜í•´ ì£¼ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ì…ë‹ˆë‹¤.\\n- ì´ íŒ¨í‚¤ì§€ëŠ” í•˜ë‚˜ ì´ìƒì˜ MCP ì„œë²„ì— ìˆëŠ” ë„êµ¬ë“¤ì„ LangChain- ë° LangGraph-í˜¸í™˜ ë„êµ¬ë¡œ ë°”ê¿”ì£¼ê³ , ì—¬ëŸ¬ MCP ì„œë²„ì˜ ë„êµ¬ë¥¼ ì—°ë™í•´ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ í•´ì¤ë‹ˆë‹¤. ë˜í•œ ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ìˆ˜ë°± ê°œì˜ ë„êµ¬ ì„œë²„ë¥¼ LangGraph ì—ì´ì „íŠ¸ì™€ ì‰½ê²Œ í†µí•©í•©ë‹ˆë‹¤.\\n\\nì£¼ìš” í¬ì¸íŠ¸\\n- ê¸°ëŠ¥\\n  - MCP ë„êµ¬ë¥¼ LangChain/LangGraphì˜ ë„êµ¬ë¡œ ë³€í™˜\\n  - í•˜ë‚˜ ì´ìƒì˜ MCP ì„œë²„ì— ë¶„ì‚°ëœ ë„êµ¬ë¥¼ í•œ ë²ˆì— ì¡°íšŒí•˜ê³  ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì§€ì›\\n  - LangGraph ì—ì´ì „íŠ¸ì— ì‰½ê²Œ í†µí•©ë˜ì–´ ë„êµ¬ë“¤ì„ ì¡°í•©í•´ ê°•ë ¥í•œ ì• í”Œë¦¬ì¼€ì´ì…˜ êµ¬ì„± ê°€ëŠ¥\\n- ì™œ ìœ ìš©í•œê°€\\n  - MCP ìƒíƒœê³„ì— ìˆëŠ” ë‹¤ì–‘í•œ ë„êµ¬ë¥¼ ë‹¤ì‹œ ì°½ì‘ ì—†ì´ ë°”ë¡œ í™œìš© ê°€ëŠ¥\\n  - ë‹¤ìˆ˜ì˜ MCP ì„œë²„ì—ì„œ ë„êµ¬ë¥¼ ëŒì–´ì™€ í•˜ë‚˜ì˜ ì—ì´ì „íŠ¸ê°€ ì—¬ëŸ¬ ë„êµ¬ë¥¼ ë™ì‹œì— ì‚¬ìš©í•  ìˆ˜ ìˆìŒ\\n  - LangChain/LangGraphì˜ ê¸°ì¡´ ë„êµ¬ ì²´ê³„ì™€ ìì—°ìŠ¤ëŸ½ê²Œ ì—°ê²°\\n\\nêµ¬ì„± ìš”ì†Œ ë° ë™ì‘ ì›ë¦¬\\n- MultiServerMCPClient(ë˜ëŠ” ì´ì™€ ìœ ì‚¬í•œ API)\\n  - ì—¬ëŸ¬ MCP ì„œë²„ë¥¼ í†µí•œ ë„êµ¬ ëª©ë¡ ì¡°íšŒ ë° ê´€ë¦¬\\n  - MCP ë„êµ¬ë¥¼ LangChainì´ ì´í•´í•  ìˆ˜ ìˆëŠ” ë„êµ¬ë¡œ ë³€í™˜\\n- ë„êµ¬ ë³€í™˜\\n  - MCP ë„êµ¬ë¥¼ LangChain/LangGraphì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” í¬ë§·ìœ¼ë¡œ ë˜í•‘\\n- ì—ì´ì „íŠ¸ í†µí•©\\n  - LangChain ë˜ëŠ” LangGraphì˜ ì—ì´ì „íŠ¸ê°€ MCP ë„êµ¬ë¥¼ í˜¸ì¶œí•˜ë„ë¡ êµ¬ì„±\\n  - ì—¬ëŸ¬ MCP ì„œë²„ì˜ ë„êµ¬ë¥¼ ë™ì‹œì— ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì§€ì›\\n\\nì°¸ê³  ìë£Œ ë° ë” ì½ì„ê±°ë¦¬\\n- LangChain changelog: MCP Adapters for LangChain and LangGraph\\n  - MCP ë„êµ¬ ì„œë²„ ecosystemê³¼ì˜ ì—°ê²° ë° ë‹¤ì¤‘ ì„œë²„ ì§€ì›ì— ëŒ€í•œ ê°œìš”\\n- LangChain ê³µì‹ ë¬¸ì„œ (OSS JS/Python ë²„ì „ì˜ MCP ëª¨ë“ˆ)\\n  - MCP ëª¨ë“ˆì˜ ì‚¬ìš© ë°©ë²•ê³¼ ì—ì´ì „íŠ¸ ì—°ë™ ì˜ˆì œ\\n- GitHub ë ˆí¬ì§€í† ë¦¬: langchain-ai/langchain-mcp-adapters\\n  - ì„¤ì¹˜ ì˜ˆì œ, API ì‚¬ìš©ë²•, ìƒ˜í”Œ ì½”ë“œ ìŠ¤ë‹ˆí«\\n- PyPI: langchain-mcp-tools\\n  - LangChain MCP Adapters ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ê³µì‹ íŒ¨í‚¤ì§€ ì •ë³´ ë° ì‚¬ìš©ì²˜\\n\\nì‹œì‘ ë°©ë²•(ê°œëµ)\\n- MCP ì„œë²„ ì¤€ë¹„: MCP ë„êµ¬ë¥¼ ì œê³µí•˜ëŠ” ì„œë²„ë“¤ì„ ì‹¤í–‰í•˜ê±°ë‚˜ ì ‘ê·¼ ê°€ëŠ¥í•˜ë„ë¡ êµ¬ì„±\\n- ì„¤ì¹˜: Pythonìš© ì˜ˆì‹œ\\n  - ì˜ˆ: pip install langchain-mcp-tools\\n- í´ë¼ì´ì–¸íŠ¸ ë§Œë“¤ê¸°: ì—¬ëŸ¬ MCP ì„œë²„ë¥¼ ì—°ê²°í•˜ëŠ” MultiServerMCPClient ìƒì„±\\n- ë„êµ¬ ë¡œë“œ: MCP ì„œë²„ì˜ ë„êµ¬ë“¤ì„ ê°€ì ¸ì™€ LangChain/LangGraphì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ë³€í™˜\\n- ì—ì´ì „íŠ¸ êµ¬ì„±: ë³€í™˜ëœ ë„êµ¬ë“¤ì„ ì—ì´ì „íŠ¸ì˜ ë„êµ¬ ëª©ë¡ì— ì—°ê²°í•˜ê³  ëŒ€í™” íë¦„ì— í†µí•©\\n- ì‹¤í–‰: ì—ì´ì „íŠ¸ê°€ í•„ìš”ì— ë”°ë¼ MCP ë„êµ¬ë¥¼ í˜¸ì¶œí•˜ë„ë¡ ëŒ€í™” íë¦„ ì‹¤í–‰\\n\\nì¶”ê°€ë¡œ ì›í•˜ì‹œëŠ” ë‚´ìš©\\n- ì„¤ì¹˜ ë° ê°„ë‹¨í•œ ì˜ˆì œ ì½”ë“œê°€ í•„ìš”í•˜ì‹ ê°€ìš”? (Python ë˜ëŠ” JS ë²„ì „ ì¤‘ ì–´ë–¤ ìª½ì´ angenehmí•œì§€ ì•Œë ¤ ì£¼ì„¸ìš”)\\n- íŠ¹ì • MCP ì„œë²„ë‚˜ ë„êµ¬ì™€ì˜ ì—°ë™ ì˜ˆì œê°€ í•„ìš”í•˜ì‹ ê°€ìš”?\\n- LangChain, LangGraph ì¤‘ ì–´ëŠ ìª½ì—ì„œ ì“°ì‹¤ì§€ì— ë”°ë¼ Python vs JavaScript ì˜ˆì œë¥¼ ì œê³µí•´ ë“œë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤. \\n\\ní•„ìš”í•˜ì‹  ë°©í–¥ì„ ì•Œë ¤ì£¼ì‹œë©´, êµ¬ì²´ì ì¸ ì„¤ì¹˜ ë‹¨ê³„ì™€ ì½”ë“œ ì˜ˆì œë¡œ ë°”ë¡œ ë„ì™€ë“œë¦¬ê² ìŠµë‹ˆë‹¤.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 2045, 'prompt_tokens': 1478, 'total_tokens': 3523, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 1216, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-D5KrpcRoXh3MvW2JtOUPOLNKK9WzL', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019c2602-8382-76d3-8d34-052b3083000c-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 1478, 'output_tokens': 2045, 'total_tokens': 3523, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 1216}})]}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847409a3",
   "metadata": {},
   "source": [
    "## Online MCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b2895fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"time\": {\n",
    "            \"transport\": \"stdio\",\n",
    "            \"command\": \"uvx\",\n",
    "            \"args\": [\n",
    "                \"mcp-server-time\",\n",
    "                \"--local-timezone=America/New_York\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "tools = await client.get_tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e264dd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_agent(\n",
    "    model=\"gpt-5-nano\",\n",
    "    tools=tools,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4725cee3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='What time is it?', additional_kwargs={}, response_metadata={}, id='c82e3556-f96b-4773-a4af-d79358cf5f69'),\n",
      "              AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 155, 'prompt_tokens': 296, 'total_tokens': 451, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 128, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-D5L2PwqyarhkSto6x7kaWEpn67D7Q', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019c260c-8308-7ad0-abd4-d9ed5f14ea21-0', tool_calls=[{'name': 'get_current_time', 'args': {'timezone': 'America/New_York'}, 'id': 'call_6nc682yMR5BenI0DskcDxV4B', 'type': 'tool_call'}], invalid_tool_calls=[], usage_metadata={'input_tokens': 296, 'output_tokens': 155, 'total_tokens': 451, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 128}}),\n",
      "              ToolMessage(content=[{'type': 'text', 'text': '{\\n  \"timezone\": \"America/New_York\",\\n  \"datetime\": \"2026-02-03T19:28:03-05:00\",\\n  \"day_of_week\": \"Tuesday\",\\n  \"is_dst\": false\\n}', 'id': 'lc_857d89f2-52c4-4789-b65f-c5c6fba20c87'}], name='get_current_time', id='da529ed7-d36f-4f18-87c3-deed609215b3', tool_call_id='call_6nc682yMR5BenI0DskcDxV4B'),\n",
      "              AIMessage(content='In New York right now: Tuesday, February 3, 2026, 7:28 PM (Eastern Standard Time). \\n\\nWant me to convert this to another timezone or give you the exact time elsewhere?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 436, 'prompt_tokens': 379, 'total_tokens': 815, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 384, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-D5L2SrecShS6vTgkx5Qs8Uvia7VGg', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019c260c-90fb-7d03-8fb7-de20a733c96e-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 379, 'output_tokens': 436, 'total_tokens': 815, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 384}})]}\n"
     ]
    }
   ],
   "source": [
    "question = HumanMessage(content=\"What time is it?\")\n",
    "\n",
    "response = await agent.ainvoke( # tools = await client.get_tools() ë•Œë¬¸ì— ë¹„ë™ê¸°ë¡œ í˜¸ì¶œí•˜ëŠ” ê²ƒì´ ì•ˆì „\n",
    "    {\"messages\": [question]}\n",
    ")\n",
    "\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40bc5152",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
